{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc5a1d4",
   "metadata": {},
   "source": [
    "# Quick subset + train pipeline (Capstone-Lazarus)\n",
    "\n",
    "This notebook inspects `data/`, creates a small balanced subset, and trains a head-only transfer-learning model (timm EfficientNet-B0). Designed for fast experimentation on a laptop.\n",
    "\n",
    "## Features:\n",
    "- üéØ Balanced stratified subset creation\n",
    "- ‚ö° Fast training with frozen backbone\n",
    "- üöÄ AMP (Automatic Mixed Precision) support\n",
    "- üíæ Automatic checkpointing\n",
    "- üìä Real-time metrics tracking\n",
    "\n",
    "**Prerequisites:** Run this notebook from the repository root where `data/` directory exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fad5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libs (run once in notebook)\n",
    "!pip install --upgrade pip\n",
    "!pip install torch torchvision timm albumentations pillow scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d3651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability and set device\n",
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"CUDA not available, using CPU\")\n",
    "    \n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef308e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect original data structure\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('data')\n",
    "assert DATA_DIR.exists(), \"Run this notebook from repo root where `data/` exists.\"\n",
    "\n",
    "print(\"üîç Inspecting original dataset structure:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class_counts = {}\n",
    "for p in sorted([d for d in DATA_DIR.iterdir() if d.is_dir()]):\n",
    "    image_count = len([f for f in p.iterdir() if f.suffix.lower() in ('.jpg','.jpeg','.png','.bmp','.tif','.tiff','.webp')])\n",
    "    class_counts[p.name] = image_count\n",
    "    print(f\"{p.name:<40} {image_count:>6} images\")\n",
    "\n",
    "total_images = sum(class_counts.values())\n",
    "num_classes = len(class_counts)\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total classes: {num_classes}\")\n",
    "print(f\"Total images: {total_images}\")\n",
    "print(f\"Average per class: {total_images/num_classes:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea3f98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create balanced subset using our script\n",
    "# Parameters: adjust samples_per_class small for quick runs\n",
    "SAMPLES_PER_CLASS = 50   # try 30-100 for quick experiments\n",
    "VAL_RATIO = 0.2\n",
    "\n",
    "print(f\"üéØ Creating balanced subset:\")\n",
    "print(f\"   Samples per class: {SAMPLES_PER_CLASS}\")\n",
    "print(f\"   Validation ratio: {VAL_RATIO}\")\n",
    "print(f\"   Expected total: ~{SAMPLES_PER_CLASS * num_classes} images\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "!python scripts/create_subset.py --data-dir data --out-dir data_subset --samples-per-class {SAMPLES_PER_CLASS} --val-ratio {VAL_RATIO} --seed 42 --symlink true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df53f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders with Albumentations transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class ImageFolderAlb(Dataset):\n",
    "    \"\"\"Custom Dataset using Albumentations for transforms\"\"\"\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.samples = []\n",
    "        exts = {'.jpg','.jpeg','.png','.bmp','.tif','.tiff','.webp'}\n",
    "        classes = sorted([p for p in self.root.iterdir() if p.is_dir()])\n",
    "        self.class_to_idx = {d.name: i for i, d in enumerate(classes)}\n",
    "        self.classes = [cls.name for cls in classes]\n",
    "        \n",
    "        for cls in classes:\n",
    "            for img in cls.iterdir():\n",
    "                if img.suffix.lower() in exts:\n",
    "                    self.samples.append((img, cls.name))\n",
    "        self.transform = transform\n",
    "        print(f\"Found {len(self.samples)} images in {len(classes)} classes\")\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        p, cls = self.samples[idx]\n",
    "        img = np.array(Image.open(p).convert('RGB'))\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        label = self.class_to_idx[cls]\n",
    "        return img, label\n",
    "\n",
    "def get_transforms(img_size=224, split='train'):\n",
    "    \"\"\"Get Albumentations transforms for train/val\"\"\"\n",
    "    if split == 'train':\n",
    "        return A.Compose([\n",
    "            A.Resize(img_size, img_size),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomResizedCrop(img_size, img_size, scale=(0.7, 1.0), p=0.6),\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02, p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:  # validation\n",
    "        return A.Compose([\n",
    "            A.Resize(img_size, img_size), \n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), \n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "# Configuration - adjust for your hardware\n",
    "IMG_SIZE = 160      # Small size for laptop-friendly training\n",
    "BATCH_SIZE = 16     # Adjust based on GPU memory\n",
    "NUM_WORKERS = 4     # Adjust based on CPU cores\n",
    "\n",
    "print(f\"üîÑ Setting up data loaders:\")\n",
    "print(f\"   Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Num workers: {NUM_WORKERS}\")\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_ds = ImageFolderAlb('data_subset/train', transform=get_transforms(IMG_SIZE, 'train'))\n",
    "val_ds = ImageFolderAlb('data_subset/val', transform=get_transforms(IMG_SIZE, 'val'))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                         num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                       num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Data loaders ready:\")\n",
    "print(f\"   Train: {len(train_ds)} images, {len(train_loader)} batches\")\n",
    "print(f\"   Val: {len(val_ds)} images, {len(val_loader)} batches\")\n",
    "print(f\"   Classes: {len(train_ds.classes)}\")\n",
    "print(f\"   Class names: {train_ds.classes[:5]}...\" if len(train_ds.classes) > 5 else f\"   Class names: {train_ds.classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f71e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with frozen backbone (transfer learning)\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = 'tf_efficientnet_b0'  # Efficient and fast\n",
    "num_classes = len(train_ds.class_to_idx)\n",
    "\n",
    "print(f\"üèóÔ∏è Initializing model:\")\n",
    "print(f\"   Architecture: {MODEL_NAME}\")\n",
    "print(f\"   Number of classes: {num_classes}\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "# Create pre-trained model\n",
    "model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=num_classes)\n",
    "\n",
    "# Freeze backbone parameters (transfer learning)\n",
    "print(\"\\nüßä Freezing backbone parameters...\")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Reset classifier head and ensure it's trainable\n",
    "print(\"üéØ Setting up classifier head...\")\n",
    "try:\n",
    "    model.reset_classifier(num_classes)\n",
    "except Exception:\n",
    "    # Fallback for different model architectures\n",
    "    if hasattr(model, 'classifier'):\n",
    "        in_features = model.classifier.in_features\n",
    "        model.classifier = nn.Linear(in_features, num_classes)\n",
    "    elif hasattr(model, 'fc'):\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(in_features, num_classes)\n",
    "    else:\n",
    "        raise RuntimeError(\"Could not find classifier layer\")\n",
    "\n",
    "# Ensure head parameters are trainable\n",
    "head_params = 0\n",
    "total_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    total_params += param.numel()\n",
    "    if any(x in name.lower() for x in ['classifier', 'fc', 'head', 'ln']):\n",
    "        param.requires_grad = True\n",
    "        head_params += param.numel()\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"‚úÖ Model ready:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {head_params:,}\")\n",
    "print(f\"   Frozen parameters: {total_params - head_params:,}\")\n",
    "print(f\"   Training only: {(head_params/total_params)*100:.1f}% of parameters\")\n",
    "\n",
    "# Show model summary\n",
    "print(f\"\\nüìã Model architecture:\")\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
