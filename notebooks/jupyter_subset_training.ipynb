{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc5a1d4",
   "metadata": {},
   "source": [
    "# Quick subset + train pipeline (Capstone-Lazarus)\n",
    "\n",
    "This notebook inspects `data/`, creates a small balanced subset, and trains a head-only transfer-learning model (timm EfficientNet-B0). Designed for fast experimentation on a laptop.\n",
    "\n",
    "## Features:\n",
    "- üéØ Balanced stratified subset creation\n",
    "- ‚ö° Fast training with frozen backbone\n",
    "- üöÄ AMP (Automatic Mixed Precision) support\n",
    "- üíæ Automatic checkpointing\n",
    "- üìä Real-time metrics tracking\n",
    "\n",
    "**Prerequisites:** Run this notebook from the repository root where `data/` directory exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fad5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libs (run once in notebook)\n",
    "!pip install --upgrade pip\n",
    "!pip install torch torchvision timm albumentations pillow scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0d3651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CUDA availability and set device\n",
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"CUDA not available, using CPU\")\n",
    "    \n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Python version: {sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef308e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect original data structure\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('data')\n",
    "assert DATA_DIR.exists(), \"Run this notebook from repo root where `data/` exists.\"\n",
    "\n",
    "print(\"üîç Inspecting original dataset structure:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "class_counts = {}\n",
    "for p in sorted([d for d in DATA_DIR.iterdir() if d.is_dir()]):\n",
    "    image_count = len([f for f in p.iterdir() if f.suffix.lower() in ('.jpg','.jpeg','.png','.bmp','.tif','.tiff','.webp')])\n",
    "    class_counts[p.name] = image_count\n",
    "    print(f\"{p.name:<40} {image_count:>6} images\")\n",
    "\n",
    "total_images = sum(class_counts.values())\n",
    "num_classes = len(class_counts)\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total classes: {num_classes}\")\n",
    "print(f\"Total images: {total_images}\")\n",
    "print(f\"Average per class: {total_images/num_classes:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea3f98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create balanced subset using our script\n",
    "# Parameters: adjust samples_per_class small for quick runs\n",
    "SAMPLES_PER_CLASS = 50   # try 30-100 for quick experiments\n",
    "VAL_RATIO = 0.2\n",
    "\n",
    "print(f\"üéØ Creating balanced subset:\")\n",
    "print(f\"   Samples per class: {SAMPLES_PER_CLASS}\")\n",
    "print(f\"   Validation ratio: {VAL_RATIO}\")\n",
    "print(f\"   Expected total: ~{SAMPLES_PER_CLASS * num_classes} images\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "!python scripts/create_subset.py --data-dir data --out-dir data_subset --samples-per-class {SAMPLES_PER_CLASS} --val-ratio {VAL_RATIO} --seed 42 --symlink true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df53f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders with Albumentations transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class ImageFolderAlb(Dataset):\n",
    "    \"\"\"Custom Dataset using Albumentations for transforms\"\"\"\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.samples = []\n",
    "        exts = {'.jpg','.jpeg','.png','.bmp','.tif','.tiff','.webp'}\n",
    "        classes = sorted([p for p in self.root.iterdir() if p.is_dir()])\n",
    "        self.class_to_idx = {d.name: i for i, d in enumerate(classes)}\n",
    "        self.classes = [cls.name for cls in classes]\n",
    "        \n",
    "        for cls in classes:\n",
    "            for img in cls.iterdir():\n",
    "                if img.suffix.lower() in exts:\n",
    "                    self.samples.append((img, cls.name))\n",
    "        self.transform = transform\n",
    "        print(f\"Found {len(self.samples)} images in {len(classes)} classes\")\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        p, cls = self.samples[idx]\n",
    "        img = np.array(Image.open(p).convert('RGB'))\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        label = self.class_to_idx[cls]\n",
    "        return img, label\n",
    "\n",
    "def get_transforms(img_size=224, split='train'):\n",
    "    \"\"\"Get Albumentations transforms for train/val\"\"\"\n",
    "    if split == 'train':\n",
    "        return A.Compose([\n",
    "            A.Resize(img_size, img_size),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.RandomResizedCrop(img_size, img_size, scale=(0.7, 1.0), p=0.6),\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.02, p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:  # validation\n",
    "        return A.Compose([\n",
    "            A.Resize(img_size, img_size), \n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), \n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "# Configuration - adjust for your hardware\n",
    "IMG_SIZE = 160      # Small size for laptop-friendly training\n",
    "BATCH_SIZE = 16     # Adjust based on GPU memory\n",
    "NUM_WORKERS = 4     # Adjust based on CPU cores\n",
    "\n",
    "print(f\"üîÑ Setting up data loaders:\")\n",
    "print(f\"   Image size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Num workers: {NUM_WORKERS}\")\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_ds = ImageFolderAlb('data_subset/train', transform=get_transforms(IMG_SIZE, 'train'))\n",
    "val_ds = ImageFolderAlb('data_subset/val', transform=get_transforms(IMG_SIZE, 'val'))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                         num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                       num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Data loaders ready:\")\n",
    "print(f\"   Train: {len(train_ds)} images, {len(train_loader)} batches\")\n",
    "print(f\"   Val: {len(val_ds)} images, {len(val_loader)} batches\")\n",
    "print(f\"   Classes: {len(train_ds.classes)}\")\n",
    "print(f\"   Class names: {train_ds.classes[:5]}...\" if len(train_ds.classes) > 5 else f\"   Class names: {train_ds.classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f71e839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with frozen backbone (transfer learning)\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = 'tf_efficientnet_b0'  # Efficient and fast\n",
    "num_classes = len(train_ds.class_to_idx)\n",
    "\n",
    "print(f\"üèóÔ∏è Initializing model:\")\n",
    "print(f\"   Architecture: {MODEL_NAME}\")\n",
    "print(f\"   Number of classes: {num_classes}\")\n",
    "print(f\"   Device: {device}\")\n",
    "\n",
    "# Create pre-trained model\n",
    "model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=num_classes)\n",
    "\n",
    "# Freeze backbone parameters (transfer learning)\n",
    "print(\"\\nüßä Freezing backbone parameters...\")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Reset classifier head and ensure it's trainable\n",
    "print(\"üéØ Setting up classifier head...\")\n",
    "try:\n",
    "    model.reset_classifier(num_classes)\n",
    "except Exception:\n",
    "    # Fallback for different model architectures\n",
    "    if hasattr(model, 'classifier'):\n",
    "        in_features = model.classifier.in_features\n",
    "        model.classifier = nn.Linear(in_features, num_classes)\n",
    "    elif hasattr(model, 'fc'):\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(in_features, num_classes)\n",
    "    else:\n",
    "        raise RuntimeError(\"Could not find classifier layer\")\n",
    "\n",
    "# Ensure head parameters are trainable\n",
    "head_params = 0\n",
    "total_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    total_params += param.numel()\n",
    "    if any(x in name.lower() for x in ['classifier', 'fc', 'head', 'ln']):\n",
    "        param.requires_grad = True\n",
    "        head_params += param.numel()\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"‚úÖ Model ready:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {head_params:,}\")\n",
    "print(f\"   Frozen parameters: {total_params - head_params:,}\")\n",
    "print(f\"   Training only: {(head_params/total_params)*100:.1f}% of parameters\")\n",
    "\n",
    "# Show model summary\n",
    "print(f\"\\nüìã Model architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8774c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with AMP and checkpointing\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Training configuration\n",
    "EPOCHS = 4          # Small number for quick experiments\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "# Setup loss, optimizer, and AMP scaler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), \n",
    "    lr=LEARNING_RATE, \n",
    "    weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "scaler = GradScaler() if torch.cuda.is_available() else None\n",
    "\n",
    "print(f\"üöÄ Training setup:\")\n",
    "print(f\"   Epochs: {EPOCHS}\")\n",
    "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"   Weight decay: {WEIGHT_DECAY}\")\n",
    "print(f\"   AMP enabled: {scaler is not None}\")\n",
    "print(f\"   Optimizer: AdamW\")\n",
    "\n",
    "def train_one_epoch(model, loader, epoch):\n",
    "    \"\"\"Train for one epoch with progress tracking\"\"\"\n",
    "    model.train()\n",
    "    losses, preds, targets = [], [], []\n",
    "    \n",
    "    loop = tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for batch_idx, (imgs, labels) in enumerate(loop):\n",
    "        imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with AMP\n",
    "        with autocast(enabled=(scaler is not None)):\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        losses.append(loss.item())\n",
    "        preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "        targets.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Update progress bar\n",
    "        current_loss = np.mean(losses)\n",
    "        current_acc = accuracy_score(targets, preds)\n",
    "        loop.set_postfix({\n",
    "            'loss': f'{current_loss:.4f}',\n",
    "            'acc': f'{current_acc:.4f}'\n",
    "        })\n",
    "    \n",
    "    return np.mean(losses), accuracy_score(targets, preds)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader):\n",
    "    \"\"\"Validate model performance\"\"\"\n",
    "    model.eval()\n",
    "    losses, preds, targets = [], [], []\n",
    "    \n",
    "    for imgs, labels in tqdm(loader, desc=\"Validating\", leave=False):\n",
    "        imgs, labels = imgs.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "        \n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        preds.extend(outputs.argmax(1).cpu().numpy())\n",
    "        targets.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return np.mean(losses), accuracy_score(targets, preds)\n",
    "\n",
    "# Training loop\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"üéØ STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "best_acc = 0.0\n",
    "training_history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Training\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, epoch)\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_acc = validate(model, val_loader)\n",
    "    \n",
    "    # Save metrics\n",
    "    training_history['train_loss'].append(train_loss)\n",
    "    training_history['train_acc'].append(train_acc)\n",
    "    training_history['val_loss'].append(val_loss)\n",
    "    training_history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"Val Loss:   {val_loss:.4f} | Val Acc:   {val_acc:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'best_acc': best_acc,\n",
    "            'training_history': training_history\n",
    "        }, 'best_head_only.pth')\n",
    "        print(f\"üíæ Saved best model! (Val Acc: {best_acc:.4f})\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"=\"*60)\n",
    "print(\"üèÅ TRAINING COMPLETED\")\n",
    "print(f\"‚è±Ô∏è Total time: {elapsed_time:.1f}s ({elapsed_time/60:.1f}m)\")\n",
    "print(f\"üéØ Best validation accuracy: {best_acc:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780dd036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results visualization and final evaluation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation curves\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Plot loss\n",
    "    ax1.plot(history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "    ax1.plot(history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "    ax1.set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    ax2.plot(history['train_acc'], 'b-', label='Training Accuracy', linewidth=2)\n",
    "    ax2.plot(history['val_acc'], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "    ax2.set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training curves\n",
    "print(\"üìä TRAINING RESULTS VISUALIZATION\")\n",
    "print(\"=\"*60)\n",
    "plot_training_history(training_history)\n",
    "\n",
    "# Load best model for final evaluation\n",
    "print(\"\\\\nüîç FINAL MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "checkpoint = torch.load('best_head_only.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"‚úÖ Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "\n",
    "# Final validation\n",
    "final_val_loss, final_val_acc = validate(model, val_loader)\n",
    "print(f\"üéØ Final Validation Accuracy: {final_val_acc:.4f}\")\n",
    "print(f\"üìâ Final Validation Loss: {final_val_loss:.4f}\")\n",
    "\n",
    "# Class-wise performance analysis\n",
    "@torch.no_grad()\n",
    "def detailed_evaluation(model, loader, class_names):\n",
    "    \"\"\"Detailed per-class evaluation\"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "    \n",
    "    for imgs, labels in tqdm(loader, desc=\"Detailed evaluation\"):\n",
    "        imgs = imgs.to(device)\n",
    "        outputs = model(imgs)\n",
    "        preds = outputs.argmax(1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_targets.extend(labels.numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    # Class-wise accuracy\n",
    "    print(\"\\\\nüìã CLASS-WISE PERFORMANCE:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_mask = (all_targets == i)\n",
    "        if class_mask.sum() > 0:\n",
    "            class_acc = (all_preds[class_mask] == all_targets[class_mask]).mean()\n",
    "            print(f\"{class_name:<30} | Acc: {class_acc:.4f} | Samples: {class_mask.sum()}\")\n",
    "    \n",
    "    overall_acc = (all_preds == all_targets).mean()\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'OVERALL':<30} | Acc: {overall_acc:.4f} | Samples: {len(all_targets)}\")\n",
    "    \n",
    "    return all_preds, all_targets\n",
    "\n",
    "# Perform detailed evaluation\n",
    "pred_labels, true_labels = detailed_evaluation(model, val_loader, class_names)\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*60)\n",
    "print(\"üéâ SUBSET TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìÅ Subset size: {samples_per_class} samples per class\")\n",
    "print(f\"üî• Best accuracy: {best_acc:.4f}\")\n",
    "print(f\"‚è±Ô∏è Total training time: {elapsed_time:.1f}s\")\n",
    "print(f\"üíæ Best model saved as: best_head_only.pth\")\n",
    "print(\"\\\\n‚ú® Ready for full dataset training or further experiments!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
