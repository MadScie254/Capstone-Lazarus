{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6e9265",
   "metadata": {},
   "source": [
    "# 🔥 Micro-Job Feature Extraction Pipeline\n",
    "\n",
    "**Mission**: Eliminate training bottlenecks with resumable feature caching  \n",
    "**Target**: 4GB VRAM, 64 images per job, <2min per job  \n",
    "**Strategy**: EfficientNet-B0 encoder → float16 NPZ cache → head-only training\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Pipeline Overview\n",
    "\n",
    "1. **Job Queue Creation**: Split dataset into 64-image chunks\n",
    "2. **Feature Extraction**: Process jobs with encoder (batch_size=8)\n",
    "3. **Feature Caching**: Save as `features/encoder_*/img_*.npz` (float16)\n",
    "4. **Manifest Generation**: Create `features/manifest_features.v001.csv`\n",
    "5. **Resume Logic**: Skip completed jobs via `.done` files\n",
    "\n",
    "### 📊 Resource Targets\n",
    "- **VRAM**: <2.5GB peak (within 4GB constraint)\n",
    "- **Speed**: 64 images in <2 minutes\n",
    "- **Storage**: ~50MB per 1000 images (float16 compression)\n",
    "- **Quality**: Equivalent to full training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7397f3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔧 Setup & Imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Project imports\n",
    "sys.path.append('../src')\n",
    "from data_utils import ImageFolderAlb\n",
    "\n",
    "# 🎮 Device & Memory Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"🚀 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"💾 VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "else:\n",
    "    print(\"⚠️ Running on CPU - feature extraction will be slower\")\n",
    "\n",
    "print(f\"🔧 PyTorch: {torch.__version__}\")\n",
    "print(f\"📁 Working dir: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c3c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⚙️ Configuration\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'data_dir': '../data',\n",
    "    'features_dir': '../features',\n",
    "    'encoder_name': 'efficientnet_b0',\n",
    "    \n",
    "    # Job settings (4GB VRAM optimized)\n",
    "    'job_size': 64,          # Images per job\n",
    "    'batch_size': 8,         # Processing batch (VRAM constraint)\n",
    "    'img_size': 224,         # Input resolution\n",
    "    'feature_dtype': 'float16',  # Memory compression\n",
    "    \n",
    "    # Feature extraction\n",
    "    'use_global_pool': True,     # Extract global features\n",
    "    'extract_spatial': False,    # Skip spatial for now (head-only training)\n",
    "    'normalize_features': True,  # L2 normalize\n",
    "    \n",
    "    # Performance\n",
    "    'num_workers': 4,        # DataLoader workers\n",
    "    'pin_memory': True,      # GPU transfer optimization\n",
    "    'prefetch_factor': 2,    # Async data loading\n",
    "}\n",
    "\n",
    "print(\"🎯 MICRO-JOB CONFIGURATION:\")\n",
    "print(f\"   📊 Job size: {CONFIG['job_size']} images\")\n",
    "print(f\"   🎬 Batch size: {CONFIG['batch_size']} (VRAM-safe)\")\n",
    "print(f\"   📐 Image size: {CONFIG['img_size']}px\")\n",
    "print(f\"   🗜️ Feature dtype: {CONFIG['feature_dtype']}\")\n",
    "print(f\"   🏗️ Encoder: {CONFIG['encoder_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e88835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Dataset Scanning & Job Queue Creation\n",
    "\n",
    "def scan_dataset(data_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"Scan dataset and create image manifest\"\"\"\n",
    "    print(f\"🔍 Scanning dataset: {data_dir}\")\n",
    "    \n",
    "    data_path = Path(data_dir)\n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(f\"Data directory not found: {data_dir}\")\n",
    "    \n",
    "    # Collect all images\n",
    "    images = []\n",
    "    for class_dir in data_path.iterdir():\n",
    "        if not class_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        class_name = class_dir.name\n",
    "        print(f\"   📁 Processing class: {class_name}\")\n",
    "        \n",
    "        for img_file in class_dir.glob('*'):\n",
    "            if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
    "                images.append({\n",
    "                    'image_path': str(img_file),\n",
    "                    'class_name': class_name,\n",
    "                    'image_id': f\"{class_name}_{img_file.stem}\",\n",
    "                    'file_size': img_file.stat().st_size\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(images)\n",
    "    print(f\"\\n✅ Dataset scan complete:\")\n",
    "    print(f\"   🖼️ Total images: {len(df):,}\")\n",
    "    print(f\"   🏷️ Classes: {df['class_name'].nunique()}\")\n",
    "    print(f\"   💾 Total size: {df['file_size'].sum() / 1e9:.2f}GB\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_job_queue(image_df: pd.DataFrame, job_size: int = 64) -> pd.DataFrame:\n",
    "    \"\"\"Split images into job chunks for micro-job processing\"\"\"\n",
    "    print(f\"\\n📋 Creating job queue (job_size={job_size})...\")\n",
    "    \n",
    "    # Shuffle for balanced jobs across classes\n",
    "    shuffled_df = image_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Create job chunks\n",
    "    jobs = []\n",
    "    for i in range(0, len(shuffled_df), job_size):\n",
    "        job_images = shuffled_df.iloc[i:i+job_size]\n",
    "        \n",
    "        jobs.append({\n",
    "            'job_id': len(jobs),\n",
    "            'image_paths': ','.join(job_images['image_path'].tolist()),\n",
    "            'image_ids': ','.join(job_images['image_id'].tolist()),\n",
    "            'num_images': len(job_images),\n",
    "            'classes': ','.join(job_images['class_name'].unique()),\n",
    "            'status': 'pending',\n",
    "            'created_at': datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    job_df = pd.DataFrame(jobs)\n",
    "    print(f\"✅ Job queue created: {len(job_df)} jobs\")\n",
    "    print(f\"   📊 Average job size: {job_df['num_images'].mean():.1f} images\")\n",
    "    print(f\"   🎯 Estimated time: {len(job_df) * 2:.0f} minutes (2min/job)\")\n",
    "    \n",
    "    return job_df\n",
    "\n",
    "# Execute dataset scanning\n",
    "image_manifest = scan_dataset(CONFIG['data_dir'])\n",
    "job_queue = create_job_queue(image_manifest, CONFIG['job_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b1d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🏗️ Feature Extraction Setup\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"Lightweight feature extractor with global pooling\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder_name: str = 'efficientnet_b0', pretrained: bool = True):\n",
    "        super().__init__()\n",
    "        self.encoder_name = encoder_name\n",
    "        \n",
    "        # Load pretrained encoder\n",
    "        self.backbone = timm.create_model(\n",
    "            encoder_name, \n",
    "            pretrained=pretrained,\n",
    "            num_classes=0,  # Remove classifier head\n",
    "            global_pool='avg'  # Global average pooling\n",
    "        )\n",
    "        \n",
    "        # Get feature dimensions\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 224, 224)\n",
    "            dummy_output = self.backbone(dummy_input)\n",
    "            self.feature_dim = dummy_output.shape[1]\n",
    "        \n",
    "        print(f\"🏗️ Feature extractor: {encoder_name}\")\n",
    "        print(f\"   📐 Feature dim: {self.feature_dim}\")\n",
    "        print(f\"   💾 Parameters: {sum(p.numel() for p in self.parameters()):,}\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Extract global features\"\"\"\n",
    "        features = self.backbone(x)  # [B, feature_dim]\n",
    "        return features\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Simple dataset for feature extraction\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths: List[str], transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error loading {img_path}: {e}\")\n",
    "            # Return black image as fallback\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, img_path\n",
    "\n",
    "# Initialize feature extractor\n",
    "feature_extractor = FeatureExtractor(CONFIG['encoder_name']).to(device)\n",
    "feature_extractor.eval()\n",
    "\n",
    "# Define transforms (minimal - just resize & normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(f\"✅ Feature extraction setup complete\")\n",
    "print(f\"   🎯 Ready for {CONFIG['job_size']}-image micro-jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb3a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔥 Core Job Execution Function\n",
    "\n",
    "def run_feature_job(job_id: int, job_queue: pd.DataFrame, force_rerun: bool = False) -> bool:\n",
    "    \"\"\"Execute single feature extraction job\"\"\"\n",
    "    \n",
    "    if job_id >= len(job_queue):\n",
    "        print(f\"❌ Job ID {job_id} out of range (max: {len(job_queue)-1})\")\n",
    "        return False\n",
    "    \n",
    "    job = job_queue.iloc[job_id]\n",
    "    \n",
    "    # Create output directories\n",
    "    features_dir = Path(CONFIG['features_dir'])\n",
    "    encoder_dir = features_dir / f\"encoder_{CONFIG['encoder_name']}\"\n",
    "    encoder_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Check if job already completed\n",
    "    done_file = features_dir / f\"_job_{job_id:04d}_{int(time.time())}.done\"\n",
    "    existing_done = list(features_dir.glob(f\"_job_{job_id:04d}_*.done\"))\n",
    "    \n",
    "    if existing_done and not force_rerun:\n",
    "        print(f\"✅ Job {job_id} already completed: {existing_done[0].name}\")\n",
    "        return True\n",
    "    \n",
    "    print(f\"\\n🚀 Starting job {job_id}/{len(job_queue)-1}\")\n",
    "    print(f\"   📊 Images: {job['num_images']}\")\n",
    "    print(f\"   🏷️ Classes: {job['classes']}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Parse image paths\n",
    "        image_paths = job['image_paths'].split(',')\n",
    "        image_ids = job['image_ids'].split(',')\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        dataset = ImageDataset(image_paths, transform=transform)\n",
    "        dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=CONFIG['num_workers'],\n",
    "            pin_memory=CONFIG['pin_memory'],\n",
    "            prefetch_factor=CONFIG['prefetch_factor']\n",
    "        )\n",
    "        \n",
    "        # Extract features\n",
    "        all_features = []\n",
    "        all_paths = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_images, batch_paths in tqdm(dataloader, \n",
    "                                                 desc=f\"Job {job_id}\", \n",
    "                                                 leave=False):\n",
    "                batch_images = batch_images.to(device, non_blocking=True)\n",
    "                \n",
    "                # Extract features\n",
    "                features = feature_extractor(batch_images)  # [B, feature_dim]\n",
    "                \n",
    "                # Normalize if requested\n",
    "                if CONFIG['normalize_features']:\n",
    "                    features = torch.nn.functional.normalize(features, p=2, dim=1)\n",
    "                \n",
    "                # Convert to numpy and compress to float16\n",
    "                features_np = features.cpu().numpy().astype(CONFIG['feature_dtype'])\n",
    "                \n",
    "                all_features.append(features_np)\n",
    "                all_paths.extend(batch_paths)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        all_features = np.concatenate(all_features, axis=0)\n",
    "        \n",
    "        print(f\"   ✅ Extracted: {all_features.shape} features\")\n",
    "        \n",
    "        # Save features individually\n",
    "        saved_count = 0\n",
    "        for i, (img_path, img_id) in enumerate(zip(all_paths, image_ids)):\n",
    "            feature_file = encoder_dir / f\"{img_id}.npz\"\n",
    "            \n",
    "            np.savez_compressed(\n",
    "                feature_file,\n",
    "                features=all_features[i],\n",
    "                image_path=img_path,\n",
    "                image_id=img_id,\n",
    "                encoder_name=CONFIG['encoder_name'],\n",
    "                extraction_time=datetime.now().isoformat()\n",
    "            )\n",
    "            saved_count += 1\n",
    "        \n",
    "        # Create completion marker\n",
    "        job_metadata = {\n",
    "            'job_id': job_id,\n",
    "            'num_images': len(image_paths),\n",
    "            'feature_shape': list(all_features.shape),\n",
    "            'processing_time': time.time() - start_time,\n",
    "            'encoder_name': CONFIG['encoder_name'],\n",
    "            'config': CONFIG,\n",
    "            'completed_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(done_file, 'w') as f:\n",
    "            json.dump(job_metadata, f, indent=2)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"✅ Job {job_id} completed: {saved_count} features saved in {elapsed:.1f}s\")\n",
    "        print(f\"   💾 Output: {encoder_dir}/\")\n",
    "        print(f\"   🏁 Done marker: {done_file.name}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Job {job_id} failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "print(\"🔥 Job execution function ready\")\n",
    "print(\"   Usage: run_feature_job(job_id, job_queue)\")\n",
    "print(\"   Target: <2 minutes per 64-image job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6527c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🧪 TEST: Single Job Execution\n",
    "# Run this cell to test the pipeline with job 0\n",
    "\n",
    "TEST_JOB_ID = 0\n",
    "\n",
    "print(f\"🧪 Testing job execution with job {TEST_JOB_ID}\")\n",
    "print(f\"Expected: {job_queue.iloc[TEST_JOB_ID]['num_images']} images processed\")\n",
    "\n",
    "# Clear GPU memory before test\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"🧹 GPU memory cleared\")\n",
    "\n",
    "# Run test job\n",
    "success = run_feature_job(TEST_JOB_ID, job_queue, force_rerun=True)\n",
    "\n",
    "if success:\n",
    "    # Verify outputs\n",
    "    encoder_dir = Path(CONFIG['features_dir']) / f\"encoder_{CONFIG['encoder_name']}\"\n",
    "    feature_files = list(encoder_dir.glob('*.npz'))\n",
    "    done_files = list(Path(CONFIG['features_dir']).glob(f'_job_{TEST_JOB_ID:04d}_*.done'))\n",
    "    \n",
    "    print(f\"\\n✅ TEST RESULTS:\")\n",
    "    print(f\"   📁 Feature files created: {len(feature_files)}\")\n",
    "    print(f\"   🏁 Done files created: {len(done_files)}\")\n",
    "    \n",
    "    # Test loading a feature file\n",
    "    if feature_files:\n",
    "        test_feature = np.load(feature_files[0])\n",
    "        print(f\"   🧪 Sample feature shape: {test_feature['features'].shape}\")\n",
    "        print(f\"   🗜️ Feature dtype: {test_feature['features'].dtype}\")\n",
    "        \n",
    "        # Check memory usage\n",
    "        feature_size = test_feature['features'].nbytes\n",
    "        total_estimated = feature_size * len(image_manifest) / 1e6\n",
    "        print(f\"   💾 Per-feature size: {feature_size} bytes\")\n",
    "        print(f\"   📊 Estimated total: {total_estimated:.1f}MB for full dataset\")\n",
    "        \n",
    "    print(f\"\\n🎯 Test job completed successfully!\")\n",
    "else:\n",
    "    print(f\"❌ Test job failed - check error messages above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669221a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🏭 Batch Job Execution (Full Pipeline)\n",
    "# WARNING: This will process ALL jobs - use for full feature extraction\n",
    "\n",
    "def run_all_jobs(job_queue: pd.DataFrame, max_jobs: int = None, \n",
    "                 start_job: int = 0) -> Dict:\n",
    "    \"\"\"Execute all feature extraction jobs with progress tracking\"\"\"\n",
    "    \n",
    "    total_jobs = len(job_queue)\n",
    "    if max_jobs:\n",
    "        total_jobs = min(total_jobs, max_jobs)\n",
    "    \n",
    "    print(f\"🏭 BATCH JOB EXECUTION\")\n",
    "    print(f\"   📊 Total jobs: {total_jobs}\")\n",
    "    print(f\"   🎯 Estimated time: {total_jobs * 2:.0f} minutes\")\n",
    "    print(f\"   💾 Estimated storage: {total_jobs * CONFIG['job_size'] * 0.05:.1f}MB\")\n",
    "    \n",
    "    results = {\n",
    "        'completed_jobs': [],\n",
    "        'failed_jobs': [],\n",
    "        'total_time': 0,\n",
    "        'total_features': 0\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for job_id in tqdm(range(start_job, min(start_job + total_jobs, len(job_queue))), \n",
    "                       desc=\"Processing jobs\"):\n",
    "        \n",
    "        job_start = time.time()\n",
    "        success = run_feature_job(job_id, job_queue)\n",
    "        job_time = time.time() - job_start\n",
    "        \n",
    "        if success:\n",
    "            results['completed_jobs'].append({\n",
    "                'job_id': job_id,\n",
    "                'time': job_time,\n",
    "                'images': job_queue.iloc[job_id]['num_images']\n",
    "            })\n",
    "            results['total_features'] += job_queue.iloc[job_id]['num_images']\n",
    "        else:\n",
    "            results['failed_jobs'].append(job_id)\n",
    "        \n",
    "        # Clear GPU memory periodically\n",
    "        if job_id % 10 == 0 and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    results['total_time'] = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n🏁 BATCH EXECUTION COMPLETE\")\n",
    "    print(f\"   ✅ Completed: {len(results['completed_jobs'])}/{total_jobs} jobs\")\n",
    "    print(f\"   ❌ Failed: {len(results['failed_jobs'])} jobs\")\n",
    "    print(f\"   ⏱️ Total time: {results['total_time']/60:.1f} minutes\")\n",
    "    print(f\"   🖼️ Total features: {results['total_features']:,}\")\n",
    "    \n",
    "    if results['completed_jobs']:\n",
    "        avg_time = np.mean([j['time'] for j in results['completed_jobs']])\n",
    "        print(f\"   📊 Average job time: {avg_time:.1f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# COMMENTED OUT - UNCOMMENT TO RUN FULL EXTRACTION\n",
    "# This will process all jobs and may take hours!\n",
    "\n",
    "# results = run_all_jobs(job_queue, max_jobs=5)  # Test with 5 jobs first\n",
    "\n",
    "print(\"⚠️ Batch execution commented out for safety\")\n",
    "print(\"Uncomment and modify max_jobs parameter to run full extraction\")\n",
    "print(f\"Total jobs available: {len(job_queue)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e0864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 Feature Manifest Generation\n",
    "\n",
    "def create_feature_manifest(features_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"Create comprehensive manifest of extracted features\"\"\"\n",
    "    \n",
    "    print(f\"📊 Creating feature manifest from {features_dir}\")\n",
    "    \n",
    "    features_path = Path(features_dir)\n",
    "    encoder_dir = features_path / f\"encoder_{CONFIG['encoder_name']}\"\n",
    "    \n",
    "    if not encoder_dir.exists():\n",
    "        print(f\"⚠️ Encoder directory not found: {encoder_dir}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Collect all feature files\n",
    "    feature_files = list(encoder_dir.glob('*.npz'))\n",
    "    print(f\"   📁 Found {len(feature_files)} feature files\")\n",
    "    \n",
    "    if not feature_files:\n",
    "        print(\"   ⚠️ No feature files found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    manifest_data = []\n",
    "    \n",
    "    for feature_file in tqdm(feature_files, desc=\"Building manifest\"):\n",
    "        try:\n",
    "            # Load metadata without loading full features\n",
    "            with np.load(feature_file) as data:\n",
    "                manifest_data.append({\n",
    "                    'image_id': str(data['image_id']),\n",
    "                    'image_path': str(data['image_path']),\n",
    "                    'feature_file': str(feature_file),\n",
    "                    'encoder_name': str(data['encoder_name']),\n",
    "                    'feature_shape': data['features'].shape,\n",
    "                    'feature_dtype': str(data['features'].dtype),\n",
    "                    'file_size': feature_file.stat().st_size,\n",
    "                    'extraction_time': str(data['extraction_time']) if 'extraction_time' in data else None,\n",
    "                    'class_name': Path(data['image_path']).parent.name\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"   ⚠️ Error reading {feature_file}: {e}\")\n",
    "    \n",
    "    if not manifest_data:\n",
    "        print(\"   ❌ No valid feature files found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    manifest_df = pd.DataFrame(manifest_data)\n",
    "    \n",
    "    # Add summary statistics\n",
    "    print(f\"\\n✅ Feature manifest created:\")\n",
    "    print(f\"   📊 Total features: {len(manifest_df):,}\")\n",
    "    print(f\"   🏷️ Classes: {manifest_df['class_name'].nunique()}\")\n",
    "    print(f\"   🗜️ Feature dtype: {manifest_df['feature_dtype'].iloc[0]}\")\n",
    "    print(f\"   📐 Feature shape: {manifest_df['feature_shape'].iloc[0]}\")\n",
    "    print(f\"   💾 Total size: {manifest_df['file_size'].sum() / 1e6:.1f}MB\")\n",
    "    \n",
    "    # Class distribution\n",
    "    class_counts = manifest_df['class_name'].value_counts()\n",
    "    print(f\"\\n📋 Class distribution (top 10):\")\n",
    "    for class_name, count in class_counts.head(10).items():\n",
    "        print(f\"   {class_name}: {count} features\")\n",
    "    \n",
    "    return manifest_df\n",
    "\n",
    "def save_manifest(manifest_df: pd.DataFrame, features_dir: str) -> str:\n",
    "    \"\"\"Save feature manifest to CSV\"\"\"\n",
    "    manifest_file = Path(features_dir) / 'manifest_features.v001.csv'\n",
    "    manifest_df.to_csv(manifest_file, index=False)\n",
    "    \n",
    "    print(f\"💾 Manifest saved: {manifest_file}\")\n",
    "    return str(manifest_file)\n",
    "\n",
    "# Generate manifest if features exist\n",
    "encoder_dir = Path(CONFIG['features_dir']) / f\"encoder_{CONFIG['encoder_name']}\"\n",
    "if encoder_dir.exists():\n",
    "    manifest = create_feature_manifest(CONFIG['features_dir'])\n",
    "    if not manifest.empty:\n",
    "        manifest_file = save_manifest(manifest, CONFIG['features_dir'])\n",
    "        print(f\"✅ Feature pipeline ready for head-only training!\")\n",
    "    else:\n",
    "        print(\"⚠️ No features found - run feature extraction jobs first\")\n",
    "else:\n",
    "    print(f\"📋 Manifest will be created after feature extraction\")\n",
    "    print(f\"Expected location: {CONFIG['features_dir']}/manifest_features.v001.csv\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
