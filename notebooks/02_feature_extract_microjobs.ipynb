{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6e9265",
   "metadata": {},
   "source": [
    "# ğŸ”¥ Micro-Job Feature Extraction Pipeline\n",
    "\n",
    "**Mission**: Eliminate training bottlenecks with resumable feature caching  \n",
    "**Target**: 4GB VRAM, 64 images per job, <2min per job  \n",
    "**Strategy**: EfficientNet-B0 encoder â†’ float16 NPZ cache â†’ head-only training\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ Pipeline Overview\n",
    "\n",
    "1. **Job Queue Creation**: Split dataset into 64-image chunks\n",
    "2. **Feature Extraction**: Process jobs with encoder (batch_size=8)\n",
    "3. **Feature Caching**: Save as `features/encoder_*/img_*.npz` (float16)\n",
    "4. **Manifest Generation**: Create `features/manifest_features.v001.csv`\n",
    "5. **Resume Logic**: Skip completed jobs via `.done` files\n",
    "\n",
    "### ğŸ“Š Resource Targets\n",
    "- **VRAM**: <2.5GB peak (within 4GB constraint)\n",
    "- **Speed**: 64 images in <2 minutes\n",
    "- **Storage**: ~50MB per 1000 images (float16 compression)\n",
    "- **Quality**: Equivalent to full training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7397f3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PIL imported successfully\n",
      "âœ… TIMM imported successfully\n",
      "âœ… Torchvision transforms imported successfully\n",
      "âš ï¸ Project imports failed - continuing without data_utils\n",
      "âš ï¸ Running on CPU - feature extraction will be slower\n",
      "ğŸ”§ PyTorch: 2.8.0+cpu\n",
      "ğŸ“ Working dir: C:\\Users\\MadScie254\\Documents\\GitHub\\Capstone-Lazarus\\notebooks\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ Setup & Imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries - Safe import strategy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Import PIL first to avoid conflicts\n",
    "try:\n",
    "    from PIL import Image\n",
    "    print(\"âœ… PIL imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ PIL import failed: {e}\")\n",
    "    Image = None\n",
    "\n",
    "# Import timm without torchvision conflicts\n",
    "try:\n",
    "    # Bypass torchvision import in timm by setting environment\n",
    "    os.environ['TIMM_FUSED_ATTN'] = '0'\n",
    "    import timm\n",
    "    print(\"âœ… TIMM imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ TIMM import failed: {e}\")\n",
    "    print(\"   This is critical - trying alternative strategy...\")\n",
    "    \n",
    "    # Try importing without problematic torchvision dependencies\n",
    "    try:\n",
    "        import torch.hub\n",
    "        # Load EfficientNet directly from torch hub as fallback\n",
    "        print(\"   Using PyTorch Hub as fallback...\")\n",
    "    except:\n",
    "        print(\"   âŒ All ML library imports failed\")\n",
    "\n",
    "# Import torch utilities\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Try transforms import\n",
    "try:\n",
    "    import torchvision.transforms as transforms\n",
    "    print(\"âœ… Torchvision transforms imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Torchvision transforms failed: {e}\")\n",
    "    print(\"   Using manual transforms as fallback\")\n",
    "    transforms = None\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Project imports\n",
    "sys.path.append('../src')\n",
    "try:\n",
    "    from data_utils import ImageFolderAlb\n",
    "    print(\"âœ… Project imports successful\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ Project imports failed - continuing without data_utils\")\n",
    "\n",
    "# ğŸ® Device & Memory Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"ğŸš€ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "else:\n",
    "    print(\"âš ï¸ Running on CPU - feature extraction will be slower\")\n",
    "\n",
    "print(f\"ğŸ”§ PyTorch: {torch.__version__}\")\n",
    "print(f\"ğŸ“ Working dir: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c42c3c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ MICRO-JOB CONFIGURATION:\n",
      "   ğŸ“Š Job size: 64 images\n",
      "   ğŸ¬ Batch size: 8 (VRAM-safe)\n",
      "   ğŸ“ Image size: 224px\n",
      "   ğŸ—œï¸ Feature dtype: float16\n",
      "   ğŸ—ï¸ Encoder: efficientnet_b0\n"
     ]
    }
   ],
   "source": [
    "# âš™ï¸ Configuration\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'data_dir': '../data',\n",
    "    'features_dir': '../features',\n",
    "    'encoder_name': 'efficientnet_b0',\n",
    "    \n",
    "    # Job settings (4GB VRAM optimized)\n",
    "    'job_size': 64,          # Images per job\n",
    "    'batch_size': 8,         # Processing batch (VRAM constraint)\n",
    "    'img_size': 224,         # Input resolution\n",
    "    'feature_dtype': 'float16',  # Memory compression\n",
    "    \n",
    "    # Feature extraction\n",
    "    'use_global_pool': True,     # Extract global features\n",
    "    'extract_spatial': False,    # Skip spatial for now (head-only training)\n",
    "    'normalize_features': True,  # L2 normalize\n",
    "    \n",
    "    # Performance\n",
    "    'num_workers': 4,        # DataLoader workers\n",
    "    'pin_memory': True,      # GPU transfer optimization\n",
    "    'prefetch_factor': 2,    # Async data loading\n",
    "}\n",
    "\n",
    "print(\"ğŸ¯ MICRO-JOB CONFIGURATION:\")\n",
    "print(f\"   ğŸ“Š Job size: {CONFIG['job_size']} images\")\n",
    "print(f\"   ğŸ¬ Batch size: {CONFIG['batch_size']} (VRAM-safe)\")\n",
    "print(f\"   ğŸ“ Image size: {CONFIG['img_size']}px\")\n",
    "print(f\"   ğŸ—œï¸ Feature dtype: {CONFIG['feature_dtype']}\")\n",
    "print(f\"   ğŸ—ï¸ Encoder: {CONFIG['encoder_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e88835c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Scanning dataset: ../data\n",
      "   ğŸ“ Processing class: Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot\n",
      "   ğŸ“ Processing class: Corn_(maize)___Common_rust_\n",
      "   ğŸ“ Processing class: Corn_(maize)___healthy\n",
      "   ğŸ“ Processing class: Corn_(maize)___Northern_Leaf_Blight\n",
      "   ğŸ“ Processing class: Corn_(maize)___Northern_Leaf_Blight_oversampled\n",
      "   ğŸ“ Processing class: Corn_(maize)___Northern_Leaf_Blight_undersampled\n",
      "   ğŸ“ Processing class: Potato___Early_blight\n",
      "   ğŸ“ Processing class: Potato___healthy\n",
      "   ğŸ“ Processing class: Potato___Late_blight\n",
      "   ğŸ“ Processing class: Tomato___Bacterial_spot\n",
      "   ğŸ“ Processing class: Tomato___Early_blight\n",
      "   ğŸ“ Processing class: Tomato___healthy\n",
      "   ğŸ“ Processing class: Tomato___Late_blight\n",
      "   ğŸ“ Processing class: Tomato___Leaf_Mold\n",
      "   ğŸ“ Processing class: Tomato___Septoria_leaf_spot\n",
      "   ğŸ“ Processing class: Tomato___Spider_mites Two-spotted_spider_mite\n",
      "   ğŸ“ Processing class: Tomato___Target_Spot\n",
      "   ğŸ“ Processing class: Tomato___Tomato_mosaic_virus\n",
      "   ğŸ“ Processing class: Tomato___Tomato_Yellow_Leaf_Curl_Virus\n",
      "\n",
      "âœ… Dataset scan complete:\n",
      "   ğŸ–¼ï¸ Total images: 26,134\n",
      "   ğŸ·ï¸ Classes: 19\n",
      "   ğŸ’¾ Total size: 0.42GB\n",
      "\n",
      "ğŸ“‹ Creating job queue (job_size=64)...\n",
      "âœ… Job queue created: 409 jobs\n",
      "   ğŸ“Š Average job size: 63.9 images\n",
      "   ğŸ¯ Estimated time: 818 minutes (2min/job)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Š Dataset Scanning & Job Queue Creation\n",
    "\n",
    "def scan_dataset(data_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"Scan dataset and create image manifest\"\"\"\n",
    "    print(f\"ğŸ” Scanning dataset: {data_dir}\")\n",
    "    \n",
    "    data_path = Path(data_dir)\n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(f\"Data directory not found: {data_dir}\")\n",
    "    \n",
    "    # Collect all images\n",
    "    images = []\n",
    "    for class_dir in data_path.iterdir():\n",
    "        if not class_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        class_name = class_dir.name\n",
    "        print(f\"   ğŸ“ Processing class: {class_name}\")\n",
    "        \n",
    "        for img_file in class_dir.glob('*'):\n",
    "            if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
    "                images.append({\n",
    "                    'image_path': str(img_file),\n",
    "                    'class_name': class_name,\n",
    "                    'image_id': f\"{class_name}_{img_file.stem}\",\n",
    "                    'file_size': img_file.stat().st_size\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(images)\n",
    "    print(f\"\\nâœ… Dataset scan complete:\")\n",
    "    print(f\"   ğŸ–¼ï¸ Total images: {len(df):,}\")\n",
    "    print(f\"   ğŸ·ï¸ Classes: {df['class_name'].nunique()}\")\n",
    "    print(f\"   ğŸ’¾ Total size: {df['file_size'].sum() / 1e9:.2f}GB\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_job_queue(image_df: pd.DataFrame, job_size: int = 64) -> pd.DataFrame:\n",
    "    \"\"\"Split images into job chunks for micro-job processing\"\"\"\n",
    "    print(f\"\\nğŸ“‹ Creating job queue (job_size={job_size})...\")\n",
    "    \n",
    "    # Shuffle for balanced jobs across classes\n",
    "    shuffled_df = image_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Create job chunks\n",
    "    jobs = []\n",
    "    for i in range(0, len(shuffled_df), job_size):\n",
    "        job_images = shuffled_df.iloc[i:i+job_size]\n",
    "        \n",
    "        jobs.append({\n",
    "            'job_id': len(jobs),\n",
    "            'image_paths': ','.join(job_images['image_path'].tolist()),\n",
    "            'image_ids': ','.join(job_images['image_id'].tolist()),\n",
    "            'num_images': len(job_images),\n",
    "            'classes': ','.join(job_images['class_name'].unique()),\n",
    "            'status': 'pending',\n",
    "            'created_at': datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    job_df = pd.DataFrame(jobs)\n",
    "    print(f\"âœ… Job queue created: {len(job_df)} jobs\")\n",
    "    print(f\"   ğŸ“Š Average job size: {job_df['num_images'].mean():.1f} images\")\n",
    "    print(f\"   ğŸ¯ Estimated time: {len(job_df) * 2:.0f} minutes (2min/job)\")\n",
    "    \n",
    "    return job_df\n",
    "\n",
    "# Execute dataset scanning\n",
    "image_manifest = scan_dataset(CONFIG['data_dir'])\n",
    "job_queue = create_job_queue(image_manifest, CONFIG['job_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1b1d6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—ï¸ Feature extractor: efficientnet_b0\n",
      "   ğŸ“ Feature dim: 1280\n",
      "   ğŸ’¾ Parameters: 4,007,548\n",
      "âœ… Feature extraction setup complete\n",
      "   ğŸ¯ Ready for 64-image micro-jobs\n"
     ]
    }
   ],
   "source": [
    "# ğŸ—ï¸ Feature Extraction Setup\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"Lightweight feature extractor with global pooling\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder_name: str = 'efficientnet_b0', pretrained: bool = True):\n",
    "        super().__init__()\n",
    "        self.encoder_name = encoder_name\n",
    "        \n",
    "        # Load pretrained encoder\n",
    "        self.backbone = timm.create_model(\n",
    "            encoder_name, \n",
    "            pretrained=pretrained,\n",
    "            num_classes=0,  # Remove classifier head\n",
    "            global_pool='avg'  # Global average pooling\n",
    "        )\n",
    "        \n",
    "        # Get feature dimensions\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 224, 224)\n",
    "            dummy_output = self.backbone(dummy_input)\n",
    "            self.feature_dim = dummy_output.shape[1]\n",
    "        \n",
    "        print(f\"ğŸ—ï¸ Feature extractor: {encoder_name}\")\n",
    "        print(f\"   ğŸ“ Feature dim: {self.feature_dim}\")\n",
    "        print(f\"   ğŸ’¾ Parameters: {sum(p.numel() for p in self.parameters()):,}\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Extract global features\"\"\"\n",
    "        features = self.backbone(x)  # [B, feature_dim]\n",
    "        return features\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Simple dataset for feature extraction\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths: List[str], transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error loading {img_path}: {e}\")\n",
    "            # Return black image as fallback\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, img_path\n",
    "\n",
    "# Initialize feature extractor\n",
    "feature_extractor = FeatureExtractor(CONFIG['encoder_name']).to(device)\n",
    "feature_extractor.eval()\n",
    "\n",
    "# Define transforms (minimal - just resize & normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(f\"âœ… Feature extraction setup complete\")\n",
    "print(f\"   ğŸ¯ Ready for {CONFIG['job_size']}-image micro-jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dfb3a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ Job execution function ready\n",
      "   Usage: run_feature_job(job_id, job_queue)\n",
      "   Target: <2 minutes per 64-image job\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¥ Core Job Execution Function\n",
    "\n",
    "def run_feature_job(job_id: int, job_queue: pd.DataFrame, force_rerun: bool = False) -> bool:\n",
    "    \"\"\"Execute single feature extraction job\"\"\"\n",
    "    \n",
    "    if job_id >= len(job_queue):\n",
    "        print(f\"âŒ Job ID {job_id} out of range (max: {len(job_queue)-1})\")\n",
    "        return False\n",
    "    \n",
    "    job = job_queue.iloc[job_id]\n",
    "    \n",
    "    # Create output directories\n",
    "    features_dir = Path(CONFIG['features_dir'])\n",
    "    encoder_dir = features_dir / f\"encoder_{CONFIG['encoder_name']}\"\n",
    "    encoder_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Check if job already completed\n",
    "    done_file = features_dir / f\"_job_{job_id:04d}_{int(time.time())}.done\"\n",
    "    existing_done = list(features_dir.glob(f\"_job_{job_id:04d}_*.done\"))\n",
    "    \n",
    "    if existing_done and not force_rerun:\n",
    "        print(f\"âœ… Job {job_id} already completed: {existing_done[0].name}\")\n",
    "        return True\n",
    "    \n",
    "    print(f\"\\nğŸš€ Starting job {job_id}/{len(job_queue)-1}\")\n",
    "    print(f\"   ğŸ“Š Images: {job['num_images']}\")\n",
    "    print(f\"   ğŸ·ï¸ Classes: {job['classes']}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Parse image paths\n",
    "        image_paths = job['image_paths'].split(',')\n",
    "        image_ids = job['image_ids'].split(',')\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        dataset = ImageDataset(image_paths, transform=transform)\n",
    "        dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=CONFIG['num_workers'],\n",
    "            pin_memory=CONFIG['pin_memory'],\n",
    "            prefetch_factor=CONFIG['prefetch_factor']\n",
    "        )\n",
    "        \n",
    "        # Extract features\n",
    "        all_features = []\n",
    "        all_paths = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_images, batch_paths in tqdm(dataloader, \n",
    "                                                 desc=f\"Job {job_id}\", \n",
    "                                                 leave=False):\n",
    "                batch_images = batch_images.to(device, non_blocking=True)\n",
    "                \n",
    "                # Extract features\n",
    "                features = feature_extractor(batch_images)  # [B, feature_dim]\n",
    "                \n",
    "                # Normalize if requested\n",
    "                if CONFIG['normalize_features']:\n",
    "                    features = torch.nn.functional.normalize(features, p=2, dim=1)\n",
    "                \n",
    "                # Convert to numpy and compress to float16\n",
    "                features_np = features.cpu().numpy().astype(CONFIG['feature_dtype'])\n",
    "                \n",
    "                all_features.append(features_np)\n",
    "                all_paths.extend(batch_paths)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        all_features = np.concatenate(all_features, axis=0)\n",
    "        \n",
    "        print(f\"   âœ… Extracted: {all_features.shape} features\")\n",
    "        \n",
    "        # Save features individually\n",
    "        saved_count = 0\n",
    "        for i, (img_path, img_id) in enumerate(zip(all_paths, image_ids)):\n",
    "            feature_file = encoder_dir / f\"{img_id}.npz\"\n",
    "            \n",
    "            np.savez_compressed(\n",
    "                feature_file,\n",
    "                features=all_features[i],\n",
    "                image_path=img_path,\n",
    "                image_id=img_id,\n",
    "                encoder_name=CONFIG['encoder_name'],\n",
    "                extraction_time=datetime.now().isoformat()\n",
    "            )\n",
    "            saved_count += 1\n",
    "        \n",
    "        # Create completion marker\n",
    "        job_metadata = {\n",
    "            'job_id': job_id,\n",
    "            'num_images': len(image_paths),\n",
    "            'feature_shape': list(all_features.shape),\n",
    "            'processing_time': time.time() - start_time,\n",
    "            'encoder_name': CONFIG['encoder_name'],\n",
    "            'config': CONFIG,\n",
    "            'completed_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(done_file, 'w') as f:\n",
    "            json.dump(job_metadata, f, indent=2)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"âœ… Job {job_id} completed: {saved_count} features saved in {elapsed:.1f}s\")\n",
    "        print(f\"   ğŸ’¾ Output: {encoder_dir}/\")\n",
    "        print(f\"   ğŸ Done marker: {done_file.name}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Job {job_id} failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "print(\"ğŸ”¥ Job execution function ready\")\n",
    "print(\"   Usage: run_feature_job(job_id, job_queue)\")\n",
    "print(\"   Target: <2 minutes per 64-image job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6527c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing job execution with job 0\n",
      "Expected: 64 images processed\n",
      "\n",
      "ğŸš€ Starting job 0/408\n",
      "   ğŸ“Š Images: 64\n",
      "   ğŸ·ï¸ Classes: Tomato___Tomato_Yellow_Leaf_Curl_Virus,Potato___Late_blight,Corn_(maize)___Northern_Leaf_Blight,Tomato___Spider_mites Two-spotted_spider_mite,Tomato___Late_blight,Tomato___Leaf_Mold,Corn_(maize)___Northern_Leaf_Blight_oversampled,Tomato___Septoria_leaf_spot,Tomato___Early_blight,Tomato___healthy,Tomato___Bacterial_spot,Potato___Early_blight,Corn_(maize)___healthy,Potato___healthy,Corn_(maize)___Northern_Leaf_Blight_undersampled,Tomato___Target_Spot\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84498ff897ff4fa4bc82ff92677ec55a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Job 0:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Job 0 failed: DataLoader worker (pid(s) 9920, 19672, 19448, 6528) exited unexpectedly\n",
      "âŒ Test job failed - check error messages above\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\MadScie254\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1285, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MadScie254\\anaconda3\\envs\\ml_env\\Lib\\multiprocessing\\queues.py\", line 114, in get\n",
      "    raise Empty\n",
      "_queue.Empty\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\MadScie254\\AppData\\Local\\Temp\\ipykernel_14040\\1345036200.py\", line 52, in run_feature_job\n",
      "    for batch_images, batch_paths in tqdm(dataloader,\n",
      "  File \"C:\\Users\\MadScie254\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\tqdm\\notebook.py\", line 250, in __iter__\n",
      "    for obj in it:\n",
      "  File \"C:\\Users\\MadScie254\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\tqdm\\std.py\", line 1181, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"C:\\Users\\MadScie254\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 734, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MadScie254\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1492, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "                ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MadScie254\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1454, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "                    ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MadScie254\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1298, in _try_get_data\n",
      "    raise RuntimeError(\n",
      "RuntimeError: DataLoader worker (pid(s) 9920, 19672, 19448, 6528) exited unexpectedly\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§ª TEST: Single Job Execution\n",
    "# Run this cell to test the pipeline with job 0\n",
    "\n",
    "TEST_JOB_ID = 0\n",
    "\n",
    "print(f\"ğŸ§ª Testing job execution with job {TEST_JOB_ID}\")\n",
    "print(f\"Expected: {job_queue.iloc[TEST_JOB_ID]['num_images']} images processed\")\n",
    "\n",
    "# Clear GPU memory before test\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"ğŸ§¹ GPU memory cleared\")\n",
    "\n",
    "# Run test job\n",
    "success = run_feature_job(TEST_JOB_ID, job_queue, force_rerun=True)\n",
    "\n",
    "if success:\n",
    "    # Verify outputs\n",
    "    encoder_dir = Path(CONFIG['features_dir']) / f\"encoder_{CONFIG['encoder_name']}\"\n",
    "    feature_files = list(encoder_dir.glob('*.npz'))\n",
    "    done_files = list(Path(CONFIG['features_dir']).glob(f'_job_{TEST_JOB_ID:04d}_*.done'))\n",
    "    \n",
    "    print(f\"\\nâœ… TEST RESULTS:\")\n",
    "    print(f\"   ğŸ“ Feature files created: {len(feature_files)}\")\n",
    "    print(f\"   ğŸ Done files created: {len(done_files)}\")\n",
    "    \n",
    "    # Test loading a feature file\n",
    "    if feature_files:\n",
    "        test_feature = np.load(feature_files[0])\n",
    "        print(f\"   ğŸ§ª Sample feature shape: {test_feature['features'].shape}\")\n",
    "        print(f\"   ğŸ—œï¸ Feature dtype: {test_feature['features'].dtype}\")\n",
    "        \n",
    "        # Check memory usage\n",
    "        feature_size = test_feature['features'].nbytes\n",
    "        total_estimated = feature_size * len(image_manifest) / 1e6\n",
    "        print(f\"   ğŸ’¾ Per-feature size: {feature_size} bytes\")\n",
    "        print(f\"   ğŸ“Š Estimated total: {total_estimated:.1f}MB for full dataset\")\n",
    "        \n",
    "    print(f\"\\nğŸ¯ Test job completed successfully!\")\n",
    "else:\n",
    "    print(f\"âŒ Test job failed - check error messages above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669221a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ­ Batch Job Execution (Full Pipeline)\n",
    "# WARNING: This will process ALL jobs - use for full feature extraction\n",
    "\n",
    "def run_all_jobs(job_queue: pd.DataFrame, max_jobs: int = None, \n",
    "                 start_job: int = 0) -> Dict:\n",
    "    \"\"\"Execute all feature extraction jobs with progress tracking\"\"\"\n",
    "    \n",
    "    total_jobs = len(job_queue)\n",
    "    if max_jobs:\n",
    "        total_jobs = min(total_jobs, max_jobs)\n",
    "    \n",
    "    print(f\"ğŸ­ BATCH JOB EXECUTION\")\n",
    "    print(f\"   ğŸ“Š Total jobs: {total_jobs}\")\n",
    "    print(f\"   ğŸ¯ Estimated time: {total_jobs * 2:.0f} minutes\")\n",
    "    print(f\"   ğŸ’¾ Estimated storage: {total_jobs * CONFIG['job_size'] * 0.05:.1f}MB\")\n",
    "    \n",
    "    results = {\n",
    "        'completed_jobs': [],\n",
    "        'failed_jobs': [],\n",
    "        'total_time': 0,\n",
    "        'total_features': 0\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for job_id in tqdm(range(start_job, min(start_job + total_jobs, len(job_queue))), \n",
    "                       desc=\"Processing jobs\"):\n",
    "        \n",
    "        job_start = time.time()\n",
    "        success = run_feature_job(job_id, job_queue)\n",
    "        job_time = time.time() - job_start\n",
    "        \n",
    "        if success:\n",
    "            results['completed_jobs'].append({\n",
    "                'job_id': job_id,\n",
    "                'time': job_time,\n",
    "                'images': job_queue.iloc[job_id]['num_images']\n",
    "            })\n",
    "            results['total_features'] += job_queue.iloc[job_id]['num_images']\n",
    "        else:\n",
    "            results['failed_jobs'].append(job_id)\n",
    "        \n",
    "        # Clear GPU memory periodically\n",
    "        if job_id % 10 == 0 and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    results['total_time'] = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nğŸ BATCH EXECUTION COMPLETE\")\n",
    "    print(f\"   âœ… Completed: {len(results['completed_jobs'])}/{total_jobs} jobs\")\n",
    "    print(f\"   âŒ Failed: {len(results['failed_jobs'])} jobs\")\n",
    "    print(f\"   â±ï¸ Total time: {results['total_time']/60:.1f} minutes\")\n",
    "    print(f\"   ğŸ–¼ï¸ Total features: {results['total_features']:,}\")\n",
    "    \n",
    "    if results['completed_jobs']:\n",
    "        avg_time = np.mean([j['time'] for j in results['completed_jobs']])\n",
    "        print(f\"   ğŸ“Š Average job time: {avg_time:.1f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# COMMENTED OUT - UNCOMMENT TO RUN FULL EXTRACTION\n",
    "# This will process all jobs and may take hours!\n",
    "\n",
    "# results = run_all_jobs(job_queue, max_jobs=5)  # Test with 5 jobs first\n",
    "\n",
    "print(\"âš ï¸ Batch execution commented out for safety\")\n",
    "print(\"Uncomment and modify max_jobs parameter to run full extraction\")\n",
    "print(f\"Total jobs available: {len(job_queue)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e0864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“Š Feature Manifest Generation\n",
    "\n",
    "def create_feature_manifest(features_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"Create comprehensive manifest of extracted features\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ“Š Creating feature manifest from {features_dir}\")\n",
    "    \n",
    "    features_path = Path(features_dir)\n",
    "    encoder_dir = features_path / f\"encoder_{CONFIG['encoder_name']}\"\n",
    "    \n",
    "    if not encoder_dir.exists():\n",
    "        print(f\"âš ï¸ Encoder directory not found: {encoder_dir}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Collect all feature files\n",
    "    feature_files = list(encoder_dir.glob('*.npz'))\n",
    "    print(f\"   ğŸ“ Found {len(feature_files)} feature files\")\n",
    "    \n",
    "    if not feature_files:\n",
    "        print(\"   âš ï¸ No feature files found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    manifest_data = []\n",
    "    \n",
    "    for feature_file in tqdm(feature_files, desc=\"Building manifest\"):\n",
    "        try:\n",
    "            # Load metadata without loading full features\n",
    "            with np.load(feature_file) as data:\n",
    "                manifest_data.append({\n",
    "                    'image_id': str(data['image_id']),\n",
    "                    'image_path': str(data['image_path']),\n",
    "                    'feature_file': str(feature_file),\n",
    "                    'encoder_name': str(data['encoder_name']),\n",
    "                    'feature_shape': data['features'].shape,\n",
    "                    'feature_dtype': str(data['features'].dtype),\n",
    "                    'file_size': feature_file.stat().st_size,\n",
    "                    'extraction_time': str(data['extraction_time']) if 'extraction_time' in data else None,\n",
    "                    'class_name': Path(data['image_path']).parent.name\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"   âš ï¸ Error reading {feature_file}: {e}\")\n",
    "    \n",
    "    if not manifest_data:\n",
    "        print(\"   âŒ No valid feature files found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    manifest_df = pd.DataFrame(manifest_data)\n",
    "    \n",
    "    # Add summary statistics\n",
    "    print(f\"\\nâœ… Feature manifest created:\")\n",
    "    print(f\"   ğŸ“Š Total features: {len(manifest_df):,}\")\n",
    "    print(f\"   ğŸ·ï¸ Classes: {manifest_df['class_name'].nunique()}\")\n",
    "    print(f\"   ğŸ—œï¸ Feature dtype: {manifest_df['feature_dtype'].iloc[0]}\")\n",
    "    print(f\"   ğŸ“ Feature shape: {manifest_df['feature_shape'].iloc[0]}\")\n",
    "    print(f\"   ğŸ’¾ Total size: {manifest_df['file_size'].sum() / 1e6:.1f}MB\")\n",
    "    \n",
    "    # Class distribution\n",
    "    class_counts = manifest_df['class_name'].value_counts()\n",
    "    print(f\"\\nğŸ“‹ Class distribution (top 10):\")\n",
    "    for class_name, count in class_counts.head(10).items():\n",
    "        print(f\"   {class_name}: {count} features\")\n",
    "    \n",
    "    return manifest_df\n",
    "\n",
    "def save_manifest(manifest_df: pd.DataFrame, features_dir: str) -> str:\n",
    "    \"\"\"Save feature manifest to CSV\"\"\"\n",
    "    manifest_file = Path(features_dir) / 'manifest_features.v001.csv'\n",
    "    manifest_df.to_csv(manifest_file, index=False)\n",
    "    \n",
    "    print(f\"ğŸ’¾ Manifest saved: {manifest_file}\")\n",
    "    return str(manifest_file)\n",
    "\n",
    "# Generate manifest if features exist\n",
    "encoder_dir = Path(CONFIG['features_dir']) / f\"encoder_{CONFIG['encoder_name']}\"\n",
    "if encoder_dir.exists():\n",
    "    manifest = create_feature_manifest(CONFIG['features_dir'])\n",
    "    if not manifest.empty:\n",
    "        manifest_file = save_manifest(manifest, CONFIG['features_dir'])\n",
    "        print(f\"âœ… Feature pipeline ready for head-only training!\")\n",
    "    else:\n",
    "        print(\"âš ï¸ No features found - run feature extraction jobs first\")\n",
    "else:\n",
    "    print(f\"ğŸ“‹ Manifest will be created after feature extraction\")\n",
    "    print(f\"Expected location: {CONFIG['features_dir']}/manifest_features.v001.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
