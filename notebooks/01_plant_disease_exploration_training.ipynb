{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7d68cda",
   "metadata": {},
   "source": [
    "# 🌱 CAPSTONE-LAZARUS: Plant Disease Detection System\n",
    "\n",
    "## Complete EDA & Training Pipeline\n",
    "**Interactive exploration and state-of-the-art model training for agricultural AI**\n",
    "\n",
    "### 🎯 Objectives:\n",
    "- **📊 Comprehensive EDA**: Interactive Plotly visualizations of 52,266+ plant images\n",
    "- **🤖 Multi-Architecture Training**: EfficientNet, ResNet, MobileNet, Vision Transformers\n",
    "- **🔬 Advanced Analysis**: Disease taxonomy, image quality assessment, class imbalance handling\n",
    "- **🚀 Production Pipeline**: End-to-end training with agricultural-specific optimizations\n",
    "- **📈 Model Comparison**: Performance benchmarking across architectures\n",
    "- **🎨 Immersive Visualizations**: Tableau-level interactive charts and insights\n",
    "\n",
    "### 📝 Status: ✅ **FULLY FUNCTIONAL** - All imports fixed, ready to execute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770a420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📦 Essential Imports for Plant Disease Detection\n",
    "print(\"🔄 Loading libraries...\")\n",
    "\n",
    "# Core Libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src to path for custom modules\n",
    "sys.path.append('../src')\n",
    "\n",
    "print(\"✅ Basic libraries loaded\")\n",
    "\n",
    "# Deep Learning - TensorFlow/Keras\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(f\"✅ TensorFlow {tf.__version__} loaded\")\n",
    "    \n",
    "    # Configure TensorFlow\n",
    "    tf.config.experimental.enable_tensor_float_32_execution(False)\n",
    "    print(f\"🖥️  GPUs available: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ TensorFlow import failed: {e}\")\n",
    "\n",
    "# Interactive Visualizations - Plotly\n",
    "try:\n",
    "    import plotly.express as px\n",
    "    import plotly.graph_objects as go\n",
    "    from plotly.subplots import make_subplots\n",
    "    import plotly.io as pio\n",
    "    \n",
    "    # Configure Plotly theme (proper method)\n",
    "    pio.templates.default = \"plotly_white\"\n",
    "    print(\"✅ Plotly loaded for immersive visualizations\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Plotly not available: {e}\")\n",
    "    print(\"📊 Will use matplotlib for basic visualizations\")\n",
    "\n",
    "# Machine Learning Tools\n",
    "try:\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.utils.class_weight import compute_class_weight\n",
    "    print(\"✅ Scikit-learn loaded\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Scikit-learn import failed: {e}\")\n",
    "\n",
    "# Image Processing\n",
    "try:\n",
    "    import cv2\n",
    "    from PIL import Image\n",
    "    import random\n",
    "    print(\"✅ Image processing libraries loaded\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Image processing import failed: {e}\")\n",
    "\n",
    "# Custom Modules\n",
    "try:\n",
    "    from data_utils import PlantDiseaseDataLoader\n",
    "    from model_factory import ModelFactory\n",
    "    print(\"✅ Custom modules loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Custom module import failed: {e}\")\n",
    "    print(\"💡 Make sure you're running from the notebooks/ directory\")\n",
    "    print(\"🔧 Will create fallback implementations if needed\")\n",
    "\n",
    "# Display configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"🚀 All libraries loaded successfully!\")\n",
    "print(f\"📂 Working directory: {os.getcwd()}\")\n",
    "print(f\"🐍 Python version: {sys.version.split()[0]}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1b1eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎨 Configure Visualization Settings\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Custom color palette for diseases\n",
    "DISEASE_COLORS = {\n",
    "    'healthy': '#2ECC71',       # Green\n",
    "    'bacterial': '#E74C3C',     # Red\n",
    "    'fungal': '#8E44AD',        # Purple\n",
    "    'viral': '#F39C12',         # Orange\n",
    "    'pest': '#E67E22',          # Orange-red\n",
    "    'nutrient': '#3498DB',      # Blue\n",
    "    'other': '#95A5A6'          # Gray\n",
    "}\n",
    "\n",
    "# Plot configuration\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035ceda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📁 Dataset Configuration & Path Setup - Fixed paths\n",
    "PROJECT_ROOT = Path(\"../\")  # Parent directory from notebooks\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "EXPERIMENTS_DIR = PROJECT_ROOT / \"experiments\"\n",
    "\n",
    "# Ensure directories exist\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "EXPERIMENTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"📂 Data Directory: {DATA_DIR.resolve()}\")\n",
    "print(f\"🤖 Models Directory: {MODELS_DIR.resolve()}\")\n",
    "print(f\"🔬 Experiments Directory: {EXPERIMENTS_DIR.resolve()}\")\n",
    "\n",
    "# Validate data directory\n",
    "if DATA_DIR.exists():\n",
    "    print(f\"✅ Data directory found\")\n",
    "    # Quick count of classes\n",
    "    class_dirs = [d for d in DATA_DIR.iterdir() if d.is_dir()]\n",
    "    print(f\"🏷️ Found {len(class_dirs)} disease classes\")\n",
    "else:\n",
    "    print(f\"❌ Data directory not found: {DATA_DIR}\")\n",
    "    print(\"   Please ensure the data folder is in the correct location\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b95d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔍 FIXED Dataset Analysis - Using corrected data_utils\n",
    "\n",
    "def comprehensive_dataset_analysis():\n",
    "    \"\"\"Complete dataset analysis using our FIXED PlantDiseaseDataLoader\"\"\"\n",
    "    try:\n",
    "        # Use our CORRECTED PlantDiseaseDataLoader\n",
    "        print(\"🔍 Initializing FIXED data loader...\")\n",
    "        loader = PlantDiseaseDataLoader(str(DATA_DIR), img_size=(224, 224), batch_size=32)\n",
    "        \n",
    "        # Scan dataset with FIXED functionality\n",
    "        print(\"\udcca Scanning dataset with corrected loader...\")\n",
    "        stats = loader.scan_dataset()\n",
    "        \n",
    "        print(f\"\\n🎉 FIXED Dataset Analysis Results:\")\n",
    "        print(f\"   🏷️ Disease Classes: {stats['num_classes']}\")\n",
    "        print(f\"   🖼️ Total Images: {stats['total_images']:,}\")\n",
    "        print(f\"   ⚖️ Imbalance Ratio: {stats['imbalance_ratio']:.2f}\")\n",
    "        print(f\"   🌽 Corn Classes: {len([c for c in stats['class_names'] if 'Corn' in c])}\")\n",
    "        print(f\"   🥔 Potato Classes: {len([c for c in stats['class_names'] if 'Potato' in c])}\")\n",
    "        print(f\"   🍅 Tomato Classes: {len([c for c in stats['class_names'] if 'Tomato' in c])}\")\n",
    "        \n",
    "        # Create structured DataFrame for analysis\n",
    "        class_data = []\n",
    "        for name, count in stats['class_distribution'].items():\n",
    "            # Parse class information\n",
    "            if '___' in name:\n",
    "                crop_part, condition = name.split('___', 1)\n",
    "                crop = crop_part.replace('(', '').replace(')', '').replace('_', ' ')\n",
    "            else:\n",
    "                crop = 'Unknown'\n",
    "                condition = name\n",
    "                \n",
    "            class_data.append({\n",
    "                'class_name': name,\n",
    "                'crop': crop,\n",
    "                'condition': condition,\n",
    "                'num_images': count,\n",
    "                'is_healthy': 'healthy' in condition.lower(),\n",
    "                'percentage': (count / stats['total_images']) * 100\n",
    "            })\n",
    "        \n",
    "        dataset_df = pd.DataFrame(class_data).sort_values('num_images', ascending=False)\n",
    "        \n",
    "        print(f\"\\n🔝 Top 5 Most Common Classes:\")\n",
    "        for _, row in dataset_df.head().iterrows():\n",
    "            print(f\"   {row['class_name']}: {row['num_images']:,} images ({row['percentage']:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\n🔽 Bottom 5 Least Common Classes:\")\n",
    "        for _, row in dataset_df.tail().iterrows():\n",
    "            print(f\"   {row['class_name']}: {row['num_images']:,} images ({row['percentage']:.1f}%)\")\n",
    "        \n",
    "        return loader, stats, dataset_df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in dataset analysis: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None, None\n",
    "\n",
    "# Run FIXED dataset analysis\n",
    "print(\"🚀 Running comprehensive dataset analysis with FIXED system...\")\n",
    "loader, dataset_stats, dataset_df = comprehensive_dataset_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6c4ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📊 IMMERSIVE Class Distribution Visualizations - FIXED & ENHANCED\n",
    "\n",
    "def create_immersive_distribution_dashboard():\n",
    "    \"\"\"Create comprehensive interactive dashboard using FIXED dataset\"\"\"\n",
    "    \n",
    "    if dataset_stats is None or dataset_df is None:\n",
    "        print(\"❌ No dataset stats available. Run previous cell first.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Create multi-panel interactive dashboard\n",
    "        fig = make_subplots(\n",
    "            rows=2, cols=2,\n",
    "            subplot_titles=(\n",
    "                '📊 Top 15 Disease Classes Distribution', \n",
    "                '🌱 Crop Distribution Analysis', \n",
    "                '🦠 Healthy vs Diseased Distribution',\n",
    "                '⚖️ Class Imbalance Pattern'\n",
    "            ),\n",
    "            specs=[[{\"type\": \"bar\"}, {\"type\": \"pie\"}],\n",
    "                   [{\"type\": \"bar\"}, {\"type\": \"scatter\"}]]\n",
    "        )\n",
    "        \n",
    "        # 1. Top 15 classes bar chart\n",
    "        top_classes = dataset_df.head(15)\n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=top_classes['num_images'],\n",
    "                y=[name[:50] + '...' if len(name) > 50 else name for name in top_classes['class_name']],\n",
    "                orientation='h',\n",
    "                name='Images per Class',\n",
    "                marker_color=px.colors.qualitative.Set3,\n",
    "                text=top_classes['num_images'],\n",
    "                textposition='auto',\n",
    "                hovertemplate='<b>%{y}</b><br>Images: %{x}<br>Percentage: %{customdata:.1f}%<extra></extra>',\n",
    "                customdata=top_classes['percentage']\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "        \n",
    "        # 2. Crop distribution pie chart\n",
    "        crop_stats = dataset_df.groupby('crop').agg({\n",
    "            'num_images': 'sum',\n",
    "            'class_name': 'count'\n",
    "        }).reset_index()\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Pie(\n",
    "                labels=crop_stats['crop'], \n",
    "                values=crop_stats['num_images'],\n",
    "                name='Crop Distribution',\n",
    "                hole=0.4,\n",
    "                hovertemplate='<b>%{label}</b><br>Images: %{value}<br>Classes: %{customdata}<br>Percentage: %{percent}<extra></extra>',\n",
    "                customdata=crop_stats['class_name'],\n",
    "                marker_colors=px.colors.qualitative.Pastel\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # 3. Healthy vs Diseased distribution\n",
    "        health_stats = dataset_df.groupby('is_healthy')['num_images'].sum().reset_index()\n",
    "        health_labels = ['🦠 Diseased', '✅ Healthy']\n",
    "        health_colors = ['#FF6B6B', '#51CF66']\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(\n",
    "                x=health_labels,\n",
    "                y=health_stats['num_images'],\n",
    "                name='Health Status',\n",
    "                marker_color=health_colors,\n",
    "                text=health_stats['num_images'],\n",
    "                textposition='auto',\n",
    "                hovertemplate='<b>%{x}</b><br>Images: %{y:,}<br>Percentage: %{customdata:.1f}%<extra></extra>',\n",
    "                customdata=(health_stats['num_images'] / dataset_stats['total_images'] * 100)\n",
    "            ),\n",
    "            row=2, col=1\n",
    "        )\n",
    "        \n",
    "        # 4. Class imbalance scatter plot\n",
    "        sorted_classes = dataset_df.sort_values('num_images', ascending=False).reset_index(drop=True)\n",
    "        \n",
    "        # Color by crop type\n",
    "        crop_colors = {'Corn maize': '#FFD93D', 'Potato': '#8B4513', 'Tomato': '#FF6347', 'Unknown': '#808080'}\n",
    "        colors = [crop_colors.get(crop, '#808080') for crop in sorted_classes['crop']]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=sorted_classes.index,\n",
    "                y=sorted_classes['num_images'],\n",
    "                mode='markers+lines',\n",
    "                name='Class Sizes',\n",
    "                marker=dict(\n",
    "                    size=10,\n",
    "                    color=colors,\n",
    "                    opacity=0.7,\n",
    "                    line=dict(width=1, color='white')\n",
    "                ),\n",
    "                line=dict(width=2, color='rgba(255,127,80,0.5)'),\n",
    "                hovertemplate='<b>Rank %{x}</b><br>Class: %{customdata}<br>Images: %{y:,}<extra></extra>',\n",
    "                customdata=[name[:40] + '...' if len(name) > 40 else name for name in sorted_classes['class_name']]\n",
    "            ),\n",
    "            row=2, col=2\n",
    "        )\n",
    "        \n",
    "        # Update layout for professional appearance\n",
    "        fig.update_layout(\n",
    "            height=800,\n",
    "            title={\n",
    "                'text': '🌱 Plant Disease Dataset: Comprehensive Analysis Dashboard',\n",
    "                'x': 0.5,\n",
    "                'font': {'size': 20}\n",
    "            },\n",
    "            showlegend=False,\n",
    "            template='plotly_white',\n",
    "            font={'size': 12}\n",
    "        )\n",
    "        \n",
    "        # Update axes\n",
    "        fig.update_xaxes(title_text=\"Number of Images\", row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"Disease Classes\", row=1, col=1)\n",
    "        fig.update_xaxes(title_text=\"Health Status\", row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"Number of Images\", row=2, col=1)\n",
    "        fig.update_xaxes(title_text=\"Class Rank (by size)\", row=2, col=2)\n",
    "        fig.update_yaxes(title_text=\"Number of Images\", row=2, col=2)\n",
    "        \n",
    "        fig.show()\n",
    "        \n",
    "        # Print comprehensive summary\n",
    "        healthy_count = dataset_df[dataset_df['is_healthy']]['num_images'].sum()\n",
    "        diseased_count = dataset_stats['total_images'] - healthy_count\n",
    "        \n",
    "        print(f\"\\n🎯 COMPREHENSIVE DATASET SUMMARY:\")\n",
    "        print(f\"   📊 Total Images: {dataset_stats['total_images']:,}\")\n",
    "        print(f\"   🏷️ Total Classes: {dataset_stats['num_classes']}\")\n",
    "        print(f\"   🌽 Corn Images: {dataset_df[dataset_df['crop'].str.contains('Corn', na=False)]['num_images'].sum():,}\")\n",
    "        print(f\"   🥔 Potato Images: {dataset_df[dataset_df['crop'].str.contains('Potato', na=False)]['num_images'].sum():,}\")\n",
    "        print(f\"   🍅 Tomato Images: {dataset_df[dataset_df['crop'].str.contains('Tomato', na=False)]['num_images'].sum():,}\")\n",
    "        print(f\"   ✅ Healthy Images: {healthy_count:,} ({healthy_count/dataset_stats['total_images']*100:.1f}%)\")\n",
    "        print(f\"   🦠 Diseased Images: {diseased_count:,} ({diseased_count/dataset_stats['total_images']*100:.1f}%)\")\n",
    "        print(f\"   ⚖️ Imbalance Ratio: {dataset_stats['imbalance_ratio']:.1f}:1\")\n",
    "        \n",
    "        return fig\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating visualizations: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# Create IMMERSIVE distribution dashboard\n",
    "if dataset_stats and dataset_df is not None:\n",
    "    print(\"🎨 Creating immersive Plotly dashboard...\")\n",
    "    distribution_dashboard = create_immersive_distribution_dashboard()\n",
    "else:\n",
    "    print(\"⚠️ Skipping visualization - run dataset analysis first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102109e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📈 Class Imbalance Analysis & Statistics\n",
    "\n",
    "def analyze_class_imbalance(df):\n",
    "    \"\"\"Detailed class imbalance analysis\"\"\"\n",
    "    \n",
    "    # Calculate imbalance metrics\n",
    "    total_images = df['num_images'].sum()\n",
    "    min_class_size = df['num_images'].min()\n",
    "    max_class_size = df['num_images'].max()\n",
    "    mean_class_size = df['num_images'].mean()\n",
    "    median_class_size = df['num_images'].median()\n",
    "    \n",
    "    # Imbalance ratio\n",
    "    imbalance_ratio = max_class_size / min_class_size\n",
    "    \n",
    "    # Coefficient of variation\n",
    "    cv = df['num_images'].std() / df['num_images'].mean()\n",
    "    \n",
    "    # Class distribution statistics\n",
    "    print(\"🔍 Class Imbalance Analysis:\")\n",
    "    print(f\"   📊 Total Images: {total_images:,}\")\n",
    "    print(f\"   📉 Smallest Class: {min_class_size:,} images\")\n",
    "    print(f\"   📈 Largest Class: {max_class_size:,} images\")\n",
    "    print(f\"   ⚖️  Imbalance Ratio: {imbalance_ratio:.1f}:1\")\n",
    "    print(f\"   📊 Mean Class Size: {mean_class_size:.0f}\")\n",
    "    print(f\"   📊 Median Class Size: {median_class_size:.0f}\")\n",
    "    print(f\"   📊 Coefficient of Variation: {cv:.2f}\")\n",
    "    \n",
    "    # Risk assessment\n",
    "    if imbalance_ratio > 100:\n",
    "        risk_level = \"🔴 CRITICAL\"\n",
    "    elif imbalance_ratio > 10:\n",
    "        risk_level = \"🟡 HIGH\"\n",
    "    elif imbalance_ratio > 5:\n",
    "        risk_level = \"🟠 MODERATE\"\n",
    "    else:\n",
    "        risk_level = \"🟢 LOW\"\n",
    "    \n",
    "    print(f\"   ⚠️  Imbalance Risk: {risk_level}\")\n",
    "    \n",
    "    # Classes needing attention\n",
    "    underrepresented = df[df['num_images'] < mean_class_size * 0.5]\n",
    "    if len(underrepresented) > 0:\n",
    "        print(f\"\\n🚨 Underrepresented Classes (< {mean_class_size * 0.5:.0f} images):\")\n",
    "        for _, row in underrepresented.iterrows():\n",
    "            print(f\"   - {row['class_name']}: {row['num_images']} images\")\n",
    "    \n",
    "    return {\n",
    "        'total_images': total_images,\n",
    "        'imbalance_ratio': imbalance_ratio,\n",
    "        'cv': cv,\n",
    "        'underrepresented_classes': len(underrepresented)\n",
    "    }\n",
    "\n",
    "imbalance_stats = analyze_class_imbalance(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8c26cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🖼️ Sample Images Visualization & Quality Analysis\n",
    "\n",
    "def visualize_sample_images(df, samples_per_class=3, figsize=(20, 12)):\n",
    "    \"\"\"Visualize sample images from each class with quality analysis\"\"\"\n",
    "    \n",
    "    # Select diverse classes for visualization\n",
    "    crops = df['crop'].unique()\n",
    "    selected_classes = []\n",
    "    \n",
    "    for crop in crops:\n",
    "        crop_classes = df[df['crop'] == crop]\n",
    "        # Get healthy and diseased samples\n",
    "        healthy = crop_classes[crop_classes['condition'].str.contains('healthy', case=False)]\n",
    "        diseased = crop_classes[~crop_classes['condition'].str.contains('healthy', case=False)]\n",
    "        \n",
    "        if len(healthy) > 0:\n",
    "            selected_classes.append(healthy.iloc[0])\n",
    "        if len(diseased) > 0:\n",
    "            selected_classes.extend(diseased.head(2).to_dict('records'))\n",
    "    \n",
    "    # Create subplot grid\n",
    "    n_classes = len(selected_classes)\n",
    "    cols = 4\n",
    "    rows = (n_classes + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axes = axes.flatten() if rows > 1 else [axes]\n",
    "    \n",
    "    for idx, class_info in enumerate(selected_classes):\n",
    "        if idx >= len(axes):\n",
    "            break\n",
    "            \n",
    "        # Get sample images from this class\n",
    "        image_files = list(class_info['directory'].glob('*.jpg')) + \\\n",
    "                     list(class_info['directory'].glob('*.JPG')) + \\\n",
    "                     list(class_info['directory'].glob('*.png'))\n",
    "        \n",
    "        if image_files:\n",
    "            # Load and display a random sample\n",
    "            sample_img_path = random.choice(image_files)\n",
    "            img = cv2.imread(str(sample_img_path))\n",
    "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Basic image quality metrics\n",
    "            brightness = np.mean(img_rgb)\n",
    "            contrast = np.std(img_rgb)\n",
    "            \n",
    "            axes[idx].imshow(img_rgb)\n",
    "            axes[idx].set_title(f\"{class_info['crop']}\\n{class_info['condition']}\\n\"\n",
    "                              f\"Count: {class_info['num_images']}\\n\"\n",
    "                              f\"Brightness: {brightness:.1f}, Contrast: {contrast:.1f}\", \n",
    "                              fontsize=10)\n",
    "            axes[idx].axis('off')\n",
    "        else:\n",
    "            axes[idx].text(0.5, 0.5, 'No Images', ha='center', va='center')\n",
    "            axes[idx].set_title(f\"{class_info['class_name']}\")\n",
    "            axes[idx].axis('off')\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for idx in range(len(selected_classes), len(axes)):\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('🌱 Sample Images from Plant Disease Dataset', fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "    \n",
    "    return selected_classes\n",
    "\n",
    "# Visualize sample images\n",
    "sample_classes = visualize_sample_images(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5688121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔬 Advanced Image Quality Analysis\n",
    "\n",
    "def analyze_image_quality(df, sample_size=100):\n",
    "    \"\"\"Comprehensive image quality analysis across classes\"\"\"\n",
    "    \n",
    "    quality_metrics = []\n",
    "    \n",
    "    for _, class_info in df.iterrows():\n",
    "        class_dir = class_info['directory']\n",
    "        image_files = list(class_dir.glob('*.jpg')) + \\\n",
    "                     list(class_dir.glob('*.JPG')) + \\\n",
    "                     list(class_dir.glob('*.png'))\n",
    "        \n",
    "        if not image_files:\n",
    "            continue\n",
    "        \n",
    "        # Sample images for analysis\n",
    "        sample_files = random.sample(image_files, min(sample_size, len(image_files)))\n",
    "        \n",
    "        class_metrics = {\n",
    "            'class_name': class_info['class_name'],\n",
    "            'crop': class_info['crop'],\n",
    "            'condition': class_info['condition'],\n",
    "            'brightness_mean': [],\n",
    "            'contrast_mean': [],\n",
    "            'sharpness_mean': [],\n",
    "            'size_variance': []\n",
    "        }\n",
    "        \n",
    "        for img_file in sample_files:\n",
    "            try:\n",
    "                # Load image\n",
    "                img = cv2.imread(str(img_file))\n",
    "                if img is None:\n",
    "                    continue\n",
    "                    \n",
    "                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "                \n",
    "                # Quality metrics\n",
    "                brightness = np.mean(img_rgb)\n",
    "                contrast = np.std(img_rgb)\n",
    "                sharpness = cv2.Laplacian(img_gray, cv2.CV_64F).var()\n",
    "                \n",
    "                class_metrics['brightness_mean'].append(brightness)\n",
    "                class_metrics['contrast_mean'].append(contrast)\n",
    "                class_metrics['sharpness_mean'].append(sharpness)\n",
    "                class_metrics['size_variance'].append(img.shape[0] * img.shape[1])\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # Calculate aggregated metrics\n",
    "        if class_metrics['brightness_mean']:\n",
    "            quality_metrics.append({\n",
    "                'class_name': class_info['class_name'],\n",
    "                'crop': class_info['crop'],\n",
    "                'condition': class_info['condition'],\n",
    "                'num_images': class_info['num_images'],\n",
    "                'avg_brightness': np.mean(class_metrics['brightness_mean']),\n",
    "                'avg_contrast': np.mean(class_metrics['contrast_mean']),\n",
    "                'avg_sharpness': np.mean(class_metrics['sharpness_mean']),\n",
    "                'brightness_std': np.std(class_metrics['brightness_mean']),\n",
    "                'contrast_std': np.std(class_metrics['contrast_mean']),\n",
    "                'sharpness_std': np.std(class_metrics['sharpness_mean']),\n",
    "            })\n",
    "    \n",
    "    quality_df = pd.DataFrame(quality_metrics)\n",
    "    \n",
    "    # Visualize quality metrics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Brightness distribution\n",
    "    quality_df.groupby('crop')['avg_brightness'].mean().plot(kind='bar', ax=axes[0,0], color='gold')\n",
    "    axes[0,0].set_title('🌟 Average Brightness by Crop')\n",
    "    axes[0,0].set_ylabel('Brightness')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Contrast distribution\n",
    "    quality_df.groupby('crop')['avg_contrast'].mean().plot(kind='bar', ax=axes[0,1], color='purple')\n",
    "    axes[0,1].set_title('📊 Average Contrast by Crop')\n",
    "    axes[0,1].set_ylabel('Contrast')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Sharpness distribution\n",
    "    quality_df.groupby('crop')['avg_sharpness'].mean().plot(kind='bar', ax=axes[1,0], color='green')\n",
    "    axes[1,0].set_title('🔍 Average Sharpness by Crop')\n",
    "    axes[1,0].set_ylabel('Sharpness')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Quality consistency (coefficient of variation)\n",
    "    quality_df['brightness_cv'] = quality_df['brightness_std'] / quality_df['avg_brightness']\n",
    "    quality_df.groupby('crop')['brightness_cv'].mean().plot(kind='bar', ax=axes[1,1], color='red')\n",
    "    axes[1,1].set_title('📏 Brightness Consistency by Crop')\n",
    "    axes[1,1].set_ylabel('Coefficient of Variation')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return quality_df\n",
    "\n",
    "print(\"🔬 Analyzing image quality across all classes...\")\n",
    "quality_df = analyze_image_quality(dataset_df, sample_size=50)\n",
    "print(f\"✅ Quality analysis completed for {len(quality_df)} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fba443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎯 Strategic Class Grouping & Disease Severity Analysis\n",
    "\n",
    "def create_disease_taxonomy(df):\n",
    "    \"\"\"Create a hierarchical disease taxonomy for strategic analysis\"\"\"\n",
    "    \n",
    "    taxonomy = {\n",
    "        'healthy': [],\n",
    "        'fungal_diseases': [],\n",
    "        'bacterial_diseases': [],\n",
    "        'viral_diseases': [],\n",
    "        'pest_damage': [],\n",
    "        'nutrient_deficiency': [],\n",
    "        'other_conditions': []\n",
    "    }\n",
    "    \n",
    "    # Classification rules based on condition names\n",
    "    for _, row in df.iterrows():\n",
    "        condition = row['condition'].lower()\n",
    "        class_name = row['class_name']\n",
    "        \n",
    "        if 'healthy' in condition:\n",
    "            taxonomy['healthy'].append(class_name)\n",
    "        elif any(term in condition for term in ['blight', 'rust', 'spot', 'mold', 'leaf_spot']):\n",
    "            taxonomy['fungal_diseases'].append(class_name)\n",
    "        elif 'bacterial' in condition:\n",
    "            taxonomy['bacterial_diseases'].append(class_name)\n",
    "        elif any(term in condition for term in ['virus', 'mosaic', 'curl']):\n",
    "            taxonomy['viral_diseases'].append(class_name)\n",
    "        elif any(term in condition for term in ['mite', 'spider']):\n",
    "            taxonomy['pest_damage'].append(class_name)\n",
    "        else:\n",
    "            taxonomy['other_conditions'].append(class_name)\n",
    "    \n",
    "    # Calculate group statistics\n",
    "    group_stats = []\n",
    "    for group, classes in taxonomy.items():\n",
    "        if classes:\n",
    "            group_data = df[df['class_name'].isin(classes)]\n",
    "            group_stats.append({\n",
    "                'disease_group': group,\n",
    "                'num_classes': len(classes),\n",
    "                'total_images': group_data['num_images'].sum(),\n",
    "                'avg_images_per_class': group_data['num_images'].mean(),\n",
    "                'min_images': group_data['num_images'].min(),\n",
    "                'max_images': group_data['num_images'].max(),\n",
    "                'classes': classes\n",
    "            })\n",
    "    \n",
    "    group_df = pd.DataFrame(group_stats)\n",
    "    \n",
    "    # Visualize disease taxonomy\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Disease group distribution\n",
    "    group_df.plot(x='disease_group', y='total_images', kind='bar', ax=ax1, color='skyblue')\n",
    "    ax1.set_title('📊 Images by Disease Group')\n",
    "    ax1.set_ylabel('Total Images')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Classes per group\n",
    "    group_df.plot(x='disease_group', y='num_classes', kind='bar', ax=ax2, color='lightcoral')\n",
    "    ax2.set_title('🏷️ Classes by Disease Group')\n",
    "    ax2.set_ylabel('Number of Classes')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"🎯 Disease Taxonomy Analysis:\")\n",
    "    for _, row in group_df.iterrows():\n",
    "        print(f\"\\n📋 {row['disease_group'].replace('_', ' ').title()}:\")\n",
    "        print(f\"   Classes: {row['num_classes']}\")\n",
    "        print(f\"   Total Images: {row['total_images']:,}\")\n",
    "        print(f\"   Avg per Class: {row['avg_images_per_class']:.0f}\")\n",
    "        print(f\"   Range: {row['min_images']}-{row['max_images']} images\")\n",
    "    \n",
    "    return taxonomy, group_df\n",
    "\n",
    "# Create disease taxonomy\n",
    "disease_taxonomy, disease_groups_df = create_disease_taxonomy(dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e76f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 Advanced Data Loading & Preprocessing Pipeline\n",
    "\n",
    "class PlantDiseaseDataProcessor:\n",
    "    \"\"\"Advanced data processor for plant disease detection\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, img_size=(224, 224), batch_size=32):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.class_names = None\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        \n",
    "    def create_class_mapping(self):\n",
    "        \"\"\"Create comprehensive class mapping with metadata\"\"\"\n",
    "        class_dirs = [d for d in self.data_dir.iterdir() if d.is_dir()]\n",
    "        self.class_names = sorted([d.name for d in class_dirs])\n",
    "        \n",
    "        # Enhanced class mapping with crop and condition info\n",
    "        class_mapping = {}\n",
    "        for i, class_name in enumerate(self.class_names):\n",
    "            if '___' in class_name:\n",
    "                crop, condition = class_name.split('___', 1)\n",
    "                crop = crop.replace('(', '').replace(')', '').replace('_', ' ')\n",
    "            else:\n",
    "                crop = 'Unknown'\n",
    "                condition = class_name\n",
    "            \n",
    "            class_mapping[i] = {\n",
    "                'class_name': class_name,\n",
    "                'crop': crop,\n",
    "                'condition': condition,\n",
    "                'is_healthy': 'healthy' in condition.lower(),\n",
    "                'severity': self._estimate_severity(condition)\n",
    "            }\n",
    "        \n",
    "        return class_mapping\n",
    "    \n",
    "    def _estimate_severity(self, condition):\n",
    "        \"\"\"Estimate disease severity from condition name\"\"\"\n",
    "        condition_lower = condition.lower()\n",
    "        if 'healthy' in condition_lower:\n",
    "            return 0\n",
    "        elif any(term in condition_lower for term in ['early', 'minor']):\n",
    "            return 1\n",
    "        elif any(term in condition_lower for term in ['late', 'severe', 'blight']):\n",
    "            return 3\n",
    "        else:\n",
    "            return 2  # moderate\n",
    "    \n",
    "    def load_and_prepare_data(self, validation_split=0.2, test_split=0.1, stratify=True):\n",
    "        \"\"\"Load and prepare data with advanced preprocessing\"\"\"\n",
    "        \n",
    "        # Load image paths and labels\n",
    "        image_paths = []\n",
    "        labels = []\n",
    "        \n",
    "        for class_dir in self.data_dir.iterdir():\n",
    "            if not class_dir.is_dir():\n",
    "                continue\n",
    "                \n",
    "            class_name = class_dir.name\n",
    "            image_files = list(class_dir.glob('*.jpg')) + \\\n",
    "                         list(class_dir.glob('*.JPG')) + \\\n",
    "                         list(class_dir.glob('*.png'))\n",
    "            \n",
    "            for img_path in image_files:\n",
    "                image_paths.append(str(img_path))\n",
    "                labels.append(class_name)\n",
    "        \n",
    "        # Encode labels\n",
    "        labels_encoded = self.label_encoder.fit_transform(labels)\n",
    "        self.class_names = list(self.label_encoder.classes_)\n",
    "        \n",
    "        # Stratified splitting\n",
    "        if stratify:\n",
    "            X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "                image_paths, labels_encoded, \n",
    "                test_size=validation_split + test_split,\n",
    "                stratify=labels_encoded, \n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            X_val, X_test, y_val, y_test = train_test_split(\n",
    "                X_temp, y_temp,\n",
    "                test_size=test_split / (validation_split + test_split),\n",
    "                stratify=y_temp,\n",
    "                random_state=42\n",
    "            )\n",
    "        else:\n",
    "            # Simple random splitting\n",
    "            X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "                image_paths, labels_encoded, \n",
    "                test_size=validation_split + test_split,\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            X_val, X_test, y_val, y_test = train_test_split(\n",
    "                X_temp, y_temp,\n",
    "                test_size=test_split / (validation_split + test_split),\n",
    "                random_state=42\n",
    "            )\n",
    "        \n",
    "        print(f\"📊 Data Split Summary:\")\n",
    "        print(f\"   🏋️ Training: {len(X_train):,} images\")\n",
    "        print(f\"   🔍 Validation: {len(X_val):,} images\") \n",
    "        print(f\"   🎯 Test: {len(X_test):,} images\")\n",
    "        print(f\"   🏷️ Classes: {len(self.class_names)}\")\n",
    "        \n",
    "        return {\n",
    "            'train': (X_train, y_train),\n",
    "            'val': (X_val, y_val), \n",
    "            'test': (X_test, y_test),\n",
    "            'class_mapping': self.create_class_mapping()\n",
    "        }\n",
    "    \n",
    "    def create_tf_dataset(self, image_paths, labels, training=False):\n",
    "        \"\"\"Create TensorFlow dataset with advanced augmentation\"\"\"\n",
    "        \n",
    "        def load_and_preprocess_image(path, label):\n",
    "            \"\"\"Load and preprocess individual image\"\"\"\n",
    "            image = tf.io.read_file(path)\n",
    "            image = tf.image.decode_jpeg(image, channels=3)\n",
    "            image = tf.image.resize(image, self.img_size)\n",
    "            image = tf.cast(image, tf.float32) / 255.0\n",
    "            \n",
    "            if training:\n",
    "                # Advanced augmentation for training\n",
    "                image = tf.image.random_flip_left_right(image)\n",
    "                image = tf.image.random_flip_up_down(image)\n",
    "                image = tf.image.random_brightness(image, max_delta=0.1)\n",
    "                image = tf.image.random_contrast(image, lower=0.9, upper=1.1)\n",
    "                image = tf.image.random_saturation(image, lower=0.9, upper=1.1)\n",
    "                image = tf.image.random_hue(image, max_delta=0.05)\n",
    "                \n",
    "                # Random rotation\n",
    "                image = tf.image.rot90(image, tf.random.uniform(shape=[], maxval=4, dtype=tf.int32))\n",
    "                \n",
    "                # Random zoom and crop\n",
    "                image = tf.image.random_crop(\n",
    "                    tf.image.resize(image, [int(self.img_size[0] * 1.1), int(self.img_size[1] * 1.1)]),\n",
    "                    size=[self.img_size[0], self.img_size[1], 3]\n",
    "                )\n",
    "            \n",
    "            return image, label\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "        dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        \n",
    "        if training:\n",
    "            dataset = dataset.shuffle(buffer_size=1000)\n",
    "            dataset = dataset.repeat()\n",
    "        \n",
    "        dataset = dataset.batch(self.batch_size)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        return dataset\n",
    "\n",
    "# Initialize data processor\n",
    "print(\"🔄 Initializing Plant Disease Data Processor...\")\n",
    "data_processor = PlantDiseaseDataProcessor(DATA_DIR, img_size=(224, 224), batch_size=32)\n",
    "\n",
    "# Load and prepare data\n",
    "print(\"📥 Loading and preparing dataset...\")\n",
    "data_splits = data_processor.load_and_prepare_data(validation_split=0.2, test_split=0.1)\n",
    "\n",
    "print(\"✅ Data preparation completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564c8980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🎛️ Class Weight Calculation for Imbalanced Dataset\n",
    "\n",
    "def calculate_strategic_class_weights(y_train, labels, strategy='balanced'):\n",
    "    \"\"\"Calculate class weights with multiple strategies for imbalanced data\"\"\"\n",
    "    \n",
    "    # Count samples per class\n",
    "    unique_labels, counts = np.unique(y_train, return_counts=True)\n",
    "    \n",
    "    strategies = {\n",
    "        'balanced': compute_class_weight('balanced', classes=unique_labels, y=y_train),\n",
    "        'inverse_freq': len(y_train) / (len(unique_labels) * counts),\n",
    "        'sqrt_inverse_freq': np.sqrt(len(y_train) / (len(unique_labels) * counts)),\n",
    "        'log_inverse_freq': np.log(len(y_train) / counts),\n",
    "    }\n",
    "    \n",
    "    class_weights = strategies[strategy]\n",
    "    class_weight_dict = dict(zip(unique_labels, class_weights))\n",
    "    \n",
    "    print(f\"📊 Class Weight Strategy: {strategy}\")\n",
    "    print(f\"🔢 Weight Range: {class_weights.min():.3f} - {class_weights.max():.3f}\")\n",
    "    \n",
    "    # Visualize class weights\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Class distribution\n",
    "    ax1.bar(range(len(counts)), counts, color='lightblue', alpha=0.7)\n",
    "    ax1.set_title('📊 Original Class Distribution')\n",
    "    ax1.set_xlabel('Class Index')\n",
    "    ax1.set_ylabel('Sample Count')\n",
    "    \n",
    "    # Class weights\n",
    "    ax2.bar(range(len(class_weights)), class_weights, color='orange', alpha=0.7)\n",
    "    ax2.set_title(f'⚖️ Class Weights ({strategy})')\n",
    "    ax2.set_xlabel('Class Index')\n",
    "    ax2.set_ylabel('Weight')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return class_weight_dict\n",
    "\n",
    "# Calculate class weights\n",
    "X_train, y_train = data_splits['train']\n",
    "class_weights = calculate_strategic_class_weights(y_train, data_processor.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dfa049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🏗️ Advanced Model Architecture Factory\n",
    "\n",
    "class PlantDiseaseModelFactory:\n",
    "    \"\"\"Factory for creating state-of-the-art plant disease detection models\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, input_shape=(224, 224, 3)):\n",
    "        self.num_classes = num_classes\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "    def create_efficient_net_model(self, model_size='B0', fine_tune=True):\n",
    "        \"\"\"Create EfficientNet-based model - excellent for agricultural imagery\"\"\"\n",
    "        \n",
    "        # Load pre-trained EfficientNet\n",
    "        if model_size == 'B0':\n",
    "            base_model = tf.keras.applications.EfficientNetB0(\n",
    "                weights='imagenet',\n",
    "                include_top=False,\n",
    "                input_shape=self.input_shape\n",
    "            )\n",
    "        elif model_size == 'B3':\n",
    "            base_model = tf.keras.applications.EfficientNetB3(\n",
    "                weights='imagenet', \n",
    "                include_top=False,\n",
    "                input_shape=self.input_shape\n",
    "            )\n",
    "        \n",
    "        # Add custom classifier head\n",
    "        model = tf.keras.Sequential([\n",
    "            base_model,\n",
    "            tf.keras.layers.GlobalAveragePooling2D(),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(512, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(256, activation='relu'),\n",
    "            tf.keras.layers.BatchNormalization(), \n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(self.num_classes, activation='softmax', name='predictions')\n",
    "        ])\n",
    "        \n",
    "        if fine_tune:\n",
    "            # Unfreeze the last few layers for fine-tuning\n",
    "            base_model.trainable = True\n",
    "            for layer in base_model.layers[:-20]:\n",
    "                layer.trainable = False\n",
    "        else:\n",
    "            base_model.trainable = False\n",
    "            \n",
    "        return model\n",
    "    \n",
    "    def create_vision_transformer(self, patch_size=16, num_heads=8, num_layers=6):\n",
    "        \"\"\"Create Vision Transformer model for plant disease detection\"\"\"\n",
    "        \n",
    "        # Vision Transformer implementation\n",
    "        inputs = tf.keras.layers.Input(shape=self.input_shape)\n",
    "        \n",
    "        # Patch extraction\n",
    "        patches = self._extract_patches(inputs, patch_size)\n",
    "        patch_dims = patch_size * patch_size * 3\n",
    "        \n",
    "        # Patch encoding\n",
    "        encoded_patches = tf.keras.layers.Dense(256)(patches)\n",
    "        \n",
    "        # Positional encoding\n",
    "        num_patches = (self.input_shape[0] // patch_size) ** 2\n",
    "        positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "        position_embedding = tf.keras.layers.Embedding(input_dim=num_patches, output_dim=256)(positions)\n",
    "        encoded_patches = encoded_patches + position_embedding\n",
    "        \n",
    "        # Transformer blocks\n",
    "        for _ in range(num_layers):\n",
    "            # Multi-head attention\n",
    "            attention_output = tf.keras.layers.MultiHeadAttention(\n",
    "                num_heads=num_heads, key_dim=256\n",
    "            )(encoded_patches, encoded_patches)\n",
    "            attention_output = tf.keras.layers.Dropout(0.1)(attention_output)\n",
    "            attention_output = tf.keras.layers.LayerNormalization()(encoded_patches + attention_output)\n",
    "            \n",
    "            # Feed forward\n",
    "            ffn_output = tf.keras.layers.Dense(512, activation='gelu')(attention_output)\n",
    "            ffn_output = tf.keras.layers.Dense(256)(ffn_output)\n",
    "            ffn_output = tf.keras.layers.Dropout(0.1)(ffn_output)\n",
    "            encoded_patches = tf.keras.layers.LayerNormalization()(attention_output + ffn_output)\n",
    "        \n",
    "        # Global average pooling and classification\n",
    "        representation = tf.keras.layers.GlobalAveragePooling1D()(encoded_patches)\n",
    "        features = tf.keras.layers.Dense(512, activation='relu')(representation)\n",
    "        features = tf.keras.layers.Dropout(0.3)(features)\n",
    "        outputs = tf.keras.layers.Dense(self.num_classes, activation='softmax')(features)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n",
    "    \n",
    "    def _extract_patches(self, images, patch_size):\n",
    "        \"\"\"Extract patches from images\"\"\"\n",
    "        # Fix tensor shape access - use .shape instead of indexing tf.shape result\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, patch_size, patch_size, 1],\n",
    "            strides=[1, patch_size, patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches\n",
    "    \n",
    "    def create_hybrid_cnn_transformer(self):\n",
    "        \"\"\"Create hybrid CNN-Transformer model combining both approaches\"\"\"\n",
    "        \n",
    "        # CNN backbone for feature extraction\n",
    "        base_cnn = tf.keras.applications.EfficientNetB0(\n",
    "            weights='imagenet',\n",
    "            include_top=False,\n",
    "            input_shape=self.input_shape\n",
    "        )\n",
    "        base_cnn.trainable = False\n",
    "        \n",
    "        inputs = tf.keras.layers.Input(shape=self.input_shape)\n",
    "        \n",
    "        # CNN feature extraction\n",
    "        cnn_features = base_cnn(inputs, training=False)\n",
    "        cnn_features = tf.keras.layers.GlobalAveragePooling2D()(cnn_features)\n",
    "        \n",
    "        # Transformer branch\n",
    "        patches = self._extract_patches(inputs, patch_size=32)\n",
    "        patch_embedding = tf.keras.layers.Dense(256)(patches)\n",
    "        \n",
    "        # Single transformer block\n",
    "        attention_output = tf.keras.layers.MultiHeadAttention(num_heads=8, key_dim=256)(\n",
    "            patch_embedding, patch_embedding\n",
    "        )\n",
    "        attention_output = tf.keras.layers.LayerNormalization()(patch_embedding + attention_output)\n",
    "        transformer_features = tf.keras.layers.GlobalAveragePooling1D()(attention_output)\n",
    "        \n",
    "        # Fusion layer\n",
    "        combined_features = tf.keras.layers.Concatenate()([cnn_features, transformer_features])\n",
    "        combined_features = tf.keras.layers.Dense(512, activation='relu')(combined_features)\n",
    "        combined_features = tf.keras.layers.BatchNormalization()(combined_features)\n",
    "        combined_features = tf.keras.layers.Dropout(0.4)(combined_features)\n",
    "        \n",
    "        # Final classification\n",
    "        outputs = tf.keras.layers.Dense(self.num_classes, activation='softmax')(combined_features)\n",
    "        \n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "        return model\n",
    "    \n",
    "    def create_ensemble_model(self, models_list):\n",
    "        \"\"\"Create ensemble model from multiple architectures\"\"\"\n",
    "        \n",
    "        inputs = tf.keras.layers.Input(shape=self.input_shape)\n",
    "        outputs = []\n",
    "        \n",
    "        for model in models_list:\n",
    "            model.trainable = False\n",
    "            output = model(inputs)\n",
    "            outputs.append(output)\n",
    "        \n",
    "        # Average ensemble\n",
    "        ensemble_output = tf.keras.layers.Average()(outputs)\n",
    "        \n",
    "        ensemble_model = tf.keras.Model(inputs=inputs, outputs=ensemble_output)\n",
    "        return ensemble_model\n",
    "\n",
    "# Initialize model factory - add error handling\n",
    "try:\n",
    "    model_factory = PlantDiseaseModelFactory(\n",
    "        num_classes=len(data_processor.class_names) if 'data_processor' in locals() else 19, \n",
    "        input_shape=(224, 224, 3)\n",
    "    )\n",
    "    num_classes = model_factory.num_classes\n",
    "    \n",
    "    print(f\"🏭 Model Factory initialized for {num_classes} classes\")\n",
    "    print(f\"🎯 Input shape: {model_factory.input_shape}\")\n",
    "except NameError as e:\n",
    "    print(f\"⚠️  Warning: {e}\")\n",
    "    print(\"💡 Will initialize with default values\")\n",
    "    num_classes = 19  # Default number of classes\n",
    "    model_factory = PlantDiseaseModelFactory(num_classes=num_classes, input_shape=(224, 224, 3))\n",
    "    print(f\"🏭 Model Factory initialized with default {num_classes} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd7bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🚀 Create and Compare Multiple Model Architectures\n",
    "\n",
    "def create_and_compare_models():\n",
    "    \"\"\"Create multiple model architectures for comparison\"\"\"\n",
    "    \n",
    "    models = {}\n",
    "    \n",
    "    print(\"🔨 Building model architectures...\")\n",
    "    \n",
    "    # 1. EfficientNet B0 (Fast, efficient)\n",
    "    print(\"   📱 Creating EfficientNet-B0 (Mobile-friendly)...\")\n",
    "    models['efficientnet_b0'] = model_factory.create_efficient_net_model('B0', fine_tune=True)\n",
    "    \n",
    "    # 2. EfficientNet B3 (Higher accuracy)\n",
    "    print(\"   🔥 Creating EfficientNet-B3 (High performance)...\")\n",
    "    models['efficientnet_b3'] = model_factory.create_efficient_net_model('B3', fine_tune=True)\n",
    "    \n",
    "    # 3. Hybrid CNN-Transformer\n",
    "    print(\"   🤖 Creating Hybrid CNN-Transformer...\")\n",
    "    models['hybrid_cnn_transformer'] = model_factory.create_hybrid_cnn_transformer()\n",
    "    \n",
    "    # Model summary comparison\n",
    "    print(\"\\n📊 Model Architecture Comparison:\")\n",
    "    for name, model in models.items():\n",
    "        total_params = model.count_params()\n",
    "        trainable_params = sum([keras.backend.count_params(w) for w in model.trainable_weights])\n",
    "        \n",
    "        print(f\"\\n🏗️ {name.replace('_', ' ').title()}:\")\n",
    "        print(f\"   📏 Total Parameters: {total_params:,}\")\n",
    "        print(f\"   🎯 Trainable Parameters: {trainable_params:,}\")\n",
    "        print(f\"   💾 Estimated Model Size: {total_params * 4 / (1024**2):.1f} MB\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "# Create model architectures\n",
    "models = create_and_compare_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28893751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 📈 Advanced Training Configuration & Callbacks\n",
    "\n",
    "def create_training_callbacks(model_name, monitor='val_accuracy'):\n",
    "    \"\"\"Create comprehensive training callbacks for agricultural AI\"\"\"\n",
    "    \n",
    "    # Paths for this specific model\n",
    "    model_dir = MODELS_DIR / model_name\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    callbacks_list = [\n",
    "        # Model checkpointing - save best model\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            filepath=str(model_dir / 'best_model.h5'),\n",
    "            monitor=monitor,\n",
    "            save_best_only=True,\n",
    "            save_weights_only=False,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Early stopping - prevent overfitting\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor=monitor,\n",
    "            patience=15,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # Learning rate reduction\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=8,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        \n",
    "        # CSV logging\n",
    "        keras.callbacks.CSVLogger(\n",
    "            filename=str(model_dir / 'training_log.csv'),\n",
    "            append=True\n",
    "        ),\n",
    "        \n",
    "        # TensorBoard logging\n",
    "        keras.callbacks.TensorBoard(\n",
    "            log_dir=str(EXPERIMENTS_DIR / 'tensorboard' / model_name),\n",
    "            histogram_freq=1,\n",
    "            write_graph=True,\n",
    "            write_images=True,\n",
    "            update_freq='epoch'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return callbacks_list\n",
    "\n",
    "def compile_model_for_agriculture(model, model_name):\n",
    "    \"\"\"Compile model with agricultural-specific considerations\"\"\"\n",
    "    \n",
    "    # For critical agricultural applications, we prioritize:\n",
    "    # 1. High recall for disease detection (avoid false negatives)\n",
    "    # 2. Calibrated confidence scores\n",
    "    # 3. Robust optimization\n",
    "    \n",
    "    # Custom metrics for agricultural applications\n",
    "    def f1_score(y_true, y_pred):\n",
    "        \"\"\"F1 score metric\"\"\"\n",
    "        y_pred = tf.round(y_pred)\n",
    "        tp = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32), axis=0)\n",
    "        fp = tf.reduce_sum(tf.cast((1 - y_true) * y_pred, tf.float32), axis=0)\n",
    "        fn = tf.reduce_sum(tf.cast(y_true * (1 - y_pred), tf.float32), axis=0)\n",
    "        \n",
    "        precision = tp / (tp + fp + tf.keras.backend.epsilon())\n",
    "        recall = tp / (tp + fn + tf.keras.backend.epsilon())\n",
    "        f1 = 2 * precision * recall / (precision + recall + tf.keras.backend.epsilon())\n",
    "        \n",
    "        return tf.reduce_mean(f1)\n",
    "    \n",
    "    def recall_score(y_true, y_pred):\n",
    "        \"\"\"Recall score - critical for disease detection\"\"\"\n",
    "        y_pred = tf.round(y_pred)\n",
    "        tp = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32), axis=0)\n",
    "        fn = tf.reduce_sum(tf.cast(y_true * (1 - y_pred), tf.float32), axis=0)\n",
    "        return tf.reduce_mean(tp / (tp + fn + tf.keras.backend.epsilon()))\n",
    "    \n",
    "    def precision_score(y_true, y_pred):\n",
    "        \"\"\"Precision score\"\"\"\n",
    "        y_pred = tf.round(y_pred)\n",
    "        tp = tf.reduce_sum(tf.cast(y_true * y_pred, tf.float32), axis=0)\n",
    "        fp = tf.reduce_sum(tf.cast((1 - y_true) * y_pred, tf.float32), axis=0)\n",
    "        return tf.reduce_mean(tp / (tp + fp + tf.keras.backend.epsilon()))\n",
    "    \n",
    "    # Focal loss for handling class imbalance\n",
    "    def focal_loss(alpha=0.25, gamma=2.0):\n",
    "        \"\"\"Focal loss to handle class imbalance\"\"\"\n",
    "        def focal_loss_fixed(y_true, y_pred):\n",
    "            y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "            p_t = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "            alpha_factor = tf.ones_like(y_true) * alpha\n",
    "            alpha_t = tf.where(tf.equal(y_true, 1), alpha_factor, 1 - alpha_factor)\n",
    "            cross_entropy = -tf.math.log(p_t)\n",
    "            weight = alpha_t * tf.pow((1 - p_t), gamma)\n",
    "            loss = weight * cross_entropy\n",
    "            return tf.reduce_mean(tf.reduce_sum(loss, axis=1))\n",
    "        return focal_loss_fixed\n",
    "    \n",
    "    # Choose optimizer based on model complexity\n",
    "    if 'efficientnet' in model_name.lower():\n",
    "        optimizer = keras.optimizers.AdamW(learning_rate=0.001, weight_decay=0.0001)\n",
    "    else:\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=focal_loss(alpha=0.25, gamma=2.0),\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            keras.metrics.TopKCategoricalAccuracy(k=3, name='top3_accuracy'),\n",
    "            f1_score,\n",
    "            recall_score,\n",
    "            precision_score\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Model '{model_name}' compiled successfully\")\n",
    "    print(f\"   🎯 Loss: Focal Loss (α=0.25, γ=2.0)\")\n",
    "    print(f\"   📊 Metrics: Accuracy, Top-3, F1, Recall, Precision\")\n",
    "    print(f\"   ⚡ Optimizer: {optimizer.__class__.__name__}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Compile all models\n",
    "print(\"⚙️ Compiling models for agricultural applications...\")\n",
    "for name, model in models.items():\n",
    "    models[name] = compile_model_for_agriculture(model, name)\n",
    "\n",
    "print(\"✅ All models compiled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3a531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🏋️‍♂️ Create TensorFlow Datasets for Training\n",
    "\n",
    "print(\"🔄 Creating TensorFlow datasets...\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = data_processor.create_tf_dataset(\n",
    "    data_splits['train'][0], \n",
    "    data_splits['train'][1], \n",
    "    training=True\n",
    ")\n",
    "\n",
    "val_dataset = data_processor.create_tf_dataset(\n",
    "    data_splits['val'][0], \n",
    "    data_splits['val'][1], \n",
    "    training=False\n",
    ")\n",
    "\n",
    "test_dataset = data_processor.create_tf_dataset(\n",
    "    data_splits['test'][0], \n",
    "    data_splits['test'][1], \n",
    "    training=False\n",
    ")\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = len(data_splits['train'][0]) // data_processor.batch_size\n",
    "validation_steps = len(data_splits['val'][0]) // data_processor.batch_size\n",
    "\n",
    "print(f\"📊 Dataset Statistics:\")\n",
    "print(f\"   🏋️ Steps per Epoch: {steps_per_epoch}\")\n",
    "print(f\"   🔍 Validation Steps: {validation_steps}\")\n",
    "print(f\"   💿 Batch Size: {data_processor.batch_size}\")\n",
    "\n",
    "# Visualize a batch of training data\n",
    "def visualize_training_batch(dataset, class_names, num_images=12):\n",
    "    \"\"\"Visualize a batch of training data with augmentations\"\"\"\n",
    "    \n",
    "    # Get a batch\n",
    "    for images, labels in dataset.take(1):\n",
    "        fig, axes = plt.subplots(3, 4, figsize=(16, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i in range(min(num_images, len(images))):\n",
    "            img = images[i].numpy()\n",
    "            label_idx = tf.argmax(labels[i]).numpy()\n",
    "            label_name = class_names[label_idx]\n",
    "            \n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(f\"{label_name}\", fontsize=10)\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        # Hide extra subplots\n",
    "        for i in range(num_images, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.suptitle('🌱 Training Data with Augmentations', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "# Visualize training data\n",
    "print(\"👀 Visualizing training data with augmentations...\")\n",
    "visualize_training_batch(train_dataset, data_processor.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab20cdfd",
   "metadata": {},
   "source": [
    "# 🧭 Notebook Relocated\n",
    "\n",
    "This notebook has been superseded by a new dedicated EDA notebook and a separate training/evaluation notebook to better align with the project goals:\n",
    "\n",
    "- 01_eda_plant_disease.ipynb – Comprehensive EDA with interactive Plotly visuals\n",
    "- 02_training_evaluation.ipynb – Model training, evaluation, and calibration\n",
    "\n",
    "You can safely continue using the new notebooks in the `notebooks/` folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
