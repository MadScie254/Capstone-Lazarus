{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6e9265",
   "metadata": {},
   "source": [
    "# üî• Micro-Job Feature Extraction Pipeline\n",
    "\n",
    "**Mission**: Eliminate training bottlenecks with resumable feature caching  \n",
    "**Target**: 4GB VRAM, 64 images per job, <2min per job  \n",
    "**Strategy**: EfficientNet-B0 encoder ‚Üí float16 NPZ cache ‚Üí head-only training\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Pipeline Overview\n",
    "\n",
    "1. **Job Queue Creation**: Split dataset into 64-image chunks\n",
    "2. **Feature Extraction**: Process jobs with encoder (batch_size=8)\n",
    "3. **Feature Caching**: Save as `features/encoder_*/img_*.npz` (float16)\n",
    "4. **Manifest Generation**: Create `features/manifest_features.v001.csv`\n",
    "5. **Resume Logic**: Skip completed jobs via `.done` files\n",
    "\n",
    "### üìä Resource Targets\n",
    "- **VRAM**: <2.5GB peak (within 4GB constraint)\n",
    "- **Speed**: 64 images in <2 minutes\n",
    "- **Storage**: ~50MB per 1000 images (float16 compression)\n",
    "- **Quality**: Equivalent to full training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7397f3be",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _imaging: The operating system cannot run %1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtimm\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransforms\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\timm\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m __version__\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     is_scriptable \u001b[38;5;28;01mas\u001b[39;00m is_scriptable,\n\u001b[32m      4\u001b[39m     is_exportable \u001b[38;5;28;01mas\u001b[39;00m is_exportable,\n\u001b[32m      5\u001b[39m     set_scriptable \u001b[38;5;28;01mas\u001b[39;00m set_scriptable,\n\u001b[32m      6\u001b[39m     set_exportable \u001b[38;5;28;01mas\u001b[39;00m set_exportable,\n\u001b[32m      7\u001b[39m )\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      9\u001b[39m     create_model \u001b[38;5;28;01mas\u001b[39;00m create_model,\n\u001b[32m     10\u001b[39m     list_models \u001b[38;5;28;01mas\u001b[39;00m list_models,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     get_pretrained_cfg_value \u001b[38;5;28;01mas\u001b[39;00m get_pretrained_cfg_value,\n\u001b[32m     18\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\timm\\layers\\__init__.py:23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mattention_pool2d\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AttentionPool2d, RotAttentionPool2d, RotaryEmbedding\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mblur_pool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BlurPool2d, create_aa\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mclassifier\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_classifier, ClassifierHead, NormMlpClassifierHead, ClNormMlpClassifierHead\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcond_conv2d\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CondConv2d, get_condconv_initializer\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     26\u001b[39m     is_exportable,\n\u001b[32m     27\u001b[39m     is_scriptable,\n\u001b[32m   (...)\u001b[39m\u001b[32m     36\u001b[39m     use_reentrant_ckpt,\n\u001b[32m     37\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\timm\\layers\\classifier.py:15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01madaptive_avgmax_pool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SelectAdaptivePool2d\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcreate_act\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_act_layer\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcreate_norm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_norm_layer\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_pool\u001b[39m(\n\u001b[32m     19\u001b[39m         num_features: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m     20\u001b[39m         num_classes: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m         input_fmt: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     24\u001b[39m ):\n\u001b[32m     25\u001b[39m     flatten_in_pool = \u001b[38;5;129;01mnot\u001b[39;00m use_conv  \u001b[38;5;66;03m# flatten when we use a Linear layer after pooling\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\timm\\layers\\create_norm.py:29\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnorm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     14\u001b[39m     GroupNorm,\n\u001b[32m     15\u001b[39m     GroupNorm1,\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     SimpleNorm2dFp32,\n\u001b[32m     28\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mops\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmisc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FrozenBatchNorm2d\n\u001b[32m     31\u001b[39m _NORM_MAP = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m     32\u001b[39m     batchnorm=nn.BatchNorm2d,\n\u001b[32m     33\u001b[39m     batchnorm2d=nn.BatchNorm2d,\n\u001b[32m   (...)\u001b[39m\u001b[32m     49\u001b[39m     frozenbatchnorm2d=FrozenBatchNorm2d,\n\u001b[32m     50\u001b[39m )\n\u001b[32m     51\u001b[39m _NORM_TYPES = {m \u001b[38;5;28;01mfor\u001b[39;00m n, m \u001b[38;5;129;01min\u001b[39;00m _NORM_MAP.items()}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torchvision\\__init__.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torchvision\\datasets\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_optical_flow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FlyingChairs, FlyingThings3D, HD1K, KittiFlow, Sintel\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_stereo_matching\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     CarlaStereo,\n\u001b[32m      4\u001b[39m     CREStereo,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     SintelStereo,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcaltech\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Caltech101, Caltech256\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torchvision\\datasets\\_optical_flow.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m decode_png, read_file\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfolder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m default_loader\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\PIL\\Image.py:90\u001b[39m\n\u001b[32m     81\u001b[39m MAX_IMAGE_PIXELS: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28mint\u001b[39m(\u001b[32m1024\u001b[39m * \u001b[32m1024\u001b[39m * \u001b[32m1024\u001b[39m // \u001b[32m4\u001b[39m // \u001b[32m3\u001b[39m)\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     85\u001b[39m     \u001b[38;5;66;03m# If the _imaging C module is not present, Pillow will not load.\u001b[39;00m\n\u001b[32m     86\u001b[39m     \u001b[38;5;66;03m# Note that other modules should not refer to _imaging directly;\u001b[39;00m\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# import Image and use the Image.core variable instead.\u001b[39;00m\n\u001b[32m     88\u001b[39m     \u001b[38;5;66;03m# Also note that Image.core is not a publicly documented interface,\u001b[39;00m\n\u001b[32m     89\u001b[39m     \u001b[38;5;66;03m# and should be considered private and subject to change.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _imaging \u001b[38;5;28;01mas\u001b[39;00m core\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m __version__ != \u001b[38;5;28mgetattr\u001b[39m(core, \u001b[33m\"\u001b[39m\u001b[33mPILLOW_VERSION\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     93\u001b[39m         msg = (\n\u001b[32m     94\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe _imaging extension was built for another version of Pillow or PIL:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCore version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mgetattr\u001b[39m(core,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mPILLOW_VERSION\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPillow version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     97\u001b[39m         )\n",
      "\u001b[31mImportError\u001b[39m: DLL load failed while importing _imaging: The operating system cannot run %1."
     ]
    }
   ],
   "source": [
    "# üîß Setup & Imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Project imports\n",
    "sys.path.append('../src')\n",
    "from data_utils import ImageFolderAlb\n",
    "\n",
    "# üéÆ Device & Memory Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"üöÄ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Running on CPU - feature extraction will be slower\")\n",
    "\n",
    "print(f\"üîß PyTorch: {torch.__version__}\")\n",
    "print(f\"üìÅ Working dir: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c3c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Configuration\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'data_dir': '../data',\n",
    "    'features_dir': '../features',\n",
    "    'encoder_name': 'efficientnet_b0',\n",
    "    \n",
    "    # Job settings (4GB VRAM optimized)\n",
    "    'job_size': 64,          # Images per job\n",
    "    'batch_size': 8,         # Processing batch (VRAM constraint)\n",
    "    'img_size': 224,         # Input resolution\n",
    "    'feature_dtype': 'float16',  # Memory compression\n",
    "    \n",
    "    # Feature extraction\n",
    "    'use_global_pool': True,     # Extract global features\n",
    "    'extract_spatial': False,    # Skip spatial for now (head-only training)\n",
    "    'normalize_features': True,  # L2 normalize\n",
    "    \n",
    "    # Performance\n",
    "    'num_workers': 4,        # DataLoader workers\n",
    "    'pin_memory': True,      # GPU transfer optimization\n",
    "    'prefetch_factor': 2,    # Async data loading\n",
    "}\n",
    "\n",
    "print(\"üéØ MICRO-JOB CONFIGURATION:\")\n",
    "print(f\"   üìä Job size: {CONFIG['job_size']} images\")\n",
    "print(f\"   üé¨ Batch size: {CONFIG['batch_size']} (VRAM-safe)\")\n",
    "print(f\"   üìê Image size: {CONFIG['img_size']}px\")\n",
    "print(f\"   üóúÔ∏è Feature dtype: {CONFIG['feature_dtype']}\")\n",
    "print(f\"   üèóÔ∏è Encoder: {CONFIG['encoder_name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e88835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Dataset Scanning & Job Queue Creation\n",
    "\n",
    "def scan_dataset(data_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"Scan dataset and create image manifest\"\"\"\n",
    "    print(f\"üîç Scanning dataset: {data_dir}\")\n",
    "    \n",
    "    data_path = Path(data_dir)\n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(f\"Data directory not found: {data_dir}\")\n",
    "    \n",
    "    # Collect all images\n",
    "    images = []\n",
    "    for class_dir in data_path.iterdir():\n",
    "        if not class_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        class_name = class_dir.name\n",
    "        print(f\"   üìÅ Processing class: {class_name}\")\n",
    "        \n",
    "        for img_file in class_dir.glob('*'):\n",
    "            if img_file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']:\n",
    "                images.append({\n",
    "                    'image_path': str(img_file),\n",
    "                    'class_name': class_name,\n",
    "                    'image_id': f\"{class_name}_{img_file.stem}\",\n",
    "                    'file_size': img_file.stat().st_size\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(images)\n",
    "    print(f\"\\n‚úÖ Dataset scan complete:\")\n",
    "    print(f\"   üñºÔ∏è Total images: {len(df):,}\")\n",
    "    print(f\"   üè∑Ô∏è Classes: {df['class_name'].nunique()}\")\n",
    "    print(f\"   üíæ Total size: {df['file_size'].sum() / 1e9:.2f}GB\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_job_queue(image_df: pd.DataFrame, job_size: int = 64) -> pd.DataFrame:\n",
    "    \"\"\"Split images into job chunks for micro-job processing\"\"\"\n",
    "    print(f\"\\nüìã Creating job queue (job_size={job_size})...\")\n",
    "    \n",
    "    # Shuffle for balanced jobs across classes\n",
    "    shuffled_df = image_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Create job chunks\n",
    "    jobs = []\n",
    "    for i in range(0, len(shuffled_df), job_size):\n",
    "        job_images = shuffled_df.iloc[i:i+job_size]\n",
    "        \n",
    "        jobs.append({\n",
    "            'job_id': len(jobs),\n",
    "            'image_paths': ','.join(job_images['image_path'].tolist()),\n",
    "            'image_ids': ','.join(job_images['image_id'].tolist()),\n",
    "            'num_images': len(job_images),\n",
    "            'classes': ','.join(job_images['class_name'].unique()),\n",
    "            'status': 'pending',\n",
    "            'created_at': datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    job_df = pd.DataFrame(jobs)\n",
    "    print(f\"‚úÖ Job queue created: {len(job_df)} jobs\")\n",
    "    print(f\"   üìä Average job size: {job_df['num_images'].mean():.1f} images\")\n",
    "    print(f\"   üéØ Estimated time: {len(job_df) * 2:.0f} minutes (2min/job)\")\n",
    "    \n",
    "    return job_df\n",
    "\n",
    "# Execute dataset scanning\n",
    "image_manifest = scan_dataset(CONFIG['data_dir'])\n",
    "job_queue = create_job_queue(image_manifest, CONFIG['job_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b1d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèóÔ∏è Feature Extraction Setup\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    \"\"\"Lightweight feature extractor with global pooling\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder_name: str = 'efficientnet_b0', pretrained: bool = True):\n",
    "        super().__init__()\n",
    "        self.encoder_name = encoder_name\n",
    "        \n",
    "        # Load pretrained encoder\n",
    "        self.backbone = timm.create_model(\n",
    "            encoder_name, \n",
    "            pretrained=pretrained,\n",
    "            num_classes=0,  # Remove classifier head\n",
    "            global_pool='avg'  # Global average pooling\n",
    "        )\n",
    "        \n",
    "        # Get feature dimensions\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, 224, 224)\n",
    "            dummy_output = self.backbone(dummy_input)\n",
    "            self.feature_dim = dummy_output.shape[1]\n",
    "        \n",
    "        print(f\"üèóÔ∏è Feature extractor: {encoder_name}\")\n",
    "        print(f\"   üìê Feature dim: {self.feature_dim}\")\n",
    "        print(f\"   üíæ Parameters: {sum(p.numel() for p in self.parameters()):,}\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Extract global features\"\"\"\n",
    "        features = self.backbone(x)  # [B, feature_dim]\n",
    "        return features\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Simple dataset for feature extraction\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths: List[str], transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        \n",
    "        # Load image\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error loading {img_path}: {e}\")\n",
    "            # Return black image as fallback\n",
    "            image = Image.new('RGB', (224, 224), (0, 0, 0))\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, img_path\n",
    "\n",
    "# Initialize feature extractor\n",
    "feature_extractor = FeatureExtractor(CONFIG['encoder_name']).to(device)\n",
    "feature_extractor.eval()\n",
    "\n",
    "# Define transforms (minimal - just resize & normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(f\"‚úÖ Feature extraction setup complete\")\n",
    "print(f\"   üéØ Ready for {CONFIG['job_size']}-image micro-jobs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb3a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üî• Core Job Execution Function\n",
    "\n",
    "def run_feature_job(job_id: int, job_queue: pd.DataFrame, force_rerun: bool = False) -> bool:\n",
    "    \"\"\"Execute single feature extraction job\"\"\"\n",
    "    \n",
    "    if job_id >= len(job_queue):\n",
    "        print(f\"‚ùå Job ID {job_id} out of range (max: {len(job_queue)-1})\")\n",
    "        return False\n",
    "    \n",
    "    job = job_queue.iloc[job_id]\n",
    "    \n",
    "    # Create output directories\n",
    "    features_dir = Path(CONFIG['features_dir'])\n",
    "    encoder_dir = features_dir / f\"encoder_{CONFIG['encoder_name']}\"\n",
    "    encoder_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Check if job already completed\n",
    "    done_file = features_dir / f\"_job_{job_id:04d}_{int(time.time())}.done\"\n",
    "    existing_done = list(features_dir.glob(f\"_job_{job_id:04d}_*.done\"))\n",
    "    \n",
    "    if existing_done and not force_rerun:\n",
    "        print(f\"‚úÖ Job {job_id} already completed: {existing_done[0].name}\")\n",
    "        return True\n",
    "    \n",
    "    print(f\"\\nüöÄ Starting job {job_id}/{len(job_queue)-1}\")\n",
    "    print(f\"   üìä Images: {job['num_images']}\")\n",
    "    print(f\"   üè∑Ô∏è Classes: {job['classes']}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Parse image paths\n",
    "        image_paths = job['image_paths'].split(',')\n",
    "        image_ids = job['image_ids'].split(',')\n",
    "        \n",
    "        # Create dataset and dataloader\n",
    "        dataset = ImageDataset(image_paths, transform=transform)\n",
    "        dataloader = DataLoader(\n",
    "            dataset, \n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=CONFIG['num_workers'],\n",
    "            pin_memory=CONFIG['pin_memory'],\n",
    "            prefetch_factor=CONFIG['prefetch_factor']\n",
    "        )\n",
    "        \n",
    "        # Extract features\n",
    "        all_features = []\n",
    "        all_paths = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_images, batch_paths in tqdm(dataloader, \n",
    "                                                 desc=f\"Job {job_id}\", \n",
    "                                                 leave=False):\n",
    "                batch_images = batch_images.to(device, non_blocking=True)\n",
    "                \n",
    "                # Extract features\n",
    "                features = feature_extractor(batch_images)  # [B, feature_dim]\n",
    "                \n",
    "                # Normalize if requested\n",
    "                if CONFIG['normalize_features']:\n",
    "                    features = torch.nn.functional.normalize(features, p=2, dim=1)\n",
    "                \n",
    "                # Convert to numpy and compress to float16\n",
    "                features_np = features.cpu().numpy().astype(CONFIG['feature_dtype'])\n",
    "                \n",
    "                all_features.append(features_np)\n",
    "                all_paths.extend(batch_paths)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        all_features = np.concatenate(all_features, axis=0)\n",
    "        \n",
    "        print(f\"   ‚úÖ Extracted: {all_features.shape} features\")\n",
    "        \n",
    "        # Save features individually\n",
    "        saved_count = 0\n",
    "        for i, (img_path, img_id) in enumerate(zip(all_paths, image_ids)):\n",
    "            feature_file = encoder_dir / f\"{img_id}.npz\"\n",
    "            \n",
    "            np.savez_compressed(\n",
    "                feature_file,\n",
    "                features=all_features[i],\n",
    "                image_path=img_path,\n",
    "                image_id=img_id,\n",
    "                encoder_name=CONFIG['encoder_name'],\n",
    "                extraction_time=datetime.now().isoformat()\n",
    "            )\n",
    "            saved_count += 1\n",
    "        \n",
    "        # Create completion marker\n",
    "        job_metadata = {\n",
    "            'job_id': job_id,\n",
    "            'num_images': len(image_paths),\n",
    "            'feature_shape': list(all_features.shape),\n",
    "            'processing_time': time.time() - start_time,\n",
    "            'encoder_name': CONFIG['encoder_name'],\n",
    "            'config': CONFIG,\n",
    "            'completed_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(done_file, 'w') as f:\n",
    "            json.dump(job_metadata, f, indent=2)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"‚úÖ Job {job_id} completed: {saved_count} features saved in {elapsed:.1f}s\")\n",
    "        print(f\"   üíæ Output: {encoder_dir}/\")\n",
    "        print(f\"   üèÅ Done marker: {done_file.name}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Job {job_id} failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "print(\"üî• Job execution function ready\")\n",
    "print(\"   Usage: run_feature_job(job_id, job_queue)\")\n",
    "print(\"   Target: <2 minutes per 64-image job\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6527c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ TEST: Single Job Execution\n",
    "# Run this cell to test the pipeline with job 0\n",
    "\n",
    "TEST_JOB_ID = 0\n",
    "\n",
    "print(f\"üß™ Testing job execution with job {TEST_JOB_ID}\")\n",
    "print(f\"Expected: {job_queue.iloc[TEST_JOB_ID]['num_images']} images processed\")\n",
    "\n",
    "# Clear GPU memory before test\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"üßπ GPU memory cleared\")\n",
    "\n",
    "# Run test job\n",
    "success = run_feature_job(TEST_JOB_ID, job_queue, force_rerun=True)\n",
    "\n",
    "if success:\n",
    "    # Verify outputs\n",
    "    encoder_dir = Path(CONFIG['features_dir']) / f\"encoder_{CONFIG['encoder_name']}\"\n",
    "    feature_files = list(encoder_dir.glob('*.npz'))\n",
    "    done_files = list(Path(CONFIG['features_dir']).glob(f'_job_{TEST_JOB_ID:04d}_*.done'))\n",
    "    \n",
    "    print(f\"\\n‚úÖ TEST RESULTS:\")\n",
    "    print(f\"   üìÅ Feature files created: {len(feature_files)}\")\n",
    "    print(f\"   üèÅ Done files created: {len(done_files)}\")\n",
    "    \n",
    "    # Test loading a feature file\n",
    "    if feature_files:\n",
    "        test_feature = np.load(feature_files[0])\n",
    "        print(f\"   üß™ Sample feature shape: {test_feature['features'].shape}\")\n",
    "        print(f\"   üóúÔ∏è Feature dtype: {test_feature['features'].dtype}\")\n",
    "        \n",
    "        # Check memory usage\n",
    "        feature_size = test_feature['features'].nbytes\n",
    "        total_estimated = feature_size * len(image_manifest) / 1e6\n",
    "        print(f\"   üíæ Per-feature size: {feature_size} bytes\")\n",
    "        print(f\"   üìä Estimated total: {total_estimated:.1f}MB for full dataset\")\n",
    "        \n",
    "    print(f\"\\nüéØ Test job completed successfully!\")\n",
    "else:\n",
    "    print(f\"‚ùå Test job failed - check error messages above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669221a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üè≠ Batch Job Execution (Full Pipeline)\n",
    "# WARNING: This will process ALL jobs - use for full feature extraction\n",
    "\n",
    "def run_all_jobs(job_queue: pd.DataFrame, max_jobs: int = None, \n",
    "                 start_job: int = 0) -> Dict:\n",
    "    \"\"\"Execute all feature extraction jobs with progress tracking\"\"\"\n",
    "    \n",
    "    total_jobs = len(job_queue)\n",
    "    if max_jobs:\n",
    "        total_jobs = min(total_jobs, max_jobs)\n",
    "    \n",
    "    print(f\"üè≠ BATCH JOB EXECUTION\")\n",
    "    print(f\"   üìä Total jobs: {total_jobs}\")\n",
    "    print(f\"   üéØ Estimated time: {total_jobs * 2:.0f} minutes\")\n",
    "    print(f\"   üíæ Estimated storage: {total_jobs * CONFIG['job_size'] * 0.05:.1f}MB\")\n",
    "    \n",
    "    results = {\n",
    "        'completed_jobs': [],\n",
    "        'failed_jobs': [],\n",
    "        'total_time': 0,\n",
    "        'total_features': 0\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for job_id in tqdm(range(start_job, min(start_job + total_jobs, len(job_queue))), \n",
    "                       desc=\"Processing jobs\"):\n",
    "        \n",
    "        job_start = time.time()\n",
    "        success = run_feature_job(job_id, job_queue)\n",
    "        job_time = time.time() - job_start\n",
    "        \n",
    "        if success:\n",
    "            results['completed_jobs'].append({\n",
    "                'job_id': job_id,\n",
    "                'time': job_time,\n",
    "                'images': job_queue.iloc[job_id]['num_images']\n",
    "            })\n",
    "            results['total_features'] += job_queue.iloc[job_id]['num_images']\n",
    "        else:\n",
    "            results['failed_jobs'].append(job_id)\n",
    "        \n",
    "        # Clear GPU memory periodically\n",
    "        if job_id % 10 == 0 and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    results['total_time'] = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nüèÅ BATCH EXECUTION COMPLETE\")\n",
    "    print(f\"   ‚úÖ Completed: {len(results['completed_jobs'])}/{total_jobs} jobs\")\n",
    "    print(f\"   ‚ùå Failed: {len(results['failed_jobs'])} jobs\")\n",
    "    print(f\"   ‚è±Ô∏è Total time: {results['total_time']/60:.1f} minutes\")\n",
    "    print(f\"   üñºÔ∏è Total features: {results['total_features']:,}\")\n",
    "    \n",
    "    if results['completed_jobs']:\n",
    "        avg_time = np.mean([j['time'] for j in results['completed_jobs']])\n",
    "        print(f\"   üìä Average job time: {avg_time:.1f}s\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# COMMENTED OUT - UNCOMMENT TO RUN FULL EXTRACTION\n",
    "# This will process all jobs and may take hours!\n",
    "\n",
    "# results = run_all_jobs(job_queue, max_jobs=5)  # Test with 5 jobs first\n",
    "\n",
    "print(\"‚ö†Ô∏è Batch execution commented out for safety\")\n",
    "print(\"Uncomment and modify max_jobs parameter to run full extraction\")\n",
    "print(f\"Total jobs available: {len(job_queue)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e0864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä Feature Manifest Generation\n",
    "\n",
    "def create_feature_manifest(features_dir: str) -> pd.DataFrame:\n",
    "    \"\"\"Create comprehensive manifest of extracted features\"\"\"\n",
    "    \n",
    "    print(f\"üìä Creating feature manifest from {features_dir}\")\n",
    "    \n",
    "    features_path = Path(features_dir)\n",
    "    encoder_dir = features_path / f\"encoder_{CONFIG['encoder_name']}\"\n",
    "    \n",
    "    if not encoder_dir.exists():\n",
    "        print(f\"‚ö†Ô∏è Encoder directory not found: {encoder_dir}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Collect all feature files\n",
    "    feature_files = list(encoder_dir.glob('*.npz'))\n",
    "    print(f\"   üìÅ Found {len(feature_files)} feature files\")\n",
    "    \n",
    "    if not feature_files:\n",
    "        print(\"   ‚ö†Ô∏è No feature files found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    manifest_data = []\n",
    "    \n",
    "    for feature_file in tqdm(feature_files, desc=\"Building manifest\"):\n",
    "        try:\n",
    "            # Load metadata without loading full features\n",
    "            with np.load(feature_file) as data:\n",
    "                manifest_data.append({\n",
    "                    'image_id': str(data['image_id']),\n",
    "                    'image_path': str(data['image_path']),\n",
    "                    'feature_file': str(feature_file),\n",
    "                    'encoder_name': str(data['encoder_name']),\n",
    "                    'feature_shape': data['features'].shape,\n",
    "                    'feature_dtype': str(data['features'].dtype),\n",
    "                    'file_size': feature_file.stat().st_size,\n",
    "                    'extraction_time': str(data['extraction_time']) if 'extraction_time' in data else None,\n",
    "                    'class_name': Path(data['image_path']).parent.name\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Error reading {feature_file}: {e}\")\n",
    "    \n",
    "    if not manifest_data:\n",
    "        print(\"   ‚ùå No valid feature files found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    manifest_df = pd.DataFrame(manifest_data)\n",
    "    \n",
    "    # Add summary statistics\n",
    "    print(f\"\\n‚úÖ Feature manifest created:\")\n",
    "    print(f\"   üìä Total features: {len(manifest_df):,}\")\n",
    "    print(f\"   üè∑Ô∏è Classes: {manifest_df['class_name'].nunique()}\")\n",
    "    print(f\"   üóúÔ∏è Feature dtype: {manifest_df['feature_dtype'].iloc[0]}\")\n",
    "    print(f\"   üìê Feature shape: {manifest_df['feature_shape'].iloc[0]}\")\n",
    "    print(f\"   üíæ Total size: {manifest_df['file_size'].sum() / 1e6:.1f}MB\")\n",
    "    \n",
    "    # Class distribution\n",
    "    class_counts = manifest_df['class_name'].value_counts()\n",
    "    print(f\"\\nüìã Class distribution (top 10):\")\n",
    "    for class_name, count in class_counts.head(10).items():\n",
    "        print(f\"   {class_name}: {count} features\")\n",
    "    \n",
    "    return manifest_df\n",
    "\n",
    "def save_manifest(manifest_df: pd.DataFrame, features_dir: str) -> str:\n",
    "    \"\"\"Save feature manifest to CSV\"\"\"\n",
    "    manifest_file = Path(features_dir) / 'manifest_features.v001.csv'\n",
    "    manifest_df.to_csv(manifest_file, index=False)\n",
    "    \n",
    "    print(f\"üíæ Manifest saved: {manifest_file}\")\n",
    "    return str(manifest_file)\n",
    "\n",
    "# Generate manifest if features exist\n",
    "encoder_dir = Path(CONFIG['features_dir']) / f\"encoder_{CONFIG['encoder_name']}\"\n",
    "if encoder_dir.exists():\n",
    "    manifest = create_feature_manifest(CONFIG['features_dir'])\n",
    "    if not manifest.empty:\n",
    "        manifest_file = save_manifest(manifest, CONFIG['features_dir'])\n",
    "        print(f\"‚úÖ Feature pipeline ready for head-only training!\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No features found - run feature extraction jobs first\")\n",
    "else:\n",
    "    print(f\"üìã Manifest will be created after feature extraction\")\n",
    "    print(f\"Expected location: {CONFIG['features_dir']}/manifest_features.v001.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
