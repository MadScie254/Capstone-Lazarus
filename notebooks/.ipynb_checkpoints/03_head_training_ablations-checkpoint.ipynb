{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9d6b4a1",
   "metadata": {},
   "source": [
    "# ğŸ¯ Head-Only Training Pipeline\n",
    "\n",
    "**Mission**: Ultra-fast classifier training with cached features  \n",
    "**Target**: 3 ablations in <10 minutes, 4GB VRAM optimized  \n",
    "**Strategy**: Load pre-extracted features â†’ train lightweight heads â†’ compare architectures\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ï¸ Pipeline Overview\n",
    "\n",
    "1. **Feature Loading**: Memory-mapped NPZ cache â†’ batch loading\n",
    "2. **Head Architectures**: Linear, MLP, Attention-based classifiers\n",
    "3. **Fast Training**: 10-20 epochs max, early stopping, mixed precision\n",
    "4. **Ablation Studies**: Compare head architectures, learning rates, regularization\n",
    "5. **Model Selection**: Best head â†’ save for ensemble/distillation\n",
    "\n",
    "### ğŸ“Š Performance Targets\n",
    "- **Speed**: <3 minutes per head architecture\n",
    "- **VRAM**: <1.5GB peak (frozen encoder + small head)\n",
    "- **Quality**: Match full training baseline\n",
    "- **Throughput**: 3 architectures Ã— 3 configs = 9 experiments <10min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e482490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Running on CPU - training will be slower\n",
      "ğŸ”§ PyTorch: 2.8.0+cpu\n",
      "ğŸ“ Working dir: C:\\Users\\MadScie254\\Documents\\GitHub\\Capstone-Lazarus\\notebooks\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ Setup & Imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Project imports\n",
    "sys.path.append('../src')\n",
    "\n",
    "# ğŸ® Device & Memory Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"ğŸš€ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "    # Enable optimizations\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "else:\n",
    "    print(\"âš ï¸ Running on CPU - training will be slower\")\n",
    "\n",
    "print(f\"ğŸ”§ PyTorch: {torch.__version__}\")\n",
    "print(f\"ğŸ“ Working dir: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a49d7765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ HEAD TRAINING CONFIGURATION:\n",
      "   ğŸ¬ Batch size: 256 (feature cached)\n",
      "   ğŸ“ˆ Max epochs: 20 (early stop: 5)\n",
      "   ğŸ—ï¸ Head types: ['linear', 'mlp', 'attention']\n",
      "   ğŸ“Š Learning rates: [0.001, 0.0003, 0.0001]\n",
      "   ğŸ”„ Mixed precision: True\n"
     ]
    }
   ],
   "source": [
    "# âš™ï¸ Configuration\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'features_dir': '../features',\n",
    "    'manifest_file': '../features/manifest_features.v001.csv',\n",
    "    'models_dir': '../test_models/head_training',\n",
    "    'encoder_name': 'efficientnet_b0',\n",
    "    \n",
    "    # Training settings (4GB VRAM optimized)\n",
    "    'batch_size': 256,        # Large batch for stability\n",
    "    'max_epochs': 20,         # Fast convergence\n",
    "    'early_stop_patience': 5, # Early stopping\n",
    "    'learning_rates': [1e-3, 3e-4, 1e-4],  # LR ablation\n",
    "    \n",
    "    # Model architectures\n",
    "    'head_types': ['linear', 'mlp', 'attention'],\n",
    "    'dropout_rates': [0.3, 0.5, 0.7],\n",
    "    'hidden_dims': [512, 256, 128],\n",
    "    \n",
    "    # Data settings\n",
    "    'test_size': 0.2,\n",
    "    'val_size': 0.1,\n",
    "    'random_state': 42,\n",
    "    \n",
    "    # Performance\n",
    "    'num_workers': 4,\n",
    "    'pin_memory': True,\n",
    "    'use_amp': True,          # Mixed precision\n",
    "    'compile_model': False,   # PyTorch 2.0 compile (disable for compatibility)\n",
    "}\n",
    "\n",
    "print(\"ğŸ¯ HEAD TRAINING CONFIGURATION:\")\n",
    "print(f\"   ğŸ¬ Batch size: {CONFIG['batch_size']} (feature cached)\")\n",
    "print(f\"   ğŸ“ˆ Max epochs: {CONFIG['max_epochs']} (early stop: {CONFIG['early_stop_patience']})\")\n",
    "print(f\"   ğŸ—ï¸ Head types: {CONFIG['head_types']}\")\n",
    "print(f\"   ğŸ“Š Learning rates: {CONFIG['learning_rates']}\")\n",
    "print(f\"   ğŸ”„ Mixed precision: {CONFIG['use_amp']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f16ab2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Feature manifest not found: ../features/manifest_features.v001.csv\n",
      "   Run feature extraction first (02_feature_extract_microjobs.ipynb)\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“Š Feature Dataset Loading\n",
    "\n",
    "class FeatureDataset(Dataset):\n",
    "    \"\"\"Memory-efficient dataset for cached features\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_files: List[str], labels: List[int], \n",
    "                 cache_features: bool = True):\n",
    "        self.feature_files = feature_files\n",
    "        self.labels = labels\n",
    "        self.cache_features = cache_features\n",
    "        self.feature_cache = {}\n",
    "        \n",
    "        # Load first feature to get dimensions\n",
    "        sample_feature = np.load(feature_files[0])['features']\n",
    "        self.feature_dim = sample_feature.shape[0]\n",
    "        self.feature_dtype = sample_feature.dtype\n",
    "        \n",
    "        print(f\"ğŸ“Š FeatureDataset initialized:\")\n",
    "        print(f\"   ğŸ–¼ï¸ Samples: {len(feature_files)}\")\n",
    "        print(f\"   ğŸ“ Feature dim: {self.feature_dim}\")\n",
    "        print(f\"   ğŸ—œï¸ Dtype: {self.feature_dtype}\")\n",
    "        print(f\"   ğŸ’¾ Caching: {cache_features}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.feature_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        feature_file = self.feature_files[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Check cache first\n",
    "        if self.cache_features and feature_file in self.feature_cache:\n",
    "            features = self.feature_cache[feature_file]\n",
    "        else:\n",
    "            # Load features\n",
    "            try:\n",
    "                data = np.load(feature_file)\n",
    "                features = data['features'].astype(np.float32)  # Ensure float32\n",
    "                \n",
    "                # Cache if enabled\n",
    "                if self.cache_features:\n",
    "                    self.feature_cache[feature_file] = features\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error loading {feature_file}: {e}\")\n",
    "                # Return zero features as fallback\n",
    "                features = np.zeros(self.feature_dim, dtype=np.float32)\n",
    "        \n",
    "        return torch.from_numpy(features), label\n",
    "\n",
    "def load_feature_manifest(manifest_file: str) -> pd.DataFrame:\n",
    "    \"\"\"Load and validate feature manifest\"\"\"\n",
    "    if not Path(manifest_file).exists():\n",
    "        raise FileNotFoundError(f\"Feature manifest not found: {manifest_file}\")\n",
    "    \n",
    "    manifest = pd.read_csv(manifest_file)\n",
    "    print(f\"ğŸ“‹ Loaded manifest: {len(manifest)} features\")\n",
    "    \n",
    "    # Validate feature files exist\n",
    "    missing_files = []\n",
    "    for feature_file in manifest['feature_file']:\n",
    "        if not Path(feature_file).exists():\n",
    "            missing_files.append(feature_file)\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"âš ï¸ Missing {len(missing_files)} feature files\")\n",
    "        # Filter out missing files\n",
    "        manifest = manifest[~manifest['feature_file'].isin(missing_files)]\n",
    "        print(f\"   ğŸ“Š Valid features: {len(manifest)}\")\n",
    "    \n",
    "    return manifest\n",
    "\n",
    "def create_train_val_test_split(manifest: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Create stratified train/val/test splits\"\"\"\n",
    "    \n",
    "    # Prepare labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    manifest['label_encoded'] = label_encoder.fit_transform(manifest['class_name'])\n",
    "    \n",
    "    # First split: train+val vs test\n",
    "    train_val, test = train_test_split(\n",
    "        manifest, \n",
    "        test_size=CONFIG['test_size'],\n",
    "        stratify=manifest['label_encoded'],\n",
    "        random_state=CONFIG['random_state']\n",
    "    )\n",
    "    \n",
    "    # Second split: train vs val\n",
    "    val_size_adjusted = CONFIG['val_size'] / (1 - CONFIG['test_size'])\n",
    "    train, val = train_test_split(\n",
    "        train_val,\n",
    "        test_size=val_size_adjusted,\n",
    "        stratify=train_val['label_encoded'],\n",
    "        random_state=CONFIG['random_state']\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ“Š Data splits:\")\n",
    "    print(f\"   ğŸ“ Train: {len(train)} samples\")\n",
    "    print(f\"   ğŸ” Val: {len(val)} samples\")\n",
    "    print(f\"   ğŸ§ª Test: {len(test)} samples\")\n",
    "    \n",
    "    # Store label encoder\n",
    "    CONFIG['label_encoder'] = label_encoder\n",
    "    CONFIG['num_classes'] = len(label_encoder.classes_)\n",
    "    \n",
    "    print(f\"   ğŸ·ï¸ Classes: {CONFIG['num_classes']}\")\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "# Load feature manifest\n",
    "try:\n",
    "    manifest = load_feature_manifest(CONFIG['manifest_file'])\n",
    "    train_df, val_df, test_df = create_train_val_test_split(manifest)\n",
    "    \n",
    "    print(f\"âœ… Feature loading ready\")\n",
    "    print(f\"   ğŸ“ Features dir: {CONFIG['features_dir']}\")\n",
    "    print(f\"   ğŸ¯ Ready for head training\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"âš ï¸ Feature manifest not found: {CONFIG['manifest_file']}\")\n",
    "    print(f\"   Run feature extraction first (02_feature_extract_microjobs.ipynb)\")\n",
    "    manifest, train_df, val_df, test_df = None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b22ad996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—ï¸ Head architectures defined:\n",
      "   ğŸ“ LinearHead: Simple dropout + linear\n",
      "   ğŸ§  MLPHead: 2-layer MLP with ReLU\n",
      "   ğŸ¯ AttentionHead: Self-attention + residual\n",
      "   âš¡ All heads support mixed precision training\n"
     ]
    }
   ],
   "source": [
    "# ğŸ—ï¸ Head Architecture Definitions\n",
    "\n",
    "class LinearHead(nn.Module):\n",
    "    \"\"\"Simple linear classifier head\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim: int, num_classes: int, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(feature_dim, num_classes)\n",
    "        \n",
    "    def forward(self, features):\n",
    "        features = self.dropout(features)\n",
    "        return self.classifier(features)\n",
    "\n",
    "class MLPHead(nn.Module):\n",
    "    \"\"\"Multi-layer perceptron head\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim: int, num_classes: int, \n",
    "                 hidden_dim: int = 512, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, features):\n",
    "        return self.layers(features)\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"Self-attention based head\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim: int, num_classes: int, \n",
    "                 hidden_dim: int = 256, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.query = nn.Linear(feature_dim, hidden_dim)\n",
    "        self.key = nn.Linear(feature_dim, hidden_dim)\n",
    "        self.value = nn.Linear(feature_dim, hidden_dim)\n",
    "        \n",
    "        # Output layers\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, features):\n",
    "        batch_size = features.size(0)\n",
    "        \n",
    "        # Self-attention (treating each sample as sequence of length 1)\n",
    "        features = features.unsqueeze(1)  # [B, 1, D]\n",
    "        \n",
    "        Q = self.query(features)  # [B, 1, H]\n",
    "        K = self.key(features)    # [B, 1, H]\n",
    "        V = self.value(features)  # [B, 1, H]\n",
    "        \n",
    "        # Attention weights (simplified for single sequence)\n",
    "        attention_weights = torch.softmax(torch.bmm(Q, K.transpose(1, 2)) / (self.hidden_dim ** 0.5), dim=-1)\n",
    "        attended_features = torch.bmm(attention_weights, V).squeeze(1)  # [B, H]\n",
    "        \n",
    "        # Residual connection\n",
    "        attended_features = self.norm(attended_features + self.query(features.squeeze(1)))\n",
    "        \n",
    "        # Classification\n",
    "        attended_features = self.dropout(attended_features)\n",
    "        return self.classifier(attended_features)\n",
    "\n",
    "def create_head_model(head_type: str, feature_dim: int, num_classes: int, \n",
    "                     hidden_dim: int = 512, dropout: float = 0.3) -> nn.Module:\n",
    "    \"\"\"Factory function for creating head models\"\"\"\n",
    "    \n",
    "    if head_type == 'linear':\n",
    "        model = LinearHead(feature_dim, num_classes, dropout)\n",
    "    elif head_type == 'mlp':\n",
    "        model = MLPHead(feature_dim, num_classes, hidden_dim, dropout)\n",
    "    elif head_type == 'attention':\n",
    "        model = AttentionHead(feature_dim, num_classes, hidden_dim, dropout)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown head type: {head_type}\")\n",
    "    \n",
    "    # Initialize weights\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"ğŸ—ï¸ Head architectures defined:\")\n",
    "print(f\"   ğŸ“ LinearHead: Simple dropout + linear\")\n",
    "print(f\"   ğŸ§  MLPHead: 2-layer MLP with ReLU\")\n",
    "print(f\"   ğŸ¯ AttentionHead: Self-attention + residual\")\n",
    "print(f\"   âš¡ All heads support mixed precision training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35bf3047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Training engine ready:\n",
      "   âš¡ Mixed precision: True\n",
      "   ğŸ“Š Early stopping: 5 epochs\n",
      "   ğŸ¯ Target: <3 minutes per head\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ Training Engine\n",
    "\n",
    "def train_head_model(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,\n",
    "                    learning_rate: float, max_epochs: int = 20) -> Dict:\n",
    "    \"\"\"Train head model with early stopping and mixed precision\"\"\"\n",
    "    \n",
    "    # Setup training\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "    scaler = GradScaler() if CONFIG['use_amp'] else None\n",
    "    \n",
    "    # Training state\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    train_history = []\n",
    "    \n",
    "    print(f\"ğŸš€ Starting training: LR={learning_rate}, Epochs={max_epochs}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_features, batch_labels in train_loader:\n",
    "            batch_features = batch_features.to(device, non_blocking=True)\n",
    "            batch_labels = batch_labels.to(device, non_blocking=True)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            if CONFIG['use_amp'] and scaler is not None:\n",
    "                with autocast(device_type='cuda'):\n",
    "                    outputs = model(batch_features)\n",
    "                    loss = criterion(outputs, batch_labels)\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(batch_features)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += batch_labels.size(0)\n",
    "            train_correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        train_acc = train_correct / train_total\n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_features, batch_labels in val_loader:\n",
    "                batch_features = batch_features.to(device, non_blocking=True)\n",
    "                batch_labels = batch_labels.to(device, non_blocking=True)\n",
    "                \n",
    "                if CONFIG['use_amp']:\n",
    "                    with autocast(device_type='cuda'):\n",
    "                        outputs = model(batch_features)\n",
    "                        loss = criterion(outputs, batch_labels)\n",
    "                else:\n",
    "                    outputs = model(batch_features)\n",
    "                    loss = criterion(outputs, batch_labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += batch_labels.size(0)\n",
    "                val_correct += (predicted == batch_labels).sum().item()\n",
    "        \n",
    "        val_acc = val_correct / val_total\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        # Early stopping and best model tracking\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Record history\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        train_history.append({\n",
    "            'epoch': epoch,\n",
    "            'train_loss': avg_train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_loss': avg_val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'lr': optimizer.param_groups[0]['lr'],\n",
    "            'epoch_time': epoch_time\n",
    "        })\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % 5 == 0 or epoch == max_epochs - 1:\n",
    "            print(f\"   Epoch {epoch:2d}: Train={train_acc:.3f}, Val={val_acc:.3f}, \"\n",
    "                  f\"Loss={avg_val_loss:.3f}, Time={epoch_time:.1f}s\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if patience_counter >= CONFIG['early_stop_patience']:\n",
    "            print(f\"   Early stopping at epoch {epoch} (patience={CONFIG['early_stop_patience']})\")\n",
    "            break\n",
    "    \n",
    "    # Restore best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"âœ… Training complete: Best Val Acc = {best_val_acc:.3f}, Time = {total_time:.1f}s\")\n",
    "    \n",
    "    return {\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'total_time': total_time,\n",
    "        'epochs_trained': len(train_history),\n",
    "        'train_history': train_history,\n",
    "        'model_state': best_model_state\n",
    "    }\n",
    "\n",
    "def evaluate_model(model: nn.Module, test_loader: DataLoader) -> Dict:\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    test_loss = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_labels in test_loader:\n",
    "            batch_features = batch_features.to(device, non_blocking=True)\n",
    "            batch_labels = batch_labels.to(device, non_blocking=True)\n",
    "            \n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_acc = accuracy_score(all_labels, all_predictions)\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    \n",
    "    return {\n",
    "        'test_acc': test_acc,\n",
    "        'test_loss': avg_test_loss,\n",
    "        'predictions': all_predictions,\n",
    "        'labels': all_labels\n",
    "    }\n",
    "\n",
    "print(\"ğŸš€ Training engine ready:\")\n",
    "print(f\"   âš¡ Mixed precision: {CONFIG['use_amp']}\")\n",
    "print(f\"   ğŸ“Š Early stopping: {CONFIG['early_stop_patience']} epochs\")\n",
    "print(f\"   ğŸ¯ Target: <3 minutes per head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d46a7b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ Ablation study function ready\n",
      "   ğŸ¯ Target: Complete 9 experiments in <10 minutes\n",
      "   ğŸ† Automated best model selection and saving\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”¬ Ablation Study Execution\n",
    "\n",
    "def run_ablation_study() -> pd.DataFrame:\n",
    "    \"\"\"Run comprehensive head architecture ablation study\"\"\"\n",
    "    \n",
    "    if train_df is None:\n",
    "        print(\"âŒ No training data available - run feature extraction first\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    print(f\"ğŸ”¬ Starting ablation study...\")\n",
    "    print(f\"   ğŸ¯ Architectures: {len(CONFIG['head_types'])}\")\n",
    "    print(f\"   ğŸ“Š Learning rates: {len(CONFIG['learning_rates'])}\")\n",
    "    print(f\"   ğŸ”„ Total experiments: {len(CONFIG['head_types']) * len(CONFIG['learning_rates'])}\")\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = FeatureDataset(\n",
    "        train_df['feature_file'].tolist(),\n",
    "        train_df['label_encoded'].tolist(),\n",
    "        cache_features=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = FeatureDataset(\n",
    "        val_df['feature_file'].tolist(),\n",
    "        val_df['label_encoded'].tolist(),\n",
    "        cache_features=True\n",
    "    )\n",
    "    \n",
    "    test_dataset = FeatureDataset(\n",
    "        test_df['feature_file'].tolist(),\n",
    "        test_df['label_encoded'].tolist(),\n",
    "        cache_features=True\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=CONFIG['batch_size'], shuffle=True,\n",
    "        num_workers=CONFIG['num_workers'], pin_memory=CONFIG['pin_memory']\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, batch_size=CONFIG['batch_size'], shuffle=False,\n",
    "        num_workers=CONFIG['num_workers'], pin_memory=CONFIG['pin_memory']\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=CONFIG['batch_size'], shuffle=False,\n",
    "        num_workers=CONFIG['num_workers'], pin_memory=CONFIG['pin_memory']\n",
    "    )\n",
    "    \n",
    "    # Get feature dimensions\n",
    "    feature_dim = train_dataset.feature_dim\n",
    "    num_classes = CONFIG['num_classes']\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Dataset ready:\")\n",
    "    print(f\"   ğŸ“ Feature dim: {feature_dim}\")\n",
    "    print(f\"   ğŸ·ï¸ Classes: {num_classes}\")\n",
    "    print(f\"   ğŸ“ Train batches: {len(train_loader)}\")\n",
    "    \n",
    "    # Run experiments\n",
    "    results = []\n",
    "    experiment_id = 0\n",
    "    \n",
    "    study_start_time = time.time()\n",
    "    \n",
    "    for head_type in CONFIG['head_types']:\n",
    "        for learning_rate in CONFIG['learning_rates']:\n",
    "            experiment_id += 1\n",
    "            exp_name = f\"{head_type}_lr{learning_rate}\"\n",
    "            \n",
    "            print(f\"\\nğŸ§ª Experiment {experiment_id}: {exp_name}\")\n",
    "            \n",
    "            try:\n",
    "                # Create model\n",
    "                model = create_head_model(\n",
    "                    head_type, feature_dim, num_classes,\n",
    "                    hidden_dim=512, dropout=0.5\n",
    "                ).to(device)\n",
    "                \n",
    "                # Count parameters\n",
    "                param_count = sum(p.numel() for p in model.parameters())\n",
    "                \n",
    "                print(f\"   ğŸ—ï¸ Model: {head_type}, Params: {param_count:,}\")\n",
    "                \n",
    "                # Train model\n",
    "                train_results = train_head_model(\n",
    "                    model, train_loader, val_loader, \n",
    "                    learning_rate, CONFIG['max_epochs']\n",
    "                )\n",
    "                \n",
    "                # Test model\n",
    "                test_results = evaluate_model(model, test_loader)\n",
    "                \n",
    "                # Store results\n",
    "                result = {\n",
    "                    'experiment_id': experiment_id,\n",
    "                    'experiment_name': exp_name,\n",
    "                    'head_type': head_type,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'param_count': param_count,\n",
    "                    'best_val_acc': train_results['best_val_acc'],\n",
    "                    'test_acc': test_results['test_acc'],\n",
    "                    'train_time': train_results['total_time'],\n",
    "                    'epochs_trained': train_results['epochs_trained'],\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                results.append(result)\n",
    "                \n",
    "                print(f\"   âœ… {exp_name}: Val={train_results['best_val_acc']:.3f}, \"\n",
    "                      f\"Test={test_results['test_acc']:.3f}, Time={train_results['total_time']:.1f}s\")\n",
    "                \n",
    "                # Save best models\n",
    "                if len(results) == 1 or test_results['test_acc'] > max(r['test_acc'] for r in results[:-1]):\n",
    "                    model_path = Path(CONFIG['models_dir']) / f\"best_head_{exp_name}.pth\"\n",
    "                    model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'model_config': {\n",
    "                            'head_type': head_type,\n",
    "                            'feature_dim': feature_dim,\n",
    "                            'num_classes': num_classes,\n",
    "                            'hidden_dim': 512,\n",
    "                            'dropout': 0.5\n",
    "                        },\n",
    "                        'results': result,\n",
    "                        'label_encoder_classes': CONFIG['label_encoder'].classes_.tolist()\n",
    "                    }, model_path)\n",
    "                    print(f\"   ğŸ’¾ Best model saved: {model_path.name}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   âŒ Experiment failed: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            \n",
    "            # Clear GPU memory\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    total_time = time.time() - study_start_time\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    print(f\"\\nğŸ Ablation study complete:\")\n",
    "    print(f\"   â±ï¸ Total time: {total_time/60:.1f} minutes\")\n",
    "    print(f\"   ğŸ§ª Experiments: {len(results)}/{len(CONFIG['head_types']) * len(CONFIG['learning_rates'])}\")\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        best_result = results_df.loc[results_df['test_acc'].idxmax()]\n",
    "        print(f\"   ğŸ† Best model: {best_result['experiment_name']} ({best_result['test_acc']:.3f} test acc)\")\n",
    "        print(f\"   ğŸ“Š Average experiment time: {results_df['train_time'].mean():.1f}s\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "print(\"ğŸ”¬ Ablation study function ready\")\n",
    "print(f\"   ğŸ¯ Target: Complete 9 experiments in <10 minutes\")\n",
    "print(f\"   ğŸ† Automated best model selection and saving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ca7010b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ Ablation study skipped - no training data available\n",
      "   Run feature extraction first (02_feature_extract_microjobs.ipynb)\n",
      "   Expected workflow:\n",
      "   1. Feature extraction â†’ cached features\n",
      "   2. Head training â†’ this notebook\n",
      "   3. Results â†’ best head architecture for ensemble\n"
     ]
    }
   ],
   "source": [
    "# ğŸš€ RUN ABLATION STUDY\n",
    "# Execute complete head architecture comparison\n",
    "\n",
    "if train_df is not None:\n",
    "    print(\"ğŸš€ EXECUTING ABLATION STUDY\")\n",
    "    print(f\"Expected time: <10 minutes for {len(CONFIG['head_types']) * len(CONFIG['learning_rates'])} experiments\")\n",
    "    \n",
    "    # Clear memory before starting\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(f\"ğŸ§¹ GPU memory cleared\")\n",
    "    \n",
    "    # Run the full ablation study\n",
    "    ablation_results = run_ablation_study()\n",
    "    \n",
    "    # Save results\n",
    "    if not ablation_results.empty:\n",
    "        results_dir = Path(CONFIG['models_dir'])\n",
    "        results_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        results_file = results_dir / f\"ablation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "        ablation_results.to_csv(results_file, index=False)\n",
    "        \n",
    "        print(f\"\\nğŸ’¾ Results saved: {results_file}\")\n",
    "        \n",
    "        # Display summary table\n",
    "        print(f\"\\nğŸ“Š ABLATION RESULTS SUMMARY:\")\n",
    "        summary_cols = ['experiment_name', 'head_type', 'learning_rate', \n",
    "                       'best_val_acc', 'test_acc', 'train_time', 'param_count']\n",
    "        if all(col in ablation_results.columns for col in summary_cols):\n",
    "            display_df = ablation_results[summary_cols].copy()\n",
    "            display_df['train_time'] = display_df['train_time'].round(1)\n",
    "            display_df = display_df.sort_values('test_acc', ascending=False)\n",
    "            print(display_df.to_string(index=False))\n",
    "        \n",
    "        print(f\"\\nğŸ¯ PHASE C COMPLETE: Head-only training pipeline ready!\")\n",
    "        print(f\"   âœ… {len(ablation_results)} experiments completed\")\n",
    "        print(f\"   ğŸ† Best head architecture identified and saved\")\n",
    "        print(f\"   âš¡ Average training time: {ablation_results['train_time'].mean():.1f}s per experiment\")\n",
    "        \n",
    "    else:\n",
    "        print(\"âŒ No successful experiments - check feature extraction and data loading\")\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸ Ablation study skipped - no training data available\")\n",
    "    print(\"   Run feature extraction first (02_feature_extract_microjobs.ipynb)\")\n",
    "    print(\"   Expected workflow:\")\n",
    "    print(\"   1. Feature extraction â†’ cached features\")\n",
    "    print(\"   2. Head training â†’ this notebook\")\n",
    "    print(\"   3. Results â†’ best head architecture for ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3206f287-c1e2-44b4-b83c-532174fff7de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
