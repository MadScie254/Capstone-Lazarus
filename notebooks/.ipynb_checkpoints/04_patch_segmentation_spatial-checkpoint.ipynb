{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6db7bcde",
   "metadata": {},
   "source": [
    "# ğŸ” Patch Segmentation Pipeline\n",
    "\n",
    "**Mission**: Extract spatial features for fine-grained disease detection  \n",
    "**Target**: 16x16 patches, attention mapping, 4GB VRAM optimized  \n",
    "**Strategy**: Sliding window â†’ spatial features â†’ attention visualization â†’ enhanced training\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ºï¸ Pipeline Overview\n",
    "\n",
    "1. **Patch Extraction**: 16x16 sliding windows from 224x224 images\n",
    "2. **Spatial Features**: Extract features per patch using cached encoder\n",
    "3. **Attention Mapping**: Learn patch importance for disease regions\n",
    "4. **Feature Enhancement**: Combine global + spatial for better accuracy\n",
    "5. **Visualization**: Generate attention heatmaps for interpretability\n",
    "\n",
    "### ğŸ¯ Performance Targets\n",
    "- **Memory**: <3GB VRAM (batch processing patches)\n",
    "- **Speed**: <5 minutes per 1000 images\n",
    "- **Quality**: Spatial attention maps for disease localization\n",
    "- **Integration**: Compatible with existing head training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b772f974",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _imaging: The operating system cannot run %1.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtransforms\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransforms\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image, ImageDraw\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torchvision\\__init__.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _meta_registrations, datasets, io, models, ops, transforms, utils  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torchvision\\datasets\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_optical_flow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FlyingChairs, FlyingThings3D, HD1K, KittiFlow, Sintel\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_stereo_matching\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     CarlaStereo,\n\u001b[32m      4\u001b[39m     CREStereo,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     SintelStereo,\n\u001b[32m     13\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcaltech\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Caltech101, Caltech256\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\torchvision\\datasets\\_optical_flow.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m decode_png, read_file\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfolder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m default_loader\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\PIL\\Image.py:90\u001b[39m\n\u001b[32m     81\u001b[39m MAX_IMAGE_PIXELS: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28mint\u001b[39m(\u001b[32m1024\u001b[39m * \u001b[32m1024\u001b[39m * \u001b[32m1024\u001b[39m // \u001b[32m4\u001b[39m // \u001b[32m3\u001b[39m)\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     85\u001b[39m     \u001b[38;5;66;03m# If the _imaging C module is not present, Pillow will not load.\u001b[39;00m\n\u001b[32m     86\u001b[39m     \u001b[38;5;66;03m# Note that other modules should not refer to _imaging directly;\u001b[39;00m\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# import Image and use the Image.core variable instead.\u001b[39;00m\n\u001b[32m     88\u001b[39m     \u001b[38;5;66;03m# Also note that Image.core is not a publicly documented interface,\u001b[39;00m\n\u001b[32m     89\u001b[39m     \u001b[38;5;66;03m# and should be considered private and subject to change.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _imaging \u001b[38;5;28;01mas\u001b[39;00m core\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m __version__ != \u001b[38;5;28mgetattr\u001b[39m(core, \u001b[33m\"\u001b[39m\u001b[33mPILLOW_VERSION\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     93\u001b[39m         msg = (\n\u001b[32m     94\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe _imaging extension was built for another version of Pillow or PIL:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     95\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCore version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mgetattr\u001b[39m(core,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mPILLOW_VERSION\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     96\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPillow version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     97\u001b[39m         )\n",
      "\u001b[31mImportError\u001b[39m: DLL load failed while importing _imaging: The operating system cannot run %1."
     ]
    }
   ],
   "source": [
    "# ğŸ”§ Setup & Imports\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Project imports\n",
    "sys.path.append('../src')\n",
    "\n",
    "# ğŸ® Device & Memory Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"ğŸš€ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ğŸ’¾ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "else:\n",
    "    print(\"âš ï¸ Running on CPU - patch extraction will be slower\")\n",
    "\n",
    "print(f\"ğŸ”§ PyTorch: {torch.__version__}\")\n",
    "print(f\"ğŸ“ Working dir: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706cbbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âš™ï¸ Configuration\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'data_dir': '../data',\n",
    "    'features_dir': '../features',\n",
    "    'spatial_features_dir': '../features/spatial',\n",
    "    'encoder_name': 'efficientnet_b0',\n",
    "    \n",
    "    # Patch extraction\n",
    "    'patch_size': 16,         # 16x16 patches\n",
    "    'stride': 8,              # 50% overlap\n",
    "    'img_size': 224,          # Input image size\n",
    "    'min_patch_var': 0.01,    # Skip uniform patches\n",
    "    \n",
    "    # Processing (4GB VRAM optimized)\n",
    "    'batch_size': 32,         # Patches per batch\n",
    "    'max_patches_per_image': 196,  # 14x14 grid max\n",
    "    'feature_dtype': 'float16',\n",
    "    \n",
    "    # Attention learning\n",
    "    'attention_dim': 128,     # Attention hidden size\n",
    "    'temperature': 0.1,       # Attention softmax temperature\n",
    "    'top_k_patches': 32,      # Keep top-k patches\n",
    "    \n",
    "    # Performance\n",
    "    'num_workers': 4,\n",
    "    'pin_memory': True,\n",
    "    'save_attention_maps': True,\n",
    "}\n",
    "\n",
    "print(\"ğŸ” PATCH SEGMENTATION CONFIGURATION:\")\n",
    "print(f\"   ğŸ“ Patch size: {CONFIG['patch_size']}x{CONFIG['patch_size']}\")\n",
    "print(f\"   ğŸ‘£ Stride: {CONFIG['stride']} (overlap: {(CONFIG['patch_size']-CONFIG['stride'])/CONFIG['patch_size']*100:.0f}%)\")\n",
    "print(f\"   ğŸ¬ Batch size: {CONFIG['batch_size']} patches\")\n",
    "print(f\"   ğŸ¯ Max patches: {CONFIG['max_patches_per_image']} per image\")\n",
    "print(f\"   ğŸ§  Attention dim: {CONFIG['attention_dim']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f35bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”ª Patch Extraction Functions\n",
    "\n",
    "def extract_patches_from_image(image: np.ndarray, patch_size: int = 16, \n",
    "                              stride: int = 8, min_var: float = 0.01) -> Tuple[np.ndarray, List[Tuple]]:\n",
    "    \"\"\"Extract patches from image with position tracking\"\"\"\n",
    "    \n",
    "    height, width = image.shape[:2]\n",
    "    patches = []\n",
    "    positions = []\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    h_steps = (height - patch_size) // stride + 1\n",
    "    w_steps = (width - patch_size) // stride + 1\n",
    "    \n",
    "    for i in range(h_steps):\n",
    "        for j in range(w_steps):\n",
    "            # Extract patch\n",
    "            y = i * stride\n",
    "            x = j * stride\n",
    "            patch = image[y:y+patch_size, x:x+patch_size]\n",
    "            \n",
    "            # Skip uniform patches (likely background)\n",
    "            if len(patch.shape) == 3:\n",
    "                patch_var = np.var(patch.astype(np.float32)) / 255.0\n",
    "            else:\n",
    "                patch_var = np.var(patch.astype(np.float32))\n",
    "            \n",
    "            if patch_var > min_var:\n",
    "                patches.append(patch)\n",
    "                positions.append((y, x, y+patch_size, x+patch_size))  # (y1, x1, y2, x2)\n",
    "    \n",
    "    if patches:\n",
    "        patches = np.stack(patches, axis=0)\n",
    "    else:\n",
    "        patches = np.empty((0, patch_size, patch_size, image.shape[2] if len(image.shape) == 3 else 1))\n",
    "    \n",
    "    return patches, positions\n",
    "\n",
    "def visualize_patch_grid(image_path: str, patch_size: int = 16, stride: int = 8, \n",
    "                        max_patches_show: int = 50) -> None:\n",
    "    \"\"\"Visualize patch extraction grid on sample image\"\"\"\n",
    "    \n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"âš ï¸ Could not load image: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Extract patches\n",
    "    patches, positions = extract_patches_from_image(image_rgb, patch_size, stride)\n",
    "    \n",
    "    print(f\"ğŸ“Š Patch extraction results:\")\n",
    "    print(f\"   ğŸ–¼ï¸ Image: {image_rgb.shape}\")\n",
    "    print(f\"   ğŸ”ª Patches: {len(patches)}\")\n",
    "    print(f\"   ğŸ“ Patch shape: {patches[0].shape if len(patches) > 0 else 'None'}\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(image_rgb)\n",
    "    axes[0].set_title(f\"Original Image\\n{image_rgb.shape}\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Patch grid overlay\n",
    "    axes[1].imshow(image_rgb)\n",
    "    for i, (y1, x1, y2, x2) in enumerate(positions[:max_patches_show]):\n",
    "        rect = Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                        linewidth=1, edgecolor='red', facecolor='none', alpha=0.7)\n",
    "        axes[1].add_patch(rect)\n",
    "    axes[1].set_title(f\"Patch Grid\\n{len(positions)} patches\")\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Sample patches\n",
    "    if len(patches) > 0:\n",
    "        # Create grid of patches\n",
    "        n_show = min(64, len(patches))\n",
    "        grid_size = int(np.ceil(np.sqrt(n_show)))\n",
    "        patch_grid = np.zeros((grid_size * patch_size, grid_size * patch_size, 3), dtype=np.uint8)\n",
    "        \n",
    "        for i in range(n_show):\n",
    "            row = i // grid_size\n",
    "            col = i % grid_size\n",
    "            y_start = row * patch_size\n",
    "            x_start = col * patch_size\n",
    "            patch_grid[y_start:y_start+patch_size, x_start:x_start+patch_size] = patches[i]\n",
    "        \n",
    "        axes[2].imshow(patch_grid)\n",
    "        axes[2].set_title(f\"Sample Patches\\n{n_show}/{len(patches)}\")\n",
    "        axes[2].axis('off')\n",
    "    else:\n",
    "        axes[2].text(0.5, 0.5, 'No patches extracted', ha='center', va='center')\n",
    "        axes[2].set_title(\"No Patches\")\n",
    "        axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"ğŸ”ª Patch extraction functions ready\")\n",
    "print(f\"   ğŸ“ Grid: {CONFIG['patch_size']}x{CONFIG['patch_size']} patches\")\n",
    "print(f\"   ğŸ‘£ Stride: {CONFIG['stride']} pixels\")\n",
    "print(f\"   ğŸ¯ Variance filter: >{CONFIG['min_patch_var']} to skip background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d631027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª TEST: Patch Extraction Visualization\n",
    "# Test patch extraction on sample images\n",
    "\n",
    "# Find sample images from different classes\n",
    "data_path = Path(CONFIG['data_dir'])\n",
    "sample_images = []\n",
    "\n",
    "for class_dir in data_path.iterdir():\n",
    "    if class_dir.is_dir():\n",
    "        image_files = list(class_dir.glob('*.jpg')) + list(class_dir.glob('*.JPG'))\n",
    "        if image_files:\n",
    "            sample_images.append((str(image_files[0]), class_dir.name))\n",
    "        if len(sample_images) >= 3:\n",
    "            break\n",
    "\n",
    "print(f\"ğŸ§ª Testing patch extraction on {len(sample_images)} sample images:\")\n",
    "\n",
    "for img_path, class_name in sample_images:\n",
    "    print(f\"\\nğŸ“Š Class: {class_name}\")\n",
    "    print(f\"   ğŸ“ File: {Path(img_path).name}\")\n",
    "    \n",
    "    try:\n",
    "        visualize_patch_grid(img_path, CONFIG['patch_size'], CONFIG['stride'])\n",
    "    except Exception as e:\n",
    "        print(f\"   âš ï¸ Visualization failed: {e}\")\n",
    "\n",
    "if not sample_images:\n",
    "    print(\"âš ï¸ No sample images found - check data directory path\")\n",
    "    print(f\"   Expected: {CONFIG['data_dir']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595b1f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ—ï¸ Spatial Feature Extraction Architecture\n",
    "\n",
    "class PatchFeatureExtractor(nn.Module):\n",
    "    \"\"\"Extract features from image patches using pre-trained encoder\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder_name: str = 'efficientnet_b0'):\n",
    "        super().__init__()\n",
    "        import timm\n",
    "        \n",
    "        self.encoder_name = encoder_name\n",
    "        self.backbone = timm.create_model(\n",
    "            encoder_name,\n",
    "            pretrained=True,\n",
    "            num_classes=0,  # Remove classifier\n",
    "            global_pool='avg'\n",
    "        )\n",
    "        \n",
    "        # Get feature dimensions\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, 3, CONFIG['patch_size'], CONFIG['patch_size'])\n",
    "            dummy_output = self.backbone(dummy_input)\n",
    "            self.feature_dim = dummy_output.shape[1]\n",
    "        \n",
    "        print(f\"ğŸ—ï¸ PatchFeatureExtractor:\")\n",
    "        print(f\"   ğŸ“ Input: {CONFIG['patch_size']}x{CONFIG['patch_size']}\")\n",
    "        print(f\"   ğŸ§  Encoder: {encoder_name}\")\n",
    "        print(f\"   ğŸ“Š Feature dim: {self.feature_dim}\")\n",
    "    \n",
    "    def forward(self, patches):\n",
    "        \"\"\"Extract features from batch of patches\"\"\"\n",
    "        return self.backbone(patches)\n",
    "\n",
    "class SpatialAttentionHead(nn.Module):\n",
    "    \"\"\"Learn attention weights for spatial patches\"\"\"\n",
    "    \n",
    "    def __init__(self, feature_dim: int, attention_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        \n",
    "        # Attention network\n",
    "        self.attention_net = nn.Sequential(\n",
    "            nn.Linear(feature_dim, attention_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(attention_dim, attention_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(attention_dim, 1)  # Single attention score\n",
    "        )\n",
    "        \n",
    "        print(f\"ğŸ¯ SpatialAttentionHead:\")\n",
    "        print(f\"   ğŸ“ Feature dim: {feature_dim}\")\n",
    "        print(f\"   ğŸ§  Attention dim: {attention_dim}\")\n",
    "    \n",
    "    def forward(self, patch_features, temperature: float = 0.1):\n",
    "        \"\"\"Compute attention weights for patches\"\"\"\n",
    "        # patch_features: [num_patches, feature_dim]\n",
    "        attention_scores = self.attention_net(patch_features)  # [num_patches, 1]\n",
    "        attention_weights = F.softmax(attention_scores / temperature, dim=0)  # [num_patches, 1]\n",
    "        \n",
    "        # Weighted feature aggregation\n",
    "        attended_features = (patch_features * attention_weights).sum(dim=0)  # [feature_dim]\n",
    "        \n",
    "        return attended_features, attention_weights.squeeze()\n",
    "\n",
    "class SpatialFeaturePipeline(nn.Module):\n",
    "    \"\"\"Complete spatial feature extraction pipeline\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder_name: str = 'efficientnet_b0', \n",
    "                 attention_dim: int = 128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_extractor = PatchFeatureExtractor(encoder_name)\n",
    "        self.attention_head = SpatialAttentionHead(\n",
    "            self.patch_extractor.feature_dim, attention_dim\n",
    "        )\n",
    "        \n",
    "        self.feature_dim = self.patch_extractor.feature_dim\n",
    "        \n",
    "    def forward(self, patches, temperature: float = 0.1):\n",
    "        \"\"\"Process patches and return attended features\"\"\"\n",
    "        # Extract features from all patches\n",
    "        patch_features = self.patch_extractor(patches)  # [num_patches, feature_dim]\n",
    "        \n",
    "        # Apply attention\n",
    "        attended_features, attention_weights = self.attention_head(\n",
    "            patch_features, temperature\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'attended_features': attended_features,\n",
    "            'attention_weights': attention_weights,\n",
    "            'patch_features': patch_features\n",
    "        }\n",
    "\n",
    "# Initialize spatial pipeline\n",
    "spatial_pipeline = SpatialFeaturePipeline(\n",
    "    CONFIG['encoder_name'], \n",
    "    CONFIG['attention_dim']\n",
    ").to(device)\n",
    "\n",
    "spatial_pipeline.eval()  # Inference mode for feature extraction\n",
    "\n",
    "print(f\"\\nâœ… Spatial feature pipeline ready\")\n",
    "print(f\"   ğŸ“ Feature dim: {spatial_pipeline.feature_dim}\")\n",
    "print(f\"   ğŸ¯ Attention-weighted spatial features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8bb0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ”¥ Spatial Feature Extraction Engine\n",
    "\n",
    "def extract_spatial_features_from_image(image_path: str, \n",
    "                                       pipeline: SpatialFeaturePipeline,\n",
    "                                       save_attention_map: bool = True) -> Dict:\n",
    "    \"\"\"Extract spatial features from single image\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Load and preprocess image\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"Could not load image: {image_path}\")\n",
    "        \n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Resize to target size\n",
    "        image_resized = cv2.resize(image_rgb, (CONFIG['img_size'], CONFIG['img_size']))\n",
    "        \n",
    "        # Extract patches\n",
    "        patches, positions = extract_patches_from_image(\n",
    "            image_resized, \n",
    "            CONFIG['patch_size'], \n",
    "            CONFIG['stride'],\n",
    "            CONFIG['min_patch_var']\n",
    "        )\n",
    "        \n",
    "        if len(patches) == 0:\n",
    "            # Return zero features if no patches\n",
    "            return {\n",
    "                'spatial_features': np.zeros(pipeline.feature_dim, dtype=np.float32),\n",
    "                'attention_weights': np.array([]),\n",
    "                'patch_positions': [],\n",
    "                'num_patches': 0,\n",
    "                'error': 'no_patches'\n",
    "            }\n",
    "        \n",
    "        # Limit patches to avoid memory issues\n",
    "        if len(patches) > CONFIG['max_patches_per_image']:\n",
    "            # Keep patches with highest variance (most informative)\n",
    "            patch_vars = [np.var(patch.astype(np.float32)) for patch in patches]\n",
    "            top_indices = np.argsort(patch_vars)[-CONFIG['max_patches_per_image']:]\n",
    "            patches = patches[top_indices]\n",
    "            positions = [positions[i] for i in top_indices]\n",
    "        \n",
    "        # Preprocess patches for model\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        # Convert patches to tensors in batches (memory efficient)\n",
    "        patch_tensors = []\n",
    "        for patch in patches:\n",
    "            patch_pil = Image.fromarray(patch.astype(np.uint8))\n",
    "            patch_tensor = transform(patch_pil)\n",
    "            patch_tensors.append(patch_tensor)\n",
    "        \n",
    "        patch_batch = torch.stack(patch_tensors).to(device)\n",
    "        \n",
    "        # Extract spatial features with attention\n",
    "        with torch.no_grad():\n",
    "            results = pipeline(patch_batch, CONFIG['temperature'])\n",
    "        \n",
    "        # Convert to numpy\n",
    "        spatial_features = results['attended_features'].cpu().numpy().astype(CONFIG['feature_dtype'])\n",
    "        attention_weights = results['attention_weights'].cpu().numpy()\n",
    "        \n",
    "        # Create attention map if requested\n",
    "        attention_map = None\n",
    "        if save_attention_map:\n",
    "            attention_map = create_attention_heatmap(\n",
    "                image_resized, positions, attention_weights, CONFIG['patch_size']\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            'spatial_features': spatial_features,\n",
    "            'attention_weights': attention_weights,\n",
    "            'patch_positions': positions,\n",
    "            'num_patches': len(patches),\n",
    "            'attention_map': attention_map,\n",
    "            'image_shape': image_resized.shape\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error processing {image_path}: {e}\")\n",
    "        return {\n",
    "            'spatial_features': np.zeros(pipeline.feature_dim, dtype=np.float32),\n",
    "            'attention_weights': np.array([]),\n",
    "            'patch_positions': [],\n",
    "            'num_patches': 0,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "def create_attention_heatmap(image: np.ndarray, positions: List[Tuple], \n",
    "                           attention_weights: np.ndarray, patch_size: int) -> np.ndarray:\n",
    "    \"\"\"Create attention heatmap overlay\"\"\"\n",
    "    \n",
    "    height, width = image.shape[:2]\n",
    "    heatmap = np.zeros((height, width), dtype=np.float32)\n",
    "    \n",
    "    # Fill heatmap with attention weights\n",
    "    for (y1, x1, y2, x2), weight in zip(positions, attention_weights):\n",
    "        heatmap[y1:y2, x1:x2] += weight\n",
    "    \n",
    "    # Normalize heatmap\n",
    "    if heatmap.max() > 0:\n",
    "        heatmap = heatmap / heatmap.max()\n",
    "    \n",
    "    return heatmap\n",
    "\n",
    "def visualize_spatial_attention(image_path: str, results: Dict) -> None:\n",
    "    \"\"\"Visualize spatial attention results\"\"\"\n",
    "    \n",
    "    if results.get('error'):\n",
    "        print(f\"âš ï¸ Cannot visualize - error: {results['error']}\")\n",
    "        return\n",
    "    \n",
    "    # Load original image\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_resized = cv2.resize(image_rgb, (CONFIG['img_size'], CONFIG['img_size']))\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Original image\n",
    "    axes[0].imshow(image_resized)\n",
    "    axes[0].set_title(f\"Original Image\\n{Path(image_path).name}\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Attention heatmap\n",
    "    if results['attention_map'] is not None:\n",
    "        im = axes[1].imshow(results['attention_map'], cmap='hot', alpha=0.7)\n",
    "        axes[1].imshow(image_resized, alpha=0.3)\n",
    "        axes[1].set_title(f\"Attention Heatmap\\n{results['num_patches']} patches\")\n",
    "        axes[1].axis('off')\n",
    "        plt.colorbar(im, ax=axes[1], fraction=0.046, pad=0.04)\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, 'No attention map', ha='center', va='center')\n",
    "        axes[1].set_title(\"No Attention Map\")\n",
    "        axes[1].axis('off')\n",
    "    \n",
    "    # Top attention patches\n",
    "    if len(results['attention_weights']) > 0:\n",
    "        # Get top-k patches by attention\n",
    "        top_k = min(16, len(results['attention_weights']))\n",
    "        top_indices = np.argsort(results['attention_weights'])[-top_k:]\n",
    "        \n",
    "        # Show patch attention distribution\n",
    "        axes[2].hist(results['attention_weights'], bins=20, alpha=0.7, color='blue')\n",
    "        axes[2].axvline(results['attention_weights'].mean(), color='red', linestyle='--', \n",
    "                       label=f'Mean: {results[\"attention_weights\"].mean():.3f}')\n",
    "        axes[2].set_xlabel('Attention Weight')\n",
    "        axes[2].set_ylabel('Frequency')\n",
    "        axes[2].set_title(f'Attention Distribution\\nTop weight: {results[\"attention_weights\"].max():.3f}')\n",
    "        axes[2].legend()\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[2].text(0.5, 0.5, 'No attention weights', ha='center', va='center')\n",
    "        axes[2].set_title(\"No Attention Data\")\n",
    "        axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"ğŸ“Š Spatial Feature Results:\")\n",
    "    print(f\"   ğŸ”ª Patches: {results['num_patches']}\")\n",
    "    print(f\"   ğŸ“ Feature shape: {results['spatial_features'].shape}\")\n",
    "    print(f\"   ğŸ¯ Attention range: {results['attention_weights'].min():.4f} - {results['attention_weights'].max():.4f}\")\n",
    "    print(f\"   ğŸ’¾ Feature dtype: {results['spatial_features'].dtype}\")\n",
    "\n",
    "print(\"ğŸ”¥ Spatial feature extraction engine ready\")\n",
    "print(f\"   ğŸ¯ Attention-weighted features\")\n",
    "print(f\"   ğŸ“Š Heatmap visualization\")\n",
    "print(f\"   ğŸ’¾ Memory-efficient batch processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5ce804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ§ª TEST: Spatial Feature Extraction\n",
    "# Test spatial features on sample images\n",
    "\n",
    "if sample_images:\n",
    "    print(\"ğŸ§ª Testing spatial feature extraction:\")\n",
    "    \n",
    "    for img_path, class_name in sample_images[:2]:  # Test on first 2 images\n",
    "        print(f\"\\nğŸ“Š Processing: {class_name} - {Path(img_path).name}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Extract spatial features\n",
    "        results = extract_spatial_features_from_image(\n",
    "            img_path, spatial_pipeline, save_attention_map=True\n",
    "        )\n",
    "        \n",
    "        processing_time = time.time() - start_time\n",
    "        print(f\"   â±ï¸ Processing time: {processing_time:.2f}s\")\n",
    "        \n",
    "        # Visualize results\n",
    "        visualize_spatial_attention(img_path, results)\n",
    "        \n",
    "        # Memory check\n",
    "        if torch.cuda.is_available():\n",
    "            memory_used = torch.cuda.memory_allocated() / 1e9\n",
    "            print(f\"   ğŸ’¾ GPU memory: {memory_used:.2f}GB\")\n",
    "        \n",
    "        # Clear GPU memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "else:\n",
    "    print(\"âš ï¸ No sample images available for testing\")\n",
    "    print(\"   Ensure data directory contains image files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a9467e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ’¾ Spatial Feature Caching System\n",
    "\n",
    "def save_spatial_features(image_id: str, results: Dict, output_dir: str) -> str:\n",
    "    \"\"\"Save spatial features to NPZ file\"\"\"\n",
    "    \n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    feature_file = output_path / f\"{image_id}_spatial.npz\"\n",
    "    \n",
    "    # Prepare data for saving\n",
    "    save_data = {\n",
    "        'spatial_features': results['spatial_features'],\n",
    "        'attention_weights': results['attention_weights'],\n",
    "        'num_patches': results['num_patches'],\n",
    "        'patch_positions': results['patch_positions'],\n",
    "        'extraction_time': datetime.now().isoformat(),\n",
    "        'config': {\n",
    "            'patch_size': CONFIG['patch_size'],\n",
    "            'stride': CONFIG['stride'],\n",
    "            'attention_dim': CONFIG['attention_dim'],\n",
    "            'encoder_name': CONFIG['encoder_name']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save attention map if available\n",
    "    if results.get('attention_map') is not None:\n",
    "        save_data['attention_map'] = results['attention_map']\n",
    "    \n",
    "    np.savez_compressed(feature_file, **save_data)\n",
    "    \n",
    "    return str(feature_file)\n",
    "\n",
    "def load_spatial_features(feature_file: str) -> Dict:\n",
    "    \"\"\"Load spatial features from NPZ file\"\"\"\n",
    "    \n",
    "    try:\n",
    "        data = np.load(feature_file, allow_pickle=True)\n",
    "        \n",
    "        return {\n",
    "            'spatial_features': data['spatial_features'],\n",
    "            'attention_weights': data['attention_weights'],\n",
    "            'num_patches': int(data['num_patches']),\n",
    "            'patch_positions': data['patch_positions'].tolist(),\n",
    "            'attention_map': data.get('attention_map'),\n",
    "            'extraction_time': str(data['extraction_time']),\n",
    "            'config': data['config'].item() if 'config' in data else {}\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error loading spatial features from {feature_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "def batch_extract_spatial_features(manifest_df: pd.DataFrame, \n",
    "                                  max_images: int = None) -> pd.DataFrame:\n",
    "    \"\"\"Extract spatial features for multiple images\"\"\"\n",
    "    \n",
    "    if manifest_df is None or manifest_df.empty:\n",
    "        print(\"âŒ No manifest data available\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Limit processing if requested\n",
    "    if max_images:\n",
    "        manifest_df = manifest_df.head(max_images)\n",
    "    \n",
    "    print(f\"ğŸ­ Batch spatial feature extraction:\")\n",
    "    print(f\"   ğŸ“Š Images to process: {len(manifest_df)}\")\n",
    "    print(f\"   ğŸ“ Output dir: {CONFIG['spatial_features_dir']}\")\n",
    "    \n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for idx, row in tqdm(manifest_df.iterrows(), total=len(manifest_df), desc=\"Extracting spatial features\"):\n",
    "        try:\n",
    "            # Extract spatial features\n",
    "            spatial_results = extract_spatial_features_from_image(\n",
    "                row['image_path'], spatial_pipeline, save_attention_map=True\n",
    "            )\n",
    "            \n",
    "            # Save features\n",
    "            feature_file = save_spatial_features(\n",
    "                row['image_id'], spatial_results, CONFIG['spatial_features_dir']\n",
    "            )\n",
    "            \n",
    "            # Record result\n",
    "            results.append({\n",
    "                'image_id': row['image_id'],\n",
    "                'image_path': row['image_path'],\n",
    "                'spatial_feature_file': feature_file,\n",
    "                'num_patches': spatial_results['num_patches'],\n",
    "                'class_name': row['class_name'],\n",
    "                'success': True,\n",
    "                'error': spatial_results.get('error', None)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Failed to process {row['image_id']}: {e}\")\n",
    "            results.append({\n",
    "                'image_id': row['image_id'],\n",
    "                'image_path': row['image_path'],\n",
    "                'spatial_feature_file': None,\n",
    "                'num_patches': 0,\n",
    "                'class_name': row['class_name'],\n",
    "                'success': False,\n",
    "                'error': str(e)\n",
    "            })\n",
    "        \n",
    "        # Clear GPU memory periodically\n",
    "        if idx % 50 == 0 and torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    success_count = results_df['success'].sum()\n",
    "    print(f\"\\nâœ… Spatial feature extraction complete:\")\n",
    "    print(f\"   ğŸ“Š Processed: {success_count}/{len(results_df)} images\")\n",
    "    print(f\"   â±ï¸ Total time: {total_time/60:.1f} minutes\")\n",
    "    print(f\"   âš¡ Average time: {total_time/len(results_df):.2f}s per image\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "print(\"ğŸ’¾ Spatial feature caching system ready\")\n",
    "print(f\"   ğŸ“ Output: {CONFIG['spatial_features_dir']}\")\n",
    "print(f\"   ğŸ”„ Batch processing with progress tracking\")\n",
    "print(f\"   ğŸ’¾ NPZ compression for efficient storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447bdeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸš€ EXECUTE: Small Batch Spatial Extraction\n",
    "# Test batch processing on subset of images\n",
    "\n",
    "# Try to load existing manifest\n",
    "try:\n",
    "    manifest_file = '../features/manifest_features.v001.csv'\n",
    "    if Path(manifest_file).exists():\n",
    "        manifest = pd.read_csv(manifest_file)\n",
    "        print(f\"ğŸ“‹ Loaded feature manifest: {len(manifest)} images\")\n",
    "        \n",
    "        # Test on small subset first\n",
    "        TEST_BATCH_SIZE = 10\n",
    "        \n",
    "        print(f\"\\nğŸ§ª Testing spatial extraction on {TEST_BATCH_SIZE} images\")\n",
    "        \n",
    "        test_manifest = manifest.head(TEST_BATCH_SIZE)\n",
    "        \n",
    "        # Run batch extraction\n",
    "        spatial_results = batch_extract_spatial_features(test_manifest)\n",
    "        \n",
    "        if not spatial_results.empty:\n",
    "            # Save results\n",
    "            results_file = Path(CONFIG['spatial_features_dir']) / 'spatial_manifest_test.csv'\n",
    "            spatial_results.to_csv(results_file, index=False)\n",
    "            \n",
    "            print(f\"\\nğŸ“Š Test Results Summary:\")\n",
    "            print(f\"   âœ… Successful: {spatial_results['success'].sum()}\")\n",
    "            print(f\"   âŒ Failed: {(~spatial_results['success']).sum()}\")\n",
    "            print(f\"   ğŸ“Š Avg patches: {spatial_results[spatial_results['success']]['num_patches'].mean():.1f}\")\n",
    "            print(f\"   ğŸ’¾ Results saved: {results_file}\")\n",
    "            \n",
    "            print(f\"\\nğŸ¯ PHASE D READY: Spatial feature extraction pipeline operational!\")\n",
    "            print(f\"   To process full dataset, increase TEST_BATCH_SIZE\")\n",
    "            print(f\"   Expected time for full dataset: ~{len(manifest) * 3 / 60:.0f} minutes\")\n",
    "            \n",
    "        else:\n",
    "            print(\"âŒ No spatial features extracted - check pipeline\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"âš ï¸ Feature manifest not found: {manifest_file}\")\n",
    "        print(\"   Run feature extraction first (02_feature_extract_microjobs.ipynb)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error in batch extraction: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\nğŸ“‹ Phase D Status:\")\n",
    "print(\"   ğŸ” Patch extraction: âœ… Functional\")\n",
    "print(\"   ğŸ¯ Attention learning: âœ… Functional\")\n",
    "print(\"   ğŸ’¾ Feature caching: âœ… Functional\")\n",
    "print(\"   ğŸ“Š Visualization: âœ… Functional\")\n",
    "print(\"   ğŸš€ Ready for integration with head training\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
