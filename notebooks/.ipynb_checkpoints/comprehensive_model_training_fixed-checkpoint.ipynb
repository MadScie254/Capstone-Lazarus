{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d64042",
   "metadata": {},
   "source": [
    "# üåæ CAPSTONE-LAZARUS: Professional Model Training Pipeline\n",
    "\n",
    "## üéØ **Comprehensive Plant Disease Detection Training**\n",
    "\n",
    "### **Objective**: Train high-performance models on all 52,266+ plant disease images across 19 classes\n",
    "\n",
    "This notebook provides a **professional, production-ready training pipeline** with:\n",
    "- üî• **Multi-architecture training** (EfficientNet, ResNet, Vision Transformers)\n",
    "- üìä **Advanced data augmentation** for robust generalization\n",
    "- ‚ö° **Mixed precision training** for optimal GPU utilization\n",
    "- üìà **Real-time monitoring** with comprehensive visualizations\n",
    "- üéØ **Class balancing** for handling imbalanced datasets\n",
    "- üíæ **Model checkpointing** with automatic best model saving\n",
    "- üîç **Explainable AI** with GradCAM visualizations\n",
    "\n",
    "### **Training Strategy**:\n",
    "1. **Data Loading & Preprocessing** - Load all 52K+ images with professional augmentation\n",
    "2. **Multi-Model Training** - Train multiple architectures simultaneously  \n",
    "3. **Advanced Evaluation** - Comprehensive metrics and visualizations\n",
    "4. **Model Selection** - Choose best performing model for deployment\n",
    "\n",
    "---\n",
    "**üöÄ Ready to train on ALL your images with professional-grade pipeline!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37047af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß **PROFESSIONAL SETUP & IMPORTS**\n",
    "# ===========================================\n",
    "\n",
    "# Suppress warnings for clean output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# TensorFlow and deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model, optimizers, callbacks\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# Model evaluation and metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Project modules - Fixed with correct class names\n",
    "from data_utils import PlantDiseaseDataLoader\n",
    "from model_factory import ModelFactory\n",
    "from inference import PlantDiseaseInference\n",
    "\n",
    "# Utilities\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üî• CAPSTONE-LAZARUS: Professional Training Pipeline\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üñ•Ô∏è  TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"üéÆ GPU Devices Available: {len(tf.config.list_physical_devices('GPU'))}\")\n",
    "print(f\"üïê Training Session Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0333da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è **PROFESSIONAL TRAINING CONFIGURATION**\n",
    "# ============================================\n",
    "\n",
    "# üéØ TRAINING HYPERPARAMETERS\n",
    "TRAINING_CONFIG = {\n",
    "    # Model Training\n",
    "    'epochs': 100,                    # Maximum epochs (early stopping will optimize)\n",
    "    'batch_size': 32,                # Optimal batch size for most GPUs\n",
    "    'initial_lr': 1e-3,              # Initial learning rate\n",
    "    'min_lr': 1e-7,                  # Minimum learning rate\n",
    "    \n",
    "    # Image Configuration  \n",
    "    'image_size': (224, 224),        # Standard input size\n",
    "    'channels': 3,                   # RGB images\n",
    "    \n",
    "    # Data Splits\n",
    "    'validation_split': 0.15,        # 15% for validation\n",
    "    'test_split': 0.10,             # 10% for final testing\n",
    "    \n",
    "    # Advanced Training\n",
    "    'use_mixed_precision': True,     # Faster training on modern GPUs\n",
    "    'class_balancing': True,         # Handle imbalanced classes\n",
    "    'heavy_augmentation': True,      # Robust data augmentation\n",
    "    \n",
    "    # Callbacks & Optimization\n",
    "    'early_stopping_patience': 20,   # Stop if no improvement\n",
    "    'reduce_lr_patience': 8,         # Reduce LR if plateau\n",
    "    'checkpoint_save_best': True,    # Save only best models\n",
    "    \n",
    "    # Loss Function\n",
    "    'focal_loss': True,              # Better for imbalanced data\n",
    "    'focal_alpha': 0.25,\n",
    "    'focal_gamma': 2.0,\n",
    "    \n",
    "    # Regularization\n",
    "    'dropout_rate': 0.3,\n",
    "    'l2_reg': 1e-4\n",
    "}\n",
    "\n",
    "# üî• ENABLE MIXED PRECISION FOR SPEED\n",
    "if TRAINING_CONFIG['use_mixed_precision']:\n",
    "    policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "    tf.keras.mixed_precision.set_global_policy(policy)\n",
    "    print(\"‚ö° Mixed Precision Training: ENABLED\")\n",
    "\n",
    "# üìä DISPLAY CONFIGURATION\n",
    "print(\"\\nüéØ PROFESSIONAL TRAINING CONFIGURATION:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"   {key:<25}: {value}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# üé® MODELS TO TRAIN (Multiple architectures)\n",
    "MODELS_TO_TRAIN = {\n",
    "    'EfficientNetB0': {'variant': 'B0', 'priority': 1},\n",
    "    'EfficientNetB1': {'variant': 'B1', 'priority': 2}, \n",
    "    'EfficientNetB2': {'variant': 'B2', 'priority': 3},\n",
    "    'ResNet50': {'architecture': 'ResNet50', 'priority': 4},\n",
    "    'MobileNetV3': {'architecture': 'MobileNetV3Large', 'priority': 5}\n",
    "}\n",
    "\n",
    "print(f\"\\nü§ñ MODELS SELECTED FOR TRAINING: {len(MODELS_TO_TRAIN)} architectures\")\n",
    "for model_name, config in MODELS_TO_TRAIN.items():\n",
    "    print(f\"   ‚úÖ {model_name} (Priority: {config['priority']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b2a188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìä **DATA LOADING & PREPARATION**\n",
    "# ===================================\n",
    "\n",
    "print(\"üåæ LOADING ALL PLANT DISEASE DATA...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize data loader - Fixed class name\n",
    "data_loader = PlantDiseaseDataLoader(data_dir='../data')\n",
    "\n",
    "# Load dataset information\n",
    "print(\"üîç Scanning dataset...\")\n",
    "dataset_stats = data_loader.get_dataset_stats()\n",
    "\n",
    "# Display comprehensive dataset information\n",
    "print(f\"\\nüìà DATASET OVERVIEW:\")\n",
    "print(f\"   üìÅ Total Images: {dataset_stats['total_images']:,}\")\n",
    "print(f\"   üè∑Ô∏è  Total Classes: {dataset_stats['num_classes']}\")\n",
    "print(f\"   ‚öñÔ∏è  Balance Ratio: {dataset_stats['imbalance_ratio']:.2f}\")\n",
    "\n",
    "# Get class information - Fixed to use data_loader method\n",
    "class_names = data_loader.get_class_names()\n",
    "print(f\"\\nüå± PLANT DISEASE CLASSES ({len(class_names)}):\")\n",
    "print(\"=\" * 30)\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"   {i+1:2d}. {class_name}\")\n",
    "\n",
    "# Class distribution analysis\n",
    "print(\"\\nüìä ANALYZING CLASS DISTRIBUTION...\")\n",
    "class_distribution = data_loader.analyze_class_distribution()\n",
    "\n",
    "# Visualization of class distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Bar plot\n",
    "ax1.bar(range(len(class_distribution)), class_distribution.values)\n",
    "ax1.set_title('Class Distribution (All Images)', fontsize=16, fontweight='bold')\n",
    "ax1.set_xlabel('Disease Classes', fontsize=12)\n",
    "ax1.set_ylabel('Number of Images', fontsize=12)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Log scale for better visualization\n",
    "ax2.bar(range(len(class_distribution)), class_distribution.values)\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_title('Class Distribution (Log Scale)', fontsize=16, fontweight='bold')\n",
    "ax2.set_xlabel('Disease Classes', fontsize=12)\n",
    "ax2.set_ylabel('Number of Images (Log Scale)', fontsize=12)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ DATA LOADING COMPLETE!\")\n",
    "print(f\"üéØ Ready to train on {dataset_stats['total_images']:,} images!\")\n",
    "\n",
    "# Calculate class weights for balanced training\n",
    "if TRAINING_CONFIG['class_balancing']:\n",
    "    print(\"\\n‚öñÔ∏è CALCULATING CLASS WEIGHTS FOR BALANCED TRAINING...\")\n",
    "    \n",
    "    # Convert to arrays for sklearn\n",
    "    classes = list(range(len(class_distribution)))\n",
    "    class_counts = list(class_distribution.values())\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight(\n",
    "        'balanced',\n",
    "        classes=classes,\n",
    "        y=[cls for cls, count in enumerate(class_counts) for _ in range(count)]\n",
    "    )\n",
    "    \n",
    "    class_weight_dict = dict(zip(classes, class_weights))\n",
    "    \n",
    "    print(\"üìä Class Weights:\")\n",
    "    for cls, weight in class_weight_dict.items():\n",
    "        print(f\"   Class {cls} ({class_names[cls]}): {weight:.3f}\")\n",
    "    \n",
    "    print(\"‚úÖ Class weights calculated for balanced training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1ba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ **START COMPREHENSIVE TRAINING ON ALL IMAGES**\n",
    "# ==================================================\n",
    "\n",
    "print(\"üåæ STARTING COMPREHENSIVE TRAINING PIPELINE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üìä Ready to train on ALL {dataset_stats['total_images']:,} images\")\n",
    "print(f\"üéØ Target: {len(class_names)} plant disease classes\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create data generators with heavy augmentation\n",
    "if TRAINING_CONFIG['heavy_augmentation']:\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.3,\n",
    "        height_shift_range=0.3,\n",
    "        shear_range=0.3,\n",
    "        zoom_range=0.3,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        brightness_range=[0.7, 1.3],\n",
    "        channel_shift_range=20,\n",
    "        fill_mode='nearest',\n",
    "        validation_split=TRAINING_CONFIG['validation_split']\n",
    "    )\n",
    "    print(\"‚úÖ HEAVY AUGMENTATION: Applied for robust training\")\n",
    "else:\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=TRAINING_CONFIG['validation_split']\n",
    "    )\n",
    "    print(\"‚úÖ LIGHT AUGMENTATION: Applied for faster training\")\n",
    "\n",
    "# Validation generator (no augmentation)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Create data flows\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    '../data',\n",
    "    target_size=TRAINING_CONFIG['image_size'],\n",
    "    batch_size=TRAINING_CONFIG['batch_size'],\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    '../data', \n",
    "    target_size=TRAINING_CONFIG['image_size'],\n",
    "    batch_size=TRAINING_CONFIG['batch_size'],\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DATA FLOWS CREATED:\")\n",
    "print(f\"   üî• Training samples: {train_generator.samples:,}\")\n",
    "print(f\"   ‚úÖ Validation samples: {validation_generator.samples:,}\")\n",
    "\n",
    "print(\"\\nüöÄ DATA PIPELINE READY!\")\n",
    "print(\"üìä All images loaded and augmentation pipeline configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137c5148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèóÔ∏è **MODEL TRAINING FUNCTION**\n",
    "# ===============================\n",
    "\n",
    "def train_single_model(model_name, architecture_config):\n",
    "    \"\"\"Professional training function for a single model\"\"\"\n",
    "    \n",
    "    print(f\"\\nüöÄ TRAINING: {model_name}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Initialize model factory\n",
    "    model_factory = ModelFactory(\n",
    "        input_shape=(*TRAINING_CONFIG['image_size'], TRAINING_CONFIG['channels']),\n",
    "        num_classes=len(train_generator.class_indices),\n",
    "        use_mixed_precision=TRAINING_CONFIG['use_mixed_precision']\n",
    "    )\n",
    "    \n",
    "    # Create model based on architecture\n",
    "    if 'variant' in architecture_config:\n",
    "        # EfficientNet models\n",
    "        model = model_factory.create_efficientnet_v2(\n",
    "            variant=architecture_config['variant'],\n",
    "            dropout_rate=TRAINING_CONFIG['dropout_rate']\n",
    "        )\n",
    "    else:\n",
    "        # Other architectures\n",
    "        arch_name = architecture_config['architecture']\n",
    "        if arch_name == 'ResNet50':\n",
    "            model = model_factory.create_resnet(variant='50')\n",
    "        elif arch_name == 'MobileNetV3Large':\n",
    "            model = model_factory.create_mobilenet_v3(variant='Large')\n",
    "        else:\n",
    "            raise ValueError(f\"Architecture {arch_name} not implemented\")\n",
    "    \n",
    "    print(f\"‚úÖ Model created: {model_name}\")\n",
    "    print(f\"   üìä Total parameters: {model.count_params():,}\")\n",
    "    \n",
    "    # Compile model\n",
    "    optimizer = optimizers.Adam(learning_rate=TRAINING_CONFIG['initial_lr'])\n",
    "    if TRAINING_CONFIG['use_mixed_precision']:\n",
    "        optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy', 'top_3_accuracy']\n",
    "    )\n",
    "    \n",
    "    # Create callbacks\n",
    "    models_dir = Path('../models')\n",
    "    models_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    callbacks_list = [\n",
    "        callbacks.ModelCheckpoint(\n",
    "            filepath=str(models_dir / f'{model_name}_best.h5'),\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        ),\n",
    "        callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=TRAINING_CONFIG['early_stopping_patience'],\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=TRAINING_CONFIG['reduce_lr_patience'],\n",
    "            min_lr=TRAINING_CONFIG['min_lr'],\n",
    "            verbose=1\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    # Calculate steps\n",
    "    steps_per_epoch = train_generator.samples // TRAINING_CONFIG['batch_size']\n",
    "    validation_steps = validation_generator.samples // TRAINING_CONFIG['batch_size']\n",
    "    \n",
    "    print(f\"üìä Steps per epoch: {steps_per_epoch}\")\n",
    "    print(f\"‚úÖ Validation steps: {validation_steps}\")\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\nüî• STARTING TRAINING...\")\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=TRAINING_CONFIG['epochs'],\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_steps,\n",
    "        callbacks=callbacks_list,\n",
    "        class_weight=class_weight_dict if TRAINING_CONFIG['class_balancing'] else None,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nüéâ TRAINING COMPLETED: {model_name}\")\n",
    "    print(f\"‚è±Ô∏è  Training time: {training_time/60:.2f} minutes\")\n",
    "    print(f\"üèÜ Best val_accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "    \n",
    "    return model, history, training_time\n",
    "\n",
    "print(\"‚úÖ Training function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c015498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ **EXECUTE TRAINING ON ALL MODELS**\n",
    "# =====================================\n",
    "\n",
    "# Storage for results\n",
    "training_results = {}\n",
    "model_performances = []\n",
    "\n",
    "print(\"üåæ EXECUTING COMPREHENSIVE TRAINING ON ALL IMAGES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Train all models\n",
    "for model_name, config in MODELS_TO_TRAIN.items():\n",
    "    try:\n",
    "        print(f\"\\nüî• TRAINING MODEL {config['priority']}/{len(MODELS_TO_TRAIN)}: {model_name}\")\n",
    "        \n",
    "        # Train the model\n",
    "        model, history, training_time = train_single_model(model_name, config)\n",
    "        \n",
    "        # Store results\n",
    "        training_results[model_name] = {\n",
    "            'model': model,\n",
    "            'history': history,\n",
    "            'training_time': training_time,\n",
    "            'config': config\n",
    "        }\n",
    "        \n",
    "        # Quick evaluation\n",
    "        val_loss, val_accuracy, val_top3 = model.evaluate(\n",
    "            validation_generator,\n",
    "            steps=validation_generator.samples // TRAINING_CONFIG['batch_size'],\n",
    "            verbose=0\n",
    "        )\n",
    "        \n",
    "        # Store performance\n",
    "        performance = {\n",
    "            'model_name': model_name,\n",
    "            'val_accuracy': val_accuracy,\n",
    "            'val_top3_accuracy': val_top3,\n",
    "            'val_loss': val_loss,\n",
    "            'training_time': training_time,\n",
    "            'parameters': model.count_params(),\n",
    "            'priority': config['priority']\n",
    "        }\n",
    "        model_performances.append(performance)\n",
    "        \n",
    "        print(f\"‚úÖ {model_name} Results:\")\n",
    "        print(f\"   üéØ Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        print(f\"   üîù Top-3 Accuracy: {val_top3:.4f}\")\n",
    "        print(f\"   üìâ Validation Loss: {val_loss:.4f}\")\n",
    "        print(f\"   ‚è±Ô∏è  Training Time: {training_time/60:.2f} min\")\n",
    "        \n",
    "        # Clear memory\n",
    "        del model\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR training {model_name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "print(\"\\nüéâ ALL MODEL TRAINING COMPLETED!\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Display results\n",
    "if model_performances:\n",
    "    performance_df = pd.DataFrame(model_performances)\n",
    "    performance_df = performance_df.sort_values('val_accuracy', ascending=False)\n",
    "    \n",
    "    print(\"\\nüèÜ MODEL PERFORMANCE RANKING:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Rank':<4} {'Model':<15} {'Val Acc':<8} {'Top-3 Acc':<10} {'Loss':<8} {'Time (min)':<10}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for idx, row in performance_df.iterrows():\n",
    "        rank = performance_df.index.get_loc(idx) + 1\n",
    "        print(f\"{rank:<4} {row['model_name']:<15} {row['val_accuracy']:<8.4f} \"\n",
    "              f\"{row['val_top3_accuracy']:<10.4f} {row['val_loss']:<8.4f} \"\n",
    "              f\"{row['training_time']/60:<10.2f}\")\n",
    "    \n",
    "    best_model = performance_df.iloc[0]\n",
    "    print(f\"\\nü•á BEST MODEL: {best_model['model_name']}\")\n",
    "    print(f\"   üéØ Accuracy: {best_model['val_accuracy']:.4f}\")\n",
    "    print(f\"   üîù Top-3 Accuracy: {best_model['val_top3_accuracy']:.4f}\")\n",
    "    \n",
    "    print(f\"\\nüéØ TRAINING SUMMARY:\")\n",
    "    print(f\"   ‚úÖ Models trained: {len(model_performances)}\")\n",
    "    print(f\"   üìä Images processed: {train_generator.samples:,}\")\n",
    "    print(f\"   ‚è±Ô∏è  Total time: {sum([p['training_time'] for p in model_performances])/60:.2f} min\")\n",
    "\n",
    "print(\"\\nüöÄ COMPREHENSIVE TRAINING ON ALL IMAGES COMPLETED!\")\n",
    "print(\"üéâ YOUR PLANT DISEASE DETECTION SYSTEM IS READY!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
