{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8237f7fd",
   "metadata": {},
   "source": [
    "# üå± Capstone-Lazarus: Plant Disease Detection with PyTorch & Quantum Computing\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MadScie254/Capstone-Lazarus/blob/main/notebooks/colab_training.ipynb)\n",
    "\n",
    "**Fast, reproducible training on resource-limited hardware with Colab scaling capability**\n",
    "\n",
    "## üéØ Key Features\n",
    "\n",
    "- **Transfer Learning**: EfficientNet, ResNet, MobileNet via `timm` library\n",
    "- **Mixed Precision**: AMP for memory efficiency and speed\n",
    "- **Quantum Computing**: Optional PennyLane integration (experimental)\n",
    "- **Production Ready**: Checkpointing, early stopping, ONNX export\n",
    "- **Resource Optimized**: HP ZBook G5 compatible (16GB RAM, Quadro P2000)\n",
    "\n",
    "## üìã Training Phases\n",
    "\n",
    "1. **Quick Test** (1 epoch, 1k samples): Validate pipeline\n",
    "2. **Development** (10 epochs, full dataset): Model selection\n",
    "3. **Production** (50+ epochs): Final training with quantum experiments\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Getting Started\n",
    "\n",
    "Run all cells in order. Toggle settings in Section 1 for your hardware configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a08e2f1",
   "metadata": {},
   "source": [
    "# 1. Environment Setup and Configuration\n",
    "\n",
    "First, we'll install all required dependencies and set up the environment for both Colab and local execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2811001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Colab\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(f\"Running in Colab: {IN_COLAB}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# For Colab: Mount Google Drive\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Clone repository if not exists\n",
    "    if not os.path.exists('/content/Capstone-Lazarus'):\n",
    "        !git clone https://github.com/MadScie254/Capstone-Lazarus.git /content/Capstone-Lazarus\n",
    "    \n",
    "    # Change to project directory\n",
    "    os.chdir('/content/Capstone-Lazarus')\n",
    "    PROJECT_ROOT = Path('/content/Capstone-Lazarus')\n",
    "else:\n",
    "    # Local execution\n",
    "    PROJECT_ROOT = Path().resolve()\n",
    "    if not (PROJECT_ROOT / 'src').exists():\n",
    "        PROJECT_ROOT = PROJECT_ROOT.parent if PROJECT_ROOT.name == 'notebooks' else PROJECT_ROOT\n",
    "    \n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "\n",
    "# Add src to path\n",
    "if str(PROJECT_ROOT / 'src') not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT / 'src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee96b222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import subprocess\n",
    "import importlib.util\n",
    "\n",
    "def install_if_missing(packages):\n",
    "    \"\"\"Install packages if they're not available.\"\"\"\n",
    "    for package in packages:\n",
    "        if importlib.util.find_spec(package.split('==')[0]) is None:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "            print(f\"‚úÖ Installed {package}\")\n",
    "        else:\n",
    "            print(f\"‚úÖ {package.split('==')[0]} already available\")\n",
    "\n",
    "# Core ML packages\n",
    "core_packages = [\n",
    "    'torch>=2.0.0',\n",
    "    'torchvision>=0.15.0',\n",
    "    'torchaudio>=2.0.0',\n",
    "    'timm>=0.9.0',\n",
    "    'torchmetrics>=0.11.0'\n",
    "]\n",
    "\n",
    "# Data processing and augmentation\n",
    "data_packages = [\n",
    "    'albumentations>=1.3.0',\n",
    "    'opencv-python>=4.8.0',\n",
    "    'Pillow>=9.5.0'\n",
    "]\n",
    "\n",
    "# Visualization and utilities\n",
    "viz_packages = [\n",
    "    'matplotlib>=3.7.0',\n",
    "    'seaborn>=0.12.0',\n",
    "    'tqdm>=4.65.0',\n",
    "    'pyyaml>=6.0',\n",
    "    'scikit-learn>=1.3.0',\n",
    "    'pandas>=2.0.0',\n",
    "    'numpy>=1.24.0'\n",
    "]\n",
    "\n",
    "# Optional packages\n",
    "optional_packages = [\n",
    "    'wandb',  # For experiment tracking\n",
    "    'pennylane>=0.32.0',  # For quantum computing\n",
    "    'onnx>=1.14.0',  # For model export\n",
    "    'onnxruntime>=1.15.0'  # For ONNX inference\n",
    "]\n",
    "\n",
    "print(\"üì¶ Installing core ML packages...\")\n",
    "install_if_missing(core_packages)\n",
    "\n",
    "print(\"\\nüì¶ Installing data processing packages...\")\n",
    "install_if_missing(data_packages)\n",
    "\n",
    "print(\"\\nüì¶ Installing visualization packages...\")\n",
    "install_if_missing(viz_packages)\n",
    "\n",
    "print(\"\\nüì¶ Installing optional packages (may fail, that's OK)...\")\n",
    "for package in optional_packages:\n",
    "    try:\n",
    "        install_if_missing([package])\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Optional package {package} failed to install: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Package installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f3d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "import yaml\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# Default configuration - optimized for HP ZBook G5 and Colab\n",
    "default_config = {\n",
    "    'seed': 42,\n",
    "    'backbone': 'tf_efficientnet_b0',  # Fast and accurate\n",
    "    'num_classes': 19,  # Adjust based on your dataset\n",
    "    'image_size': 224,\n",
    "    \n",
    "    # Hardware optimizations\n",
    "    'batch_size': 16 if not IN_COLAB else 32,  # ZBook P2000 has 4GB VRAM\n",
    "    'num_workers': 2 if not IN_COLAB else 4,\n",
    "    'pin_memory': True,\n",
    "    \n",
    "    # Training settings\n",
    "    'epochs': 5 if IN_COLAB else 30,  # Start small for testing\n",
    "    'learning_rate': 1e-3,\n",
    "    'weight_decay': 1e-4,\n",
    "    'optimizer': 'adamw',\n",
    "    'scheduler': 'onecycle',\n",
    "    \n",
    "    # Performance features\n",
    "    'use_amp': True,  # Automatic Mixed Precision\n",
    "    'gradient_accumulation_steps': 2 if not IN_COLAB else 1,\n",
    "    'use_ema': True,  # Exponential Moving Average\n",
    "    'ema_decay': 0.9999,\n",
    "    \n",
    "    # Data augmentation\n",
    "    'use_augmentations': True,\n",
    "    'augmentation_strength': 'medium',\n",
    "    'use_class_balancing': True,\n",
    "    \n",
    "    # Quantum computing (experimental)\n",
    "    'use_quantum': False,  # Start with classical training\n",
    "    'quantum': {\n",
    "        'n_qubits': 4,\n",
    "        'n_layers': 3,\n",
    "        'embedding_dim': 4\n",
    "    },\n",
    "    \n",
    "    # Checkpointing and monitoring\n",
    "    'save_every': 10,\n",
    "    'early_stopping_patience': 15,\n",
    "    'use_wandb': False,  # Enable for experiment tracking\n",
    "    \n",
    "    # Paths\n",
    "    'data_dir': str(PROJECT_ROOT / 'data'),\n",
    "    'save_dir': str(PROJECT_ROOT / 'checkpoints'),\n",
    "    \n",
    "    # Testing\n",
    "    'quick_test': True,  # Start with subset for validation\n",
    "    'test_subset_size': 1000\n",
    "}\n",
    "\n",
    "# Try to load custom config if it exists\n",
    "config_path = PROJECT_ROOT / 'config.yaml'\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'r') as f:\n",
    "        custom_config = yaml.safe_load(f)\n",
    "    default_config.update(custom_config)\n",
    "    print(f\"‚úÖ Loaded custom config from {config_path}\")\n",
    "else:\n",
    "    print(f\"‚ÑπÔ∏è  Using default config. Create {config_path} to customize.\")\n",
    "\n",
    "config = default_config\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "config['device'] = str(device)\n",
    "\n",
    "print(f\"\\nüîß Configuration:\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"Backbone: {config['backbone']}\")\n",
    "print(f\"Batch size: {config['batch_size']}\")\n",
    "print(f\"Image size: {config['image_size']}\")\n",
    "print(f\"Mixed Precision: {config['use_amp']}\")\n",
    "print(f\"Quantum layers: {config['use_quantum']}\")\n",
    "print(f\"Quick test mode: {config['quick_test']}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(config['seed'])\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd891ff",
   "metadata": {},
   "source": [
    "# 2. Data Loading and Preprocessing with Albumentations\n",
    "\n",
    "Implement efficient data loading with stratified sampling and advanced augmentations optimized for plant disease classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084d2d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data utilities\n",
    "try:\n",
    "    from data_utils_torch import make_dataloaders, create_subset_loader, analyze_dataset_distribution\n",
    "    print(\"‚úÖ Imported local data_utils_torch\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Local data_utils_torch not found. Using inline implementation.\")\n",
    "    \n",
    "    # Inline implementation for standalone notebook\n",
    "    import torch\n",
    "    from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "    from torchvision import transforms\n",
    "    from torchvision.datasets import ImageFolder\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    import random\n",
    "    \n",
    "    try:\n",
    "        import albumentations as A\n",
    "        from albumentations.pytorch import ToTensorV2\n",
    "        ALBUMENTATIONS_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        ALBUMENTATIONS_AVAILABLE = False\n",
    "        print(\"‚ö†Ô∏è  Albumentations not available, using torchvision transforms\")\n",
    "    \n",
    "    class PlantDiseaseDataset(Dataset):\n",
    "        \"\"\"Plant Disease Dataset with Albumentations support.\"\"\"\n",
    "        \n",
    "        def __init__(self, root_dir, transform=None, use_albumentations=True):\n",
    "            self.root_dir = Path(root_dir)\n",
    "            self.use_albumentations = use_albumentations and ALBUMENTATIONS_AVAILABLE\n",
    "            self.transform = transform\n",
    "            \n",
    "            # Load dataset using ImageFolder for class mapping\n",
    "            self.dataset = ImageFolder(str(root_dir))\n",
    "            self.classes = self.dataset.classes\n",
    "            self.class_to_idx = self.dataset.class_to_idx\n",
    "            self.samples = self.dataset.samples\n",
    "            \n",
    "        def __len__(self):\n",
    "            return len(self.samples)\n",
    "        \n",
    "        def __getitem__(self, idx):\n",
    "            img_path, label = self.samples[idx]\n",
    "            \n",
    "            # Load image\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            # Apply transforms\n",
    "            if self.transform:\n",
    "                if self.use_albumentations:\n",
    "                    # Convert PIL to numpy for Albumentations\n",
    "                    image = np.array(image)\n",
    "                    transformed = self.transform(image=image)\n",
    "                    image = transformed['image']\n",
    "                else:\n",
    "                    # Standard torchvision transforms\n",
    "                    image = self.transform(image)\n",
    "            else:\n",
    "                # Default: convert to tensor\n",
    "                image = transforms.ToTensor()(image)\n",
    "                \n",
    "            return image, label\n",
    "    \n",
    "    def get_albumentations_transforms(image_size=224, split=\"train\", strength=\"medium\"):\n",
    "        \"\"\"Get Albumentations transforms.\"\"\"\n",
    "        \n",
    "        if not ALBUMENTATIONS_AVAILABLE:\n",
    "            return get_torchvision_transforms(image_size, split)\n",
    "        \n",
    "        # Base transforms\n",
    "        base_transforms = [A.Resize(image_size, image_size, always_apply=True)]\n",
    "        \n",
    "        if split == \"train\":\n",
    "            if strength == \"medium\":\n",
    "                aug_transforms = [\n",
    "                    A.HorizontalFlip(p=0.5),\n",
    "                    A.VerticalFlip(p=0.2),\n",
    "                    A.Rotate(limit=25, p=0.5),\n",
    "                    A.RandomBrightnessContrast(\n",
    "                        brightness_limit=0.2, contrast_limit=0.2, p=0.5\n",
    "                    ),\n",
    "                    A.HueSaturationValue(\n",
    "                        hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.3\n",
    "                    ),\n",
    "                ]\n",
    "            elif strength == \"light\":\n",
    "                aug_transforms = [\n",
    "                    A.HorizontalFlip(p=0.5),\n",
    "                    A.Rotate(limit=15, p=0.3),\n",
    "                    A.RandomBrightnessContrast(\n",
    "                        brightness_limit=0.1, contrast_limit=0.1, p=0.3\n",
    "                    ),\n",
    "                ]\n",
    "            else:  # heavy\n",
    "                aug_transforms = [\n",
    "                    A.HorizontalFlip(p=0.5),\n",
    "                    A.VerticalFlip(p=0.3),\n",
    "                    A.Rotate(limit=35, p=0.6),\n",
    "                    A.RandomBrightnessContrast(\n",
    "                        brightness_limit=0.3, contrast_limit=0.3, p=0.6\n",
    "                    ),\n",
    "                    A.HueSaturationValue(\n",
    "                        hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5\n",
    "                    ),\n",
    "                    A.OneOf([\n",
    "                        A.ElasticTransform(p=0.3),\n",
    "                        A.GridDistortion(p=0.3),\n",
    "                        A.OpticalDistortion(p=0.3),\n",
    "                    ], p=0.3),\n",
    "                ]\n",
    "            \n",
    "            transforms_list = base_transforms + aug_transforms\n",
    "        else:\n",
    "            transforms_list = base_transforms\n",
    "        \n",
    "        # Add normalization and tensor conversion\n",
    "        transforms_list.extend([\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], always_apply=True),\n",
    "            ToTensorV2(always_apply=True)\n",
    "        ])\n",
    "        \n",
    "        return A.Compose(transforms_list)\n",
    "    \n",
    "    def get_torchvision_transforms(image_size=224, split=\"train\"):\n",
    "        \"\"\"Get torchvision transforms as fallback.\"\"\"\n",
    "        \n",
    "        if split == \"train\":\n",
    "            transform_list = [\n",
    "                transforms.Resize((image_size, image_size)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(25),\n",
    "                transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ]\n",
    "        else:\n",
    "            transform_list = [\n",
    "                transforms.Resize((image_size, image_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "            ]\n",
    "        \n",
    "        return transforms.Compose(transform_list)\n",
    "    \n",
    "    def create_weighted_sampler(dataset):\n",
    "        \"\"\"Create weighted sampler for class balancing.\"\"\"\n",
    "        \n",
    "        labels = [sample[1] for sample in dataset.samples]\n",
    "        class_counts = np.bincount(labels)\n",
    "        \n",
    "        # Calculate weights (inverse frequency)\n",
    "        num_samples = len(labels)\n",
    "        class_weights = num_samples / (len(class_counts) * class_counts)\n",
    "        \n",
    "        # Assign weight to each sample\n",
    "        sample_weights = [class_weights[label] for label in labels]\n",
    "        \n",
    "        return WeightedRandomSampler(\n",
    "            weights=sample_weights,\n",
    "            num_samples=len(sample_weights),\n",
    "            replacement=True\n",
    "        )\n",
    "    \n",
    "    def make_dataloaders(data_dir, config, train_split=0.8, val_split=0.2):\n",
    "        \"\"\"Create train and validation DataLoaders.\"\"\"\n",
    "        \n",
    "        data_path = Path(data_dir)\n",
    "        if not data_path.exists():\n",
    "            raise FileNotFoundError(f\"Data directory not found: {data_dir}\")\n",
    "        \n",
    "        # Get transforms\n",
    "        if ALBUMENTATIONS_AVAILABLE and config.get('use_augmentations', True):\n",
    "            train_transform = get_albumentations_transforms(\n",
    "                image_size=config['image_size'],\n",
    "                split='train',\n",
    "                strength=config.get('augmentation_strength', 'medium')\n",
    "            )\n",
    "            val_transform = get_albumentations_transforms(\n",
    "                image_size=config['image_size'],\n",
    "                split='val'\n",
    "            )\n",
    "            use_albu = True\n",
    "        else:\n",
    "            train_transform = get_torchvision_transforms(\n",
    "                image_size=config['image_size'],\n",
    "                split='train'\n",
    "            )\n",
    "            val_transform = get_torchvision_transforms(\n",
    "                image_size=config['image_size'],\n",
    "                split='val'\n",
    "            )\n",
    "            use_albu = False\n",
    "        \n",
    "        # Check if data is already split\n",
    "        train_dir = data_path / \"train\"\n",
    "        val_dir = data_path / \"val\"\n",
    "        \n",
    "        if train_dir.exists() and val_dir.exists():\n",
    "            print(\"Using existing train/val split\")\n",
    "            train_dataset = PlantDiseaseDataset(train_dir, transform=train_transform, use_albumentations=use_albu)\n",
    "            val_dataset = PlantDiseaseDataset(val_dir, transform=val_transform, use_albumentations=use_albu)\n",
    "        else:\n",
    "            print(\"Creating train/val split from single directory\")\n",
    "            full_dataset = PlantDiseaseDataset(data_path, transform=None, use_albumentations=use_albu)\n",
    "            \n",
    "            # Create indices for train/val split\n",
    "            dataset_size = len(full_dataset)\n",
    "            indices = list(range(dataset_size))\n",
    "            random.seed(config.get('seed', 42))\n",
    "            random.shuffle(indices)\n",
    "            \n",
    "            train_size = int(train_split * dataset_size)\n",
    "            train_indices = indices[:train_size]\n",
    "            val_indices = indices[train_size:]\n",
    "            \n",
    "            # Create subsets\n",
    "            train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "            val_dataset = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "            \n",
    "            # Apply transforms to subsets\n",
    "            train_dataset.dataset.transform = train_transform\n",
    "            val_dataset.dataset.transform = val_transform\n",
    "        \n",
    "        # Create weighted sampler for training\n",
    "        if config.get('use_class_balancing', True):\n",
    "            train_sampler = create_weighted_sampler(train_dataset)\n",
    "            shuffle = False\n",
    "        else:\n",
    "            train_sampler = None\n",
    "            shuffle = True\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            sampler=train_sampler,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=config.get('num_workers', 4),\n",
    "            pin_memory=config.get('pin_memory', True),\n",
    "            persistent_workers=True if config.get('num_workers', 4) > 0 else False\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=config.get('num_workers', 4),\n",
    "            pin_memory=config.get('pin_memory', True),\n",
    "            persistent_workers=True if config.get('num_workers', 4) > 0 else False\n",
    "        )\n",
    "        \n",
    "        print(f\"Train loader: {len(train_loader)} batches ({len(train_dataset)} samples)\")\n",
    "        print(f\"Val loader: {len(val_loader)} batches ({len(val_dataset)} samples)\")\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    def create_subset_loader(data_dir, config, subset_size=1000, split=\"train\"):\n",
    "        \"\"\"Create a DataLoader with a subset of data for quick testing.\"\"\"\n",
    "        \n",
    "        if ALBUMENTATIONS_AVAILABLE and config.get('use_augmentations', True):\n",
    "            transform = get_albumentations_transforms(\n",
    "                image_size=config['image_size'],\n",
    "                split=split,\n",
    "                strength='light'  # Light for quick testing\n",
    "            )\n",
    "            use_albu = True\n",
    "        else:\n",
    "            transform = get_torchvision_transforms(\n",
    "                image_size=config['image_size'],\n",
    "                split=split\n",
    "            )\n",
    "            use_albu = False\n",
    "        \n",
    "        # Create dataset\n",
    "        dataset = PlantDiseaseDataset(data_dir, transform=transform, use_albumentations=use_albu)\n",
    "        \n",
    "        # Create random subset\n",
    "        subset_size = min(subset_size, len(dataset))\n",
    "        indices = random.sample(range(len(dataset)), subset_size)\n",
    "        subset_dataset = torch.utils.data.Subset(dataset, indices)\n",
    "        \n",
    "        # Create loader\n",
    "        loader = DataLoader(\n",
    "            subset_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=(split == 'train'),\n",
    "            num_workers=config.get('num_workers', 2),\n",
    "            pin_memory=False\n",
    "        )\n",
    "        \n",
    "        print(f\"Subset loader created: {len(loader)} batches ({subset_size} samples)\")\n",
    "        return loader\n",
    "    \n",
    "    def analyze_dataset_distribution(data_dir):\n",
    "        \"\"\"Analyze class distribution in dataset.\"\"\"\n",
    "        \n",
    "        dataset = ImageFolder(data_dir)\n",
    "        \n",
    "        # Count samples per class\n",
    "        class_counts = {}\n",
    "        for _, class_idx in dataset.samples:\n",
    "            class_name = dataset.classes[class_idx]\n",
    "            class_counts[class_name] = class_counts.get(class_name, 0) + 1\n",
    "        \n",
    "        total_samples = len(dataset.samples)\n",
    "        \n",
    "        analysis = {\n",
    "            'total_samples': total_samples,\n",
    "            'num_classes': len(dataset.classes),\n",
    "            'class_names': dataset.classes,\n",
    "            'class_counts': class_counts,\n",
    "            'class_percentages': {\n",
    "                name: (count / total_samples) * 100 \n",
    "                for name, count in class_counts.items()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "print(\"‚úÖ Data utilities ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81352942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze dataset distribution\n",
    "data_path = Path(config['data_dir'])\n",
    "print(f\"Looking for data in: {data_path}\")\n",
    "\n",
    "if data_path.exists():\n",
    "    # Check what's in the data directory\n",
    "    subdirs = [d for d in data_path.iterdir() if d.is_dir()]\n",
    "    print(f\"Found {len(subdirs)} subdirectories:\")\n",
    "    for subdir in subdirs[:10]:  # Show first 10\n",
    "        print(f\"  üìÅ {subdir.name}\")\n",
    "    if len(subdirs) > 10:\n",
    "        print(f\"  ... and {len(subdirs) - 10} more\")\n",
    "    \n",
    "    # Try to analyze distribution\n",
    "    try:\n",
    "        analysis = analyze_dataset_distribution(data_path)\n",
    "        print(f\"\\nüìä Dataset Analysis:\")\n",
    "        print(f\"Total samples: {analysis['total_samples']:,}\")\n",
    "        print(f\"Number of classes: {analysis['num_classes']}\")\n",
    "        \n",
    "        # Show class distribution\n",
    "        print(f\"\\nClass distribution:\")\n",
    "        for class_name, count in sorted(analysis['class_counts'].items(), key=lambda x: x[1], reverse=True)[:5]:\n",
    "            percentage = analysis['class_percentages'][class_name]\n",
    "            print(f\"  {class_name}: {count:,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        if analysis['num_classes'] > 5:\n",
    "            print(f\"  ... and {analysis['num_classes'] - 5} more classes\")\n",
    "            \n",
    "        # Update config with actual number of classes\n",
    "        config['num_classes'] = analysis['num_classes']\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not analyze dataset: {e}\")\n",
    "        print(\"This might be because the data structure is different than expected.\")\n",
    "        print(\"Expected structure: data_dir/class1/, data_dir/class2/, etc.\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Data directory not found!\")\n",
    "    if IN_COLAB:\n",
    "        print(\"üìã For Colab users:\")\n",
    "        print(\"1. Upload your dataset to Google Drive\")\n",
    "        print(\"2. Update the data_dir path in config\")\n",
    "        print(\"3. Or use the sample dataset from the repository\")\n",
    "    else:\n",
    "        print(\"üìã For local users:\")\n",
    "        print(\"1. Make sure your data is in the correct directory\")\n",
    "        print(\"2. Update the data_dir path in config.yaml\")\n",
    "        print(\"3. Expected structure: data_dir/class1/, data_dir/class2/, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0917db",
   "metadata": {},
   "source": [
    "# 3. Model Factory Implementation with TIMM Integration\n",
    "\n",
    "Create models using the TIMM library with pretrained weights and optional quantum layer integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25a92b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model factory\n",
    "try:\n",
    "    from model_factory_torch import create_model\n",
    "    print(\"‚úÖ Imported local model_factory_torch\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  Local model_factory_torch not found. Using inline implementation.\")\n",
    "    \n",
    "    # Inline model factory implementation\n",
    "    import timm\n",
    "    import torch.nn as nn\n",
    "    \n",
    "    # Available backbone models\n",
    "    BACKBONE_MODELS = {\n",
    "        'tf_efficientnet_b0': {\n",
    "            'timm_name': 'tf_efficientnet_b0.aa_in1k',\n",
    "            'description': 'EfficientNet-B0 (5.3M params) - Fast and accurate',\n",
    "            'input_size': 224,\n",
    "            'params': '5.3M'\n",
    "        },\n",
    "        'tf_efficientnet_b1': {\n",
    "            'timm_name': 'tf_efficientnet_b1.aa_in1k',\n",
    "            'description': 'EfficientNet-B1 (7.8M params) - Good balance',\n",
    "            'input_size': 240,\n",
    "            'params': '7.8M'\n",
    "        },\n",
    "        'resnet18': {\n",
    "            'timm_name': 'resnet18.a1_in1k',\n",
    "            'description': 'ResNet-18 (11.7M params) - Classic, reliable',\n",
    "            'input_size': 224,\n",
    "            'params': '11.7M'\n",
    "        },\n",
    "        'resnet34': {\n",
    "            'timm_name': 'resnet34.a1_in1k',\n",
    "            'description': 'ResNet-34 (21.8M params) - More capacity',\n",
    "            'input_size': 224,\n",
    "            'params': '21.8M'\n",
    "        },\n",
    "        'mobilenetv3_small_100': {\n",
    "            'timm_name': 'mobilenetv3_small_100.lamb_in1k',\n",
    "            'description': 'MobileNetV3-Small (2.5M params) - Very efficient',\n",
    "            'input_size': 224,\n",
    "            'params': '2.5M'\n",
    "        },\n",
    "        'mobilenetv3_large_100': {\n",
    "            'timm_name': 'mobilenetv3_large_100.ra_in1k',\n",
    "            'description': 'MobileNetV3-Large (5.5M params) - Efficient',\n",
    "            'input_size': 224,\n",
    "            'params': '5.5M'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    class PlantDiseaseModel(nn.Module):\n",
    "        \\\"\\\"\\\"Plant Disease Classification Model with optional quantum layer.\\\"\\\"\\\"\\n        \n",
    "        def __init__(self, backbone_name, num_classes, pretrained=True, quantum_layer=None):\\n            super().__init__()\\n            \\n            if backbone_name not in BACKBONE_MODELS:\\n                raise ValueError(f\\\"Unknown backbone: {backbone_name}. Available: {list(BACKBONE_MODELS.keys())}\\\")\\n            \\n            timm_name = BACKBONE_MODELS[backbone_name]['timm_name']\\n            \\n            # Create backbone model\\n            self.backbone = timm.create_model(\\n                timm_name,\\n                pretrained=pretrained,\\n                num_classes=0  # Remove classifier head\\n            )\\n            \\n            # Get feature dimension\\n            with torch.no_grad():\\n                dummy_input = torch.randn(1, 3, 224, 224)\\n                features = self.backbone(dummy_input)\\n                self.feature_dim = features.shape[1]\\n            \\n            # Create classifier head\\n            if quantum_layer is not None:\\n                # Quantum-classical hybrid\\n                self.classifier = nn.Sequential(\\n                    nn.Linear(self.feature_dim, quantum_layer.embedding_dim),\\n                    nn.ReLU(),\\n                    nn.Dropout(0.3),\\n                    quantum_layer,\\n                    nn.Linear(quantum_layer.n_qubits, num_classes)  # Quantum outputs to classes\\n                )\\n            else:\\n                # Standard classifier\\n                self.classifier = nn.Sequential(\\n                    nn.Dropout(0.3),\\n                    nn.Linear(self.feature_dim, num_classes)\\n                )\\n        \\n        def forward(self, x):\\n            features = self.backbone(x)\\n            return self.classifier(features)\\n    \\n    def create_model(config, quantum_layer=None):\\n        \\\"\\\"\\\"Create a model based on configuration.\\\"\\\"\\\"\\n        \\n        backbone_name = config['backbone']\\n        num_classes = config['num_classes']\\n        \\n        model = PlantDiseaseModel(\\n            backbone_name=backbone_name,\\n            num_classes=num_classes,\\n            pretrained=True,\\n            quantum_layer=quantum_layer\\n        )\\n        \\n        return model\n",
    "    \n",
    "    def list_available_models():\\n        \\\"\\\"\\\"List all available backbone models.\\\"\\\"\\\"\\n        print(\\\"üì± Available Backbone Models:\\\")\\n        for name, info in BACKBONE_MODELS.items():\\n            print(f\\\"  {name}:\\\")\\n            print(f\\\"    - {info['description']}\\\")\\n            print(f\\\"    - Parameters: {info['params']}\\\")\\n            print(f\\\"    - Input size: {info['input_size']}\\\")\\n            print()\n",
    "\n",
    "print(\"‚úÖ Model factory ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c89aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available models and create model\n",
    "list_available_models()\n",
    "\n",
    "# Create model (without quantum layer for now)\n",
    "print(f\"üèóÔ∏è  Creating model: {config['backbone']}\")\n",
    "model = create_model(config, quantum_layer=None)\n",
    "\n",
    "# Move to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Model summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\\\nüìä Model Summary:\")\n",
    "print(f\"Backbone: {config['backbone']}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size (estimated): {total_params * 4 / (1024**2):.1f} MB\")\n",
    "\n",
    "# Test model with dummy input\n",
    "with torch.no_grad():\n",
    "    dummy_input = torch.randn(1, 3, config['image_size'], config['image_size']).to(device)\n",
    "    output = model(dummy_input)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Expected shape: [batch_size, {config['num_classes']}]\")\n",
    "    \n",
    "print(\"‚úÖ Model created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad362610",
   "metadata": {},
   "source": [
    "# 4. Optional Quantum Layer Module (PennyLane)\n",
    "\n",
    "Implement quantum neural network layers for experimental hybrid quantum-classical models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4baaaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantum layer implementation (optional)\\nquantum_available = False\\n\\ntry:\\n    from quantum_layer import QuantumLayer, HybridQuantumClassifier\\n    quantum_available = True\\n    print(\\\"‚úÖ Imported local quantum_layer\\\")\\nexcept ImportError:\\n    print(\\\"‚ÑπÔ∏è  Local quantum_layer not found. Trying inline implementation.\\\")\\n    \\n    try:\\n        import pennylane as qml\\n        from pennylane import numpy as np\\n        import torch.nn as nn\\n        \\n        class QuantumLayer(nn.Module):\\n            \\\"\\\"\\\"Quantum Neural Network Layer using PennyLane.\\\"\\\"\\\"\\n            \\n            def __init__(self, n_qubits=4, n_layers=3, embedding_dim=4):\\n                super().__init__()\\n                self.n_qubits = n_qubits\\n                self.n_layers = n_layers\\n                self.embedding_dim = embedding_dim\\n                \\n                # Create quantum device\\n                self.dev = qml.device('default.qubit', wires=n_qubits)\\n                \\n                # Define quantum circuit\\n                @qml.qnode(self.dev, interface='torch')\\n                def circuit(inputs, weights):\\n                    # Embed classical data\\n                    for i in range(n_qubits):\\n                        qml.RY(inputs[i], wires=i)\\n                    \\n                    # Variational layers\\n                    for layer in range(n_layers):\\n                        for i in range(n_qubits):\\n                            qml.RY(weights[layer, i, 0], wires=i)\\n                            qml.RZ(weights[layer, i, 1], wires=i)\\n                        \\n                        # Entangling gates\\n                        for i in range(n_qubits - 1):\\n                            qml.CNOT(wires=[i, i + 1])\\n                        if n_qubits > 2:\\n                            qml.CNOT(wires=[n_qubits - 1, 0])  # Wrap around\\n                    \\n                    # Measure expectations\\n                    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\\n                \\n                # Create weight tensor\\n                weight_shapes = {\\\"weights\\\": (n_layers, n_qubits, 2)}\\n                self.qlayer = qml.qnn.TorchLayer(circuit, weight_shapes)\\n            \\n            def forward(self, x):\\n                # Ensure input has correct dimension\\n                if x.shape[-1] != self.n_qubits:\\n                    # Project to quantum dimension\\n                    if not hasattr(self, 'projection'):\\n                        self.projection = nn.Linear(x.shape[-1], self.n_qubits).to(x.device)\\n                    x = self.projection(x)\\n                \\n                # Apply quantum circuit\\n                return self.qlayer(x)\\n        \\n        quantum_available = True\\n        print(\\\"‚úÖ Created inline quantum implementation\\\")\\n        \\n    except ImportError as e:\\n        print(f\\\"‚ö†Ô∏è  PennyLane not available: {e}\\\")\\n        print(\\\"Quantum layers will be disabled.\\\")\\n        quantum_available = False\\n\\n# Test quantum layer if available and enabled\\nif quantum_available and config.get('use_quantum', False):\\n    print(\\\"\\\\nüî¨ Testing Quantum Layer:\\\")\\n    \\n    # Create quantum layer\\n    quantum_config = config['quantum']\\n    quantum_layer = QuantumLayer(\\n        n_qubits=quantum_config['n_qubits'],\\n        n_layers=quantum_config['n_layers'],\\n        embedding_dim=quantum_config['embedding_dim']\\n    )\\n    \\n    # Test with dummy input\\n    test_input = torch.randn(2, quantum_config['embedding_dim'])  # Batch size 2\\n    with torch.no_grad():\\n        quantum_output = quantum_layer(test_input)\\n        print(f\\\"Quantum layer input shape: {test_input.shape}\\\")\\n        print(f\\\"Quantum layer output shape: {quantum_output.shape}\\\")\\n    \\n    print(\\\"‚úÖ Quantum layer test successful!\\\")\\n    print(\\\"\\\\n‚ö†Ô∏è  Note: Quantum simulation is slow on CPU. Use small datasets for testing.\\\")\\n    \\n    # Create model with quantum layer\\n    print(\\\"\\\\nüèóÔ∏è  Creating quantum-classical hybrid model...\\\")\\n    quantum_model = create_model(config, quantum_layer=quantum_layer)\\n    quantum_model = quantum_model.to(device)\\n    \\n    # Test quantum model\\n    with torch.no_grad():\\n        dummy_input = torch.randn(1, 3, config['image_size'], config['image_size']).to(device)\\n        quantum_output = quantum_model(dummy_input)\\n        print(f\\\"Quantum model output shape: {quantum_output.shape}\\\")\\n    \\n    print(\\\"‚úÖ Quantum model created successfully!\\\")\\n    \\n    # Ask user if they want to use quantum model\\n    use_quantum_model = True  # Set to False for classical training\\n    if use_quantum_model:\\n        model = quantum_model\\n        print(\\\"üöÄ Using quantum-classical hybrid model for training\\\")\\n    else:\\n        print(\\\"üì± Sticking with classical model for faster training\\\")\\n    \\nelse:\\n    if config.get('use_quantum', False):\\n        print(\\\"‚ö†Ô∏è  Quantum layers requested but not available. Using classical model.\\\")\\n    else:\\n        print(\\\"üì± Using classical model (quantum disabled in config)\\\")\\n\\nprint(\\\"\\\\n‚úÖ Model setup complete!\\\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
