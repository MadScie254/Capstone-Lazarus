{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02b939eb",
   "metadata": {},
   "source": [
    "# ğŸ§  CAPSTONE-LAZARUS: Model Training Pipeline\n",
    "\n",
    "## Advanced Plant Disease Classification Training\n",
    "**State-of-the-art transfer learning with comprehensive evaluation**\n",
    "\n",
    "### ğŸ¯ Training Objectives:\n",
    "- **Multi-Architecture Evaluation**: EfficientNet, ResNet, MobileNet comparisons\n",
    "- **Transfer Learning**: Pre-trained ImageNet â†’ Agricultural fine-tuning\n",
    "- **Balanced Training**: Class-weighted loss for imbalanced dataset\n",
    "- **Advanced Augmentation**: Field condition simulation\n",
    "- **Comprehensive Metrics**: F1, Precision, Recall, Confusion Matrix\n",
    "- **Model Optimization**: Pruning, quantization for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1a1911e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ TensorFlow version: 2.20.0\n",
      "ğŸ–¥ï¸  GPU Available: []\n",
      "ğŸ“‚ Working directory: C:\\Users\\MadScie254\\Documents\\GitHub\\Capstone-Lazarus\\notebooks\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“¦ Import Essential Libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ğŸ¤– Deep Learning Framework\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, callbacks, metrics\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# ğŸ“Š Interactive Visualizations\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# ğŸ¯ Metrics & Evaluation\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    f1_score, precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# ğŸ”§ Utilities\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "from data_utils import PlantDiseaseDataLoader\n",
    "from model_factory import ModelFactory\n",
    "\n",
    "# ğŸ¨ Configure Plotly\n",
    "px.defaults.template = \"plotly_white\"\n",
    "\n",
    "# ğŸ”§ TensorFlow Configuration\n",
    "tf.config.experimental.enable_tensor_float_32_execution(False)\n",
    "print(f\"ğŸš€ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"ğŸ–¥ï¸  GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"ğŸ“‚ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57193136",
   "metadata": {},
   "source": [
    "## ğŸ“Š Load Dataset & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "699a84e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸  Pre-computed splits not found. Running EDA first...\n",
      "ğŸ” Scanning dataset...\n",
      "âœ… Dataset scan complete:\n",
      "   ğŸ“Š Total Images: 52,266\n",
      "   ğŸ·ï¸  Classes: 19\n",
      "   âš–ï¸  Imbalance Ratio: 35.24\n",
      "ğŸ“Š Creating balanced dataset splits...\n",
      "âœ… Dataset splits created:\n",
      "   ğŸš‚ Train: 31,359 images\n",
      "   ğŸ” Validation: 10,453 images\n",
      "   ğŸ§ª Test: 10,454 images\n",
      "\n",
      "ğŸ“Š Dataset Configuration:\n",
      "   ğŸš‚ Training: 31,359 images\n",
      "   ğŸ” Validation: 10,453 images\n",
      "   ğŸ§ª Testing: 10,454 images\n",
      "   ğŸ·ï¸  Classes: 19\n",
      "   âš–ï¸  Using class weights: 19 weights computed\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“‚ Load Dataset Splits from EDA\n",
    "data_dir = \"../data\"\n",
    "models_dir = Path(\"../models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Load pre-computed splits and weights\n",
    "try:\n",
    "    split_info = np.load('../models/dataset_splits.npy', allow_pickle=True).item()\n",
    "    class_weights = np.load('../models/class_weights.npy', allow_pickle=True).item()\n",
    "    \n",
    "    X_train = split_info['train_paths']\n",
    "    y_train = split_info['train_labels']\n",
    "    X_val = split_info['val_paths']\n",
    "    y_val = split_info['val_labels']\n",
    "    X_test = split_info['test_paths']\n",
    "    y_test = split_info['test_labels']\n",
    "    class_names = split_info['class_names']\n",
    "    label_mapping = split_info['label_mapping']\n",
    "    \n",
    "    print(\"âœ… Loaded pre-computed dataset splits and class weights\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âš ï¸  Pre-computed splits not found. Running EDA first...\")\n",
    "    # Fallback: create splits\n",
    "    loader = PlantDiseaseDataLoader(data_dir, img_size=(224, 224), batch_size=32)\n",
    "    dataset_stats = loader.scan_dataset()\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = loader.create_balanced_splits()\n",
    "    class_weights = loader.compute_class_weights(y_train)\n",
    "    class_names = loader.class_names\n",
    "\n",
    "num_classes = len(class_names)\n",
    "print(f\"\\nğŸ“Š Dataset Configuration:\")\n",
    "print(f\"   ğŸš‚ Training: {len(X_train):,} images\")\n",
    "print(f\"   ğŸ” Validation: {len(X_val):,} images\")\n",
    "print(f\"   ğŸ§ª Testing: {len(X_test):,} images\")\n",
    "print(f\"   ğŸ·ï¸  Classes: {num_classes}\")\n",
    "print(f\"   âš–ï¸  Using class weights: {len(class_weights)} weights computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0c35a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TensorFlow datasets created with optimizations\n",
      "   ğŸš‚ Training batches per epoch: 979\n",
      "   ğŸ” Validation batches: 326\n",
      "   ğŸ§ª Test batches: 326\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ Create TensorFlow Datasets with Optimizations\n",
    "def create_optimized_dataset(paths: List[str], labels: List[int], \n",
    "                           batch_size: int = 32, is_training: bool = True,\n",
    "                           img_size: Tuple[int, int] = (224, 224)) -> tf.data.Dataset:\n",
    "    \"\"\"Create optimized TensorFlow dataset with augmentation.\"\"\"\n",
    "    \n",
    "    def load_and_preprocess(path, label):\n",
    "        # Load image\n",
    "        image = tf.io.read_file(path)\n",
    "        image = tf.image.decode_image(image, channels=3, expand_animations=False)\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        \n",
    "        # Resize\n",
    "        image = tf.image.resize(image, img_size)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        image = image / 255.0\n",
    "        \n",
    "        # Augmentation for training only\n",
    "        if is_training:\n",
    "            # Random flips\n",
    "            image = tf.image.random_flip_left_right(image)\n",
    "            image = tf.image.random_flip_up_down(image)\n",
    "            \n",
    "            # Color augmentation\n",
    "            image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "            image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "            image = tf.image.random_hue(image, max_delta=0.1)\n",
    "            image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
    "            \n",
    "            # Random rotation (approximate using cropping)\n",
    "            image = tf.image.random_crop(image, size=[int(img_size[0]*0.9), int(img_size[1]*0.9), 3])\n",
    "            image = tf.image.resize(image, img_size)\n",
    "        \n",
    "        # Final normalization (ImageNet stats)\n",
    "        image = (image - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    \n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=min(len(paths), 10000))\n",
    "        dataset = dataset.repeat()  # Repeat for multiple epochs\n",
    "    \n",
    "    dataset = dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# ğŸ“Š Create optimized datasets\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "train_dataset = create_optimized_dataset(X_train, y_train, BATCH_SIZE, is_training=True, img_size=IMG_SIZE)\n",
    "val_dataset = create_optimized_dataset(X_val, y_val, BATCH_SIZE, is_training=False, img_size=IMG_SIZE)\n",
    "test_dataset = create_optimized_dataset(X_test, y_test, BATCH_SIZE, is_training=False, img_size=IMG_SIZE)\n",
    "\n",
    "print(\"âœ… TensorFlow datasets created with optimizations\")\n",
    "print(f\"   ğŸš‚ Training batches per epoch: {len(X_train) // BATCH_SIZE}\")\n",
    "print(f\"   ğŸ” Validation batches: {len(X_val) // BATCH_SIZE}\")\n",
    "print(f\"   ğŸ§ª Test batches: {len(X_test) // BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec1607d",
   "metadata": {},
   "source": [
    "## ğŸ—ï¸ Model Architecture Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "361c85e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—ï¸ Architecture Comparison:\n",
      "================================================================================\n",
      "EfficientNetV2-B0    | ğŸ¥‡ Best accuracy-efficiency balance  | ~15MB    | 0.85+\n",
      "MobileNetV3-Large    | ğŸ“± Optimized for mobile deployment   | ~10MB    | 0.82+\n",
      "ResNet50             | ğŸ—ï¸ Reliable baseline performance    | ~25MB    | 0.83+\n",
      "Custom CNN           | ğŸ¨ Lightweight custom architecture   | ~5MB     | 0.78+\n",
      "================================================================================\n",
      "\n",
      "ğŸ¯ Selected primary architecture: efficientnet_v2_b0\n"
     ]
    }
   ],
   "source": [
    "# ğŸ­ Initialize Model Factory\n",
    "factory = ModelFactory(input_shape=(224, 224, 3), num_classes=num_classes, use_mixed_precision=True)\n",
    "\n",
    "# ğŸ¯ Define Model Architectures to Evaluate\n",
    "architectures_to_test = [\n",
    "    {\n",
    "        'name': 'EfficientNetV2-B0',\n",
    "        'arch': 'efficientnet_v2_b0',\n",
    "        'description': 'ğŸ¥‡ Best accuracy-efficiency balance',\n",
    "        'target_size': '~15MB',\n",
    "        'expected_accuracy': '0.85+'\n",
    "    },\n",
    "    {\n",
    "        'name': 'MobileNetV3-Large',\n",
    "        'arch': 'mobilenet_v3_large',\n",
    "        'description': 'ğŸ“± Optimized for mobile deployment',\n",
    "        'target_size': '~10MB',\n",
    "        'expected_accuracy': '0.82+'\n",
    "    },\n",
    "    {\n",
    "        'name': 'ResNet50',\n",
    "        'arch': 'resnet50',\n",
    "        'description': 'ğŸ—ï¸ Reliable baseline performance',\n",
    "        'target_size': '~25MB',\n",
    "        'expected_accuracy': '0.83+'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Custom CNN',\n",
    "        'arch': 'custom_cnn',\n",
    "        'description': 'ğŸ¨ Lightweight custom architecture',\n",
    "        'target_size': '~5MB',\n",
    "        'expected_accuracy': '0.78+'\n",
    "    }\n",
    "]\n",
    "\n",
    "# ğŸ“Š Display Architecture Comparison Table\n",
    "arch_df = pd.DataFrame(architectures_to_test)\n",
    "print(\"ğŸ—ï¸ Architecture Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "for _, row in arch_df.iterrows():\n",
    "    print(f\"{row['name']:20} | {row['description']:35} | {row['target_size']:8} | {row['expected_accuracy']}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ğŸ¯ Select primary architecture for full training\n",
    "PRIMARY_ARCHITECTURE = 'efficientnet_v2_b0'\n",
    "print(f\"\\nğŸ¯ Selected primary architecture: {PRIMARY_ARCHITECTURE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906cec23",
   "metadata": {},
   "source": [
    "## ğŸ§  Model Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d55d328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Training Configuration:\n",
      "   â€¢ epochs: 50\n",
      "   â€¢ initial_learning_rate: 0.001\n",
      "   â€¢ min_learning_rate: 1e-07\n",
      "   â€¢ patience_early_stop: 15\n",
      "   â€¢ patience_lr_reduce: 8\n",
      "   â€¢ lr_reduction_factor: 0.2\n",
      "   â€¢ validation_freq: 1\n",
      "   â€¢ save_best_only: True\n",
      "   â€¢ monitor_metric: val_f1_score\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”§ Training Configuration\n",
    "TRAINING_CONFIG = {\n",
    "    'epochs': 50,\n",
    "    'initial_learning_rate': 1e-3,\n",
    "    'min_learning_rate': 1e-7,\n",
    "    'patience_early_stop': 15,\n",
    "    'patience_lr_reduce': 8,\n",
    "    'lr_reduction_factor': 0.2,\n",
    "    'validation_freq': 1,\n",
    "    'save_best_only': True,\n",
    "    'monitor_metric': 'val_f1_score'\n",
    "}\n",
    "\n",
    "print(\"âš™ï¸ Training Configuration:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"   â€¢ {key}: {value}\")\n",
    "\n",
    "# ğŸ“Š Custom F1 Score Metric\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "    \n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.precision.reset_state()\n",
    "        self.recall.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "414d99ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—ï¸ Creating efficientnet_v2_b0 model...\n",
      "\n",
      "ğŸ“‹ Model Architecture:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"EfficientNetV2B0_PlantDisease\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"EfficientNetV2B0_PlantDisease\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ efficientnetv2-b0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)          â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">5,919,312</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,120</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)                â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">655,872</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_1                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_2                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>)                  â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,883</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)         â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ efficientnetv2-b0 (\u001b[38;5;33mFunctional\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)          â”‚       \u001b[38;5;34m5,919,312\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                â”‚           \u001b[38;5;34m5,120\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)                â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 â”‚         \u001b[38;5;34m655,872\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_1                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 â”‚           \u001b[38;5;34m2,048\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 â”‚         \u001b[38;5;34m131,328\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ batch_normalization_2                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 â”‚           \u001b[38;5;34m1,024\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 â”‚                             â”‚                 â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 â”‚               \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m)                  â”‚           \u001b[38;5;34m4,883\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,719,587</span> (25.63 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,719,587\u001b[0m (25.63 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,405,435</span> (20.62 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,405,435\u001b[0m (20.62 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,314,152</span> (5.01 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,314,152\u001b[0m (5.01 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š Model Statistics:\n",
      "   â€¢ Total parameters: 6,719,587\n",
      "   â€¢ Trainable parameters: 5,405,435\n",
      "   â€¢ Non-trainable parameters: 1,314,152\n",
      "   â€¢ Estimated size: ~25.6 MB\n"
     ]
    }
   ],
   "source": [
    "# ğŸ—ï¸ Create and Compile Model\n",
    "def create_and_compile_model(architecture: str, num_classes: int, \n",
    "                           learning_rate: float = 1e-3) -> tf.keras.Model:\n",
    "    \"\"\"Create and compile model with optimized settings.\"\"\"\n",
    "    \n",
    "    # Create model\n",
    "    model = factory.get_model(architecture, dropout_rate=0.3, freeze_backbone=False)\n",
    "    \n",
    "    # Optimizer with learning rate scheduling\n",
    "    optimizer = tf.keras.optimizers.AdamW(\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=1e-4,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-7\n",
    "    )\n",
    "    \n",
    "    # Compile with comprehensive metrics\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top_3_accuracy'),\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            F1Score()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# ğŸ¯ Create primary model\n",
    "print(f\"ğŸ—ï¸ Creating {PRIMARY_ARCHITECTURE} model...\")\n",
    "model = create_and_compile_model(PRIMARY_ARCHITECTURE, num_classes, TRAINING_CONFIG['initial_learning_rate'])\n",
    "\n",
    "# ğŸ“Š Model Summary\n",
    "print(\"\\nğŸ“‹ Model Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "# ğŸ“ˆ Count parameters\n",
    "total_params = model.count_params()\n",
    "trainable_params = sum([tf.keras.backend.count_params(layer) for layer in model.trainable_weights])\n",
    "\n",
    "print(f\"\\nğŸ“Š Model Statistics:\")\n",
    "print(f\"   â€¢ Total parameters: {total_params:,}\")\n",
    "print(f\"   â€¢ Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   â€¢ Non-trainable parameters: {total_params - trainable_params:,}\")\n",
    "print(f\"   â€¢ Estimated size: ~{total_params * 4 / (1024*1024):.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a24cb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created 5 training callbacks\n",
      "   â€¢ Model checkpoint: ../models/efficientnet_v2_b0_plant_disease_best.h5\n",
      "   â€¢ TensorBoard logs: ../models/logs/efficientnet_v2_b0_plant_disease_*\n"
     ]
    }
   ],
   "source": [
    "# ğŸ”„ Advanced Callbacks Setup\n",
    "def create_callbacks(model_name: str, config: Dict[str, Any]) -> List[tf.keras.callbacks.Callback]:\n",
    "    \"\"\"Create comprehensive training callbacks.\"\"\"\n",
    "    \n",
    "    callbacks_list = []\n",
    "    \n",
    "    # ğŸ’¾ Model Checkpoint - Save best model\n",
    "    checkpoint_path = f\"../models/{model_name}_best.h5\"\n",
    "    callbacks_list.append(\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_path,\n",
    "            monitor=config['monitor_metric'],\n",
    "            save_best_only=config['save_best_only'],\n",
    "            save_weights_only=False,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # â¹ï¸ Early Stopping\n",
    "    callbacks_list.append(\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=config['monitor_metric'],\n",
    "            patience=config['patience_early_stop'],\n",
    "            mode='max',\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # ğŸ“‰ Learning Rate Scheduler\n",
    "    callbacks_list.append(\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=config['monitor_metric'],\n",
    "            factor=config['lr_reduction_factor'],\n",
    "            patience=config['patience_lr_reduce'],\n",
    "            min_lr=config['min_learning_rate'],\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # ğŸ“Š TensorBoard Logging\n",
    "    log_dir = f\"../models/logs/{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    callbacks_list.append(\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=log_dir,\n",
    "            histogram_freq=1,\n",
    "            write_graph=True,\n",
    "            write_images=True,\n",
    "            update_freq='epoch'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # ğŸ¯ Custom Progress Callback\n",
    "    class TrainingProgressCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            if logs:\n",
    "                print(f\"\\nğŸ“Š Epoch {epoch + 1} Summary:\")\n",
    "                print(f\"   â€¢ Training Accuracy: {logs.get('accuracy', 0):.4f}\")\n",
    "                print(f\"   â€¢ Validation Accuracy: {logs.get('val_accuracy', 0):.4f}\")\n",
    "                print(f\"   â€¢ Training F1: {logs.get('f1_score', 0):.4f}\")\n",
    "                print(f\"   â€¢ Validation F1: {logs.get('val_f1_score', 0):.4f}\")\n",
    "                print(f\"   â€¢ Learning Rate: {logs.get('lr', 0):.2e}\")\n",
    "    \n",
    "    callbacks_list.append(TrainingProgressCallback())\n",
    "    \n",
    "    return callbacks_list\n",
    "\n",
    "# ğŸ”„ Create callbacks\n",
    "model_name = f\"{PRIMARY_ARCHITECTURE}_plant_disease\"\n",
    "training_callbacks = create_callbacks(model_name, TRAINING_CONFIG)\n",
    "\n",
    "print(f\"âœ… Created {len(training_callbacks)} training callbacks\")\n",
    "print(f\"   â€¢ Model checkpoint: ../models/{model_name}_best.h5\")\n",
    "print(f\"   â€¢ TensorBoard logs: ../models/logs/{model_name}_*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8871b283",
   "metadata": {},
   "source": [
    "## ğŸš‚ Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46a6e8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš‚ Starting model training...\n",
      "============================================================\n",
      "ğŸ—ï¸ Architecture: efficientnet_v2_b0\n",
      "ğŸ“Š Training samples: 31,359\n",
      "ğŸ” Validation samples: 10,453\n",
      "â±ï¸ Max epochs: 50\n",
      "ğŸ¯ Batch size: 32\n",
      "âš–ï¸ Using class weights: Yes\n",
      "============================================================\n",
      "Epoch 1/50\n",
      "âŒ Training failed: Shape must be rank 1 but is rank 0 for '{{node in_top_k/InTopKV2}} = InTopKV2[T=DT_INT32](EfficientNetV2B0_PlantDisease_1/dense_2_1/Softmax, ArgMax_1, in_top_k/InTopKV2/k)' with input shapes: [?,19], [], [].\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape must be rank 1 but is rank 0 for '{{node in_top_k/InTopKV2}} = InTopKV2[T=DT_INT32](EfficientNetV2B0_PlantDisease_1/dense_2_1/Softmax, ArgMax_1, in_top_k/InTopKV2/k)' with input shapes: [?,19], [], [].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m start_time = datetime.now()\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTRAINING_CONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mepochs\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtraining_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     training_time = datetime.now() - start_time\n\u001b[32m     33\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâœ… Training completed successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\ml_env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\math.py:47\u001b[39m, in \u001b[36min_top_k\u001b[39m\u001b[34m(targets, predictions, k)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34min_top_k\u001b[39m(targets, predictions, k):\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmath\u001b[49m\u001b[43m.\u001b[49m\u001b[43min_top_k\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: Shape must be rank 1 but is rank 0 for '{{node in_top_k/InTopKV2}} = InTopKV2[T=DT_INT32](EfficientNetV2B0_PlantDisease_1/dense_2_1/Softmax, ArgMax_1, in_top_k/InTopKV2/k)' with input shapes: [?,19], [], []."
     ]
    }
   ],
   "source": [
    "# ğŸš€ Start Training Process\n",
    "print(\"ğŸš‚ Starting model training...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"ğŸ—ï¸ Architecture: {PRIMARY_ARCHITECTURE}\")\n",
    "print(f\"ğŸ“Š Training samples: {len(X_train):,}\")\n",
    "print(f\"ğŸ” Validation samples: {len(X_val):,}\")\n",
    "print(f\"â±ï¸ Max epochs: {TRAINING_CONFIG['epochs']}\")\n",
    "print(f\"ğŸ¯ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"âš–ï¸ Using class weights: Yes\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = len(X_train) // BATCH_SIZE\n",
    "validation_steps = len(X_val) // BATCH_SIZE\n",
    "\n",
    "# ğŸ‹ï¸ Train the model\n",
    "start_time = datetime.now()\n",
    "\n",
    "try:\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=TRAINING_CONFIG['epochs'],\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=val_dataset,\n",
    "        validation_steps=validation_steps,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=training_callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    training_time = datetime.now() - start_time\n",
    "    \n",
    "    print(f\"\\nâœ… Training completed successfully!\")\n",
    "    print(f\"â±ï¸ Total training time: {training_time}\")\n",
    "    print(f\"ğŸ“ˆ Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"ğŸ” Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "    print(f\"ğŸ¯ Final validation F1: {history.history['val_f1_score'][-1]:.4f}\")\n",
    "    \n",
    "    # ğŸ’¾ Save training history\n",
    "    history_path = f\"../models/{model_name}_history.json\"\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'history': {k: [float(x) for x in v] for k, v in history.history.items()},\n",
    "            'config': TRAINING_CONFIG,\n",
    "            'training_time': str(training_time),\n",
    "            'architecture': PRIMARY_ARCHITECTURE,\n",
    "            'total_params': int(total_params),\n",
    "            'trainable_params': int(trainable_params)\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"ğŸ’¾ Training history saved: {history_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Training failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576dc550",
   "metadata": {},
   "source": [
    "## ğŸ“Š Training Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260de666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“ˆ Interactive Training History Visualization\n",
    "def plot_training_history(history_dict: Dict[str, List[float]]) -> None:\n",
    "    \"\"\"Create comprehensive training history plots.\"\"\"\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('ğŸ“ˆ Accuracy', 'ğŸ“‰ Loss', 'ğŸ¯ F1 Score', 'ğŸ“Š Learning Rate'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    epochs = list(range(1, len(history_dict['accuracy']) + 1))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=history_dict['accuracy'], name='Training Accuracy', \n",
    "                  line=dict(color='blue'), mode='lines+markers'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=history_dict['val_accuracy'], name='Validation Accuracy', \n",
    "                  line=dict(color='red'), mode='lines+markers'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Loss plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=history_dict['loss'], name='Training Loss', \n",
    "                  line=dict(color='blue'), mode='lines+markers'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=history_dict['val_loss'], name='Validation Loss', \n",
    "                  line=dict(color='red'), mode='lines+markers'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # F1 Score plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=history_dict['f1_score'], name='Training F1', \n",
    "                  line=dict(color='green'), mode='lines+markers'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=history_dict['val_f1_score'], name='Validation F1', \n",
    "                  line=dict(color='orange'), mode='lines+markers'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Learning Rate plot\n",
    "    if 'lr' in history_dict:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=epochs, y=history_dict['lr'], name='Learning Rate', \n",
    "                      line=dict(color='purple'), mode='lines+markers'),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title_text=f\"ğŸ§  {PRIMARY_ARCHITECTURE} Training History Analysis\",\n",
    "        title_x=0.5,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Update y-axes\n",
    "    fig.update_yaxes(title_text=\"Accuracy\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Loss\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"F1 Score\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Learning Rate\", type=\"log\", row=2, col=2)\n",
    "    \n",
    "    # Update x-axes\n",
    "    fig.update_xaxes(title_text=\"Epoch\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Epoch\", row=2, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# ğŸ“Š Plot training history\n",
    "if 'history' in locals():\n",
    "    plot_training_history(history.history)\n",
    "    \n",
    "    # ğŸ“ˆ Performance Summary\n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "    best_val_f1 = max(history.history['val_f1_score'])\n",
    "    final_lr = history.history['lr'][-1] if 'lr' in history.history else 'N/A'\n",
    "    \n",
    "    print(f\"\\nğŸ† Training Performance Summary:\")\n",
    "    print(f\"   â€¢ Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "    print(f\"   â€¢ Best Validation F1 Score: {best_val_f1:.4f}\")\n",
    "    print(f\"   â€¢ Final Learning Rate: {final_lr}\")\n",
    "    print(f\"   â€¢ Total Epochs Completed: {len(history.history['accuracy'])}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No training history available to plot\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
