{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02b939eb",
   "metadata": {},
   "source": [
    "# üß† CAPSTONE-LAZARUS: Model Training Pipeline\n",
    "\n",
    "## Advanced Plant Disease Classification Training\n",
    "**State-of-the-art transfer learning with comprehensive evaluation**\n",
    "\n",
    "### üéØ Training Objectives:\n",
    "- **Multi-Architecture Evaluation**: EfficientNet, ResNet, MobileNet comparisons\n",
    "- **Transfer Learning**: Pre-trained ImageNet ‚Üí Agricultural fine-tuning\n",
    "- **Balanced Training**: Class-weighted loss for imbalanced dataset\n",
    "- **Advanced Augmentation**: Field condition simulation\n",
    "- **Comprehensive Metrics**: F1, Precision, Recall, Confusion Matrix\n",
    "- **Model Optimization**: Pruning, quantization for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a1911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Import Essential Libraries\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ü§ñ Deep Learning Framework\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, optimizers, callbacks, metrics\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# üìä Interactive Visualizations\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# üéØ Metrics & Evaluation\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, \n",
    "    f1_score, precision_recall_fscore_support\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# üîß Utilities\n",
    "import pickle\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append('../src')\n",
    "from data_utils import PlantDiseaseDataLoader\n",
    "from model_factory import ModelFactory\n",
    "\n",
    "# üé® Configure Plotly\n",
    "px.defaults.template = \"plotly_white\"\n",
    "\n",
    "# üîß TensorFlow Configuration\n",
    "tf.config.experimental.enable_tensor_float_32_execution(False)\n",
    "print(f\"üöÄ TensorFlow version: {tf.__version__}\")\n",
    "print(f\"üñ•Ô∏è  GPU Available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"üìÇ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57193136",
   "metadata": {},
   "source": [
    "## üìä Load Dataset & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699a84e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÇ Load Dataset Splits from EDA\n",
    "data_dir = \"../data\"\n",
    "models_dir = Path(\"../models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Load pre-computed splits and weights\n",
    "try:\n",
    "    split_info = np.load('../models/dataset_splits.npy', allow_pickle=True).item()\n",
    "    class_weights = np.load('../models/class_weights.npy', allow_pickle=True).item()\n",
    "    \n",
    "    X_train = split_info['train_paths']\n",
    "    y_train = split_info['train_labels']\n",
    "    X_val = split_info['val_paths']\n",
    "    y_val = split_info['val_labels']\n",
    "    X_test = split_info['test_paths']\n",
    "    y_test = split_info['test_labels']\n",
    "    class_names = split_info['class_names']\n",
    "    label_mapping = split_info['label_mapping']\n",
    "    \n",
    "    print(\"‚úÖ Loaded pre-computed dataset splits and class weights\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è  Pre-computed splits not found. Running EDA first...\")\n",
    "    # Fallback: create splits\n",
    "    loader = PlantDiseaseDataLoader(data_dir, img_size=(224, 224), batch_size=32)\n",
    "    dataset_stats = loader.scan_dataset()\n",
    "    (X_train, y_train), (X_val, y_val), (X_test, y_test) = loader.create_balanced_splits()\n",
    "    class_weights = loader.compute_class_weights(y_train)\n",
    "    class_names = loader.class_names\n",
    "\n",
    "num_classes = len(class_names)\n",
    "print(f\"\\nüìä Dataset Configuration:\")\n",
    "print(f\"   üöÇ Training: {len(X_train):,} images\")\n",
    "print(f\"   üîç Validation: {len(X_val):,} images\")\n",
    "print(f\"   üß™ Testing: {len(X_test):,} images\")\n",
    "print(f\"   üè∑Ô∏è  Classes: {num_classes}\")\n",
    "print(f\"   ‚öñÔ∏è  Using class weights: {len(class_weights)} weights computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c35a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Create TensorFlow Datasets with Optimizations\n",
    "def create_optimized_dataset(paths: List[str], labels: List[int], \n",
    "                           batch_size: int = 32, is_training: bool = True,\n",
    "                           img_size: Tuple[int, int] = (224, 224)) -> tf.data.Dataset:\n",
    "    \"\"\"Create optimized TensorFlow dataset with augmentation.\"\"\"\n",
    "    \n",
    "    def load_and_preprocess(path, label):\n",
    "        # Load image\n",
    "        image = tf.io.read_file(path)\n",
    "        image = tf.image.decode_image(image, channels=3, expand_animations=False)\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        \n",
    "        # Resize\n",
    "        image = tf.image.resize(image, img_size)\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        image = image / 255.0\n",
    "        \n",
    "        # Augmentation for training only\n",
    "        if is_training:\n",
    "            # Random flips\n",
    "            image = tf.image.random_flip_left_right(image)\n",
    "            image = tf.image.random_flip_up_down(image)\n",
    "            \n",
    "            # Color augmentation\n",
    "            image = tf.image.random_brightness(image, max_delta=0.2)\n",
    "            image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
    "            image = tf.image.random_hue(image, max_delta=0.1)\n",
    "            image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
    "            \n",
    "            # Random rotation (approximate using cropping)\n",
    "            image = tf.image.random_crop(image, size=[int(img_size[0]*0.9), int(img_size[1]*0.9), 3])\n",
    "            image = tf.image.resize(image, img_size)\n",
    "        \n",
    "        # Final normalization (ImageNet stats)\n",
    "        image = (image - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    \n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size=min(len(paths), 10000))\n",
    "        dataset = dataset.repeat()  # Repeat for multiple epochs\n",
    "    \n",
    "    dataset = dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# üìä Create optimized datasets\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "train_dataset = create_optimized_dataset(X_train, y_train, BATCH_SIZE, is_training=True, img_size=IMG_SIZE)\n",
    "val_dataset = create_optimized_dataset(X_val, y_val, BATCH_SIZE, is_training=False, img_size=IMG_SIZE)\n",
    "test_dataset = create_optimized_dataset(X_test, y_test, BATCH_SIZE, is_training=False, img_size=IMG_SIZE)\n",
    "\n",
    "print(\"‚úÖ TensorFlow datasets created with optimizations\")\n",
    "print(f\"   üöÇ Training batches per epoch: {len(X_train) // BATCH_SIZE}\")\n",
    "print(f\"   üîç Validation batches: {len(X_val) // BATCH_SIZE}\")\n",
    "print(f\"   üß™ Test batches: {len(X_test) // BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec1607d",
   "metadata": {},
   "source": [
    "## üèóÔ∏è Model Architecture Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361c85e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üè≠ Initialize Model Factory\n",
    "factory = ModelFactory(input_shape=(224, 224, 3), num_classes=num_classes, use_mixed_precision=True)\n",
    "\n",
    "# üéØ Define Model Architectures to Evaluate\n",
    "architectures_to_test = [\n",
    "    {\n",
    "        'name': 'EfficientNetV2-B0',\n",
    "        'arch': 'efficientnet_v2_b0',\n",
    "        'description': 'ü•á Best accuracy-efficiency balance',\n",
    "        'target_size': '~15MB',\n",
    "        'expected_accuracy': '0.85+'\n",
    "    },\n",
    "    {\n",
    "        'name': 'MobileNetV3-Large',\n",
    "        'arch': 'mobilenet_v3_large',\n",
    "        'description': 'üì± Optimized for mobile deployment',\n",
    "        'target_size': '~10MB',\n",
    "        'expected_accuracy': '0.82+'\n",
    "    },\n",
    "    {\n",
    "        'name': 'ResNet50',\n",
    "        'arch': 'resnet50',\n",
    "        'description': 'üèóÔ∏è Reliable baseline performance',\n",
    "        'target_size': '~25MB',\n",
    "        'expected_accuracy': '0.83+'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Custom CNN',\n",
    "        'arch': 'custom_cnn',\n",
    "        'description': 'üé® Lightweight custom architecture',\n",
    "        'target_size': '~5MB',\n",
    "        'expected_accuracy': '0.78+'\n",
    "    }\n",
    "]\n",
    "\n",
    "# üìä Display Architecture Comparison Table\n",
    "arch_df = pd.DataFrame(architectures_to_test)\n",
    "print(\"üèóÔ∏è Architecture Comparison:\")\n",
    "print(\"=\" * 80)\n",
    "for _, row in arch_df.iterrows():\n",
    "    print(f\"{row['name']:20} | {row['description']:35} | {row['target_size']:8} | {row['expected_accuracy']}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# üéØ Select primary architecture for full training\n",
    "PRIMARY_ARCHITECTURE = 'efficientnet_v2_b0'\n",
    "print(f\"\\nüéØ Selected primary architecture: {PRIMARY_ARCHITECTURE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906cec23",
   "metadata": {},
   "source": [
    "## üß† Model Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d55d328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîß Training Configuration\n",
    "TRAINING_CONFIG = {\n",
    "    'epochs': 50,\n",
    "    'initial_learning_rate': 1e-3,\n",
    "    'min_learning_rate': 1e-7,\n",
    "    'patience_early_stop': 15,\n",
    "    'patience_lr_reduce': 8,\n",
    "    'lr_reduction_factor': 0.2,\n",
    "    'validation_freq': 1,\n",
    "    'save_best_only': True,\n",
    "    'monitor_metric': 'val_f1_score'\n",
    "}\n",
    "\n",
    "print(\"‚öôÔ∏è Training Configuration:\")\n",
    "for key, value in TRAINING_CONFIG.items():\n",
    "    print(f\"   ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "# üìä Custom F1 Score Metric\n",
    "class F1Score(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name='f1_score', **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "        self.precision = tf.keras.metrics.Precision()\n",
    "        self.recall = tf.keras.metrics.Recall()\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        self.precision.update_state(y_true, y_pred, sample_weight)\n",
    "        self.recall.update_state(y_true, y_pred, sample_weight)\n",
    "    \n",
    "    def result(self):\n",
    "        p = self.precision.result()\n",
    "        r = self.recall.result()\n",
    "        return 2 * ((p * r) / (p + r + tf.keras.backend.epsilon()))\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.precision.reset_state()\n",
    "        self.recall.reset_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414d99ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üèóÔ∏è Create and Compile Model\n",
    "def create_and_compile_model(architecture: str, num_classes: int, \n",
    "                           learning_rate: float = 1e-3) -> tf.keras.Model:\n",
    "    \"\"\"Create and compile model with optimized settings.\"\"\"\n",
    "    \n",
    "    # Create model\n",
    "    model = factory.get_model(architecture, dropout_rate=0.3, freeze_backbone=False)\n",
    "    \n",
    "    # Optimizer with learning rate scheduling\n",
    "    optimizer = tf.keras.optimizers.AdamW(\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=1e-4,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-7\n",
    "    )\n",
    "    \n",
    "    # Compile with comprehensive metrics\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.TopKCategoricalAccuracy(k=3, name='top_3_accuracy'),\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            F1Score()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# üéØ Create primary model\n",
    "print(f\"üèóÔ∏è Creating {PRIMARY_ARCHITECTURE} model...\")\n",
    "model = create_and_compile_model(PRIMARY_ARCHITECTURE, num_classes, TRAINING_CONFIG['initial_learning_rate'])\n",
    "\n",
    "# üìä Model Summary\n",
    "print(\"\\nüìã Model Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "# üìà Count parameters\n",
    "total_params = model.count_params()\n",
    "trainable_params = sum([tf.keras.backend.count_params(layer) for layer in model.trainable_weights])\n",
    "\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"   ‚Ä¢ Total parameters: {total_params:,}\")\n",
    "print(f\"   ‚Ä¢ Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   ‚Ä¢ Non-trainable parameters: {total_params - trainable_params:,}\")\n",
    "print(f\"   ‚Ä¢ Estimated size: ~{total_params * 4 / (1024*1024):.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a24cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîÑ Advanced Callbacks Setup\n",
    "def create_callbacks(model_name: str, config: Dict[str, Any]) -> List[tf.keras.callbacks.Callback]:\n",
    "    \"\"\"Create comprehensive training callbacks.\"\"\"\n",
    "    \n",
    "    callbacks_list = []\n",
    "    \n",
    "    # üíæ Model Checkpoint - Save best model\n",
    "    checkpoint_path = f\"../models/{model_name}_best.h5\"\n",
    "    callbacks_list.append(\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_path,\n",
    "            monitor=config['monitor_metric'],\n",
    "            save_best_only=config['save_best_only'],\n",
    "            save_weights_only=False,\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # ‚èπÔ∏è Early Stopping\n",
    "    callbacks_list.append(\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=config['monitor_metric'],\n",
    "            patience=config['patience_early_stop'],\n",
    "            mode='max',\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # üìâ Learning Rate Scheduler\n",
    "    callbacks_list.append(\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=config['monitor_metric'],\n",
    "            factor=config['lr_reduction_factor'],\n",
    "            patience=config['patience_lr_reduce'],\n",
    "            min_lr=config['min_learning_rate'],\n",
    "            mode='max',\n",
    "            verbose=1\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # üìä TensorBoard Logging\n",
    "    log_dir = f\"../models/logs/{model_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "    callbacks_list.append(\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=log_dir,\n",
    "            histogram_freq=1,\n",
    "            write_graph=True,\n",
    "            write_images=True,\n",
    "            update_freq='epoch'\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # üéØ Custom Progress Callback\n",
    "    class TrainingProgressCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs=None):\n",
    "            if logs:\n",
    "                print(f\"\\nüìä Epoch {epoch + 1} Summary:\")\n",
    "                print(f\"   ‚Ä¢ Training Accuracy: {logs.get('accuracy', 0):.4f}\")\n",
    "                print(f\"   ‚Ä¢ Validation Accuracy: {logs.get('val_accuracy', 0):.4f}\")\n",
    "                print(f\"   ‚Ä¢ Training F1: {logs.get('f1_score', 0):.4f}\")\n",
    "                print(f\"   ‚Ä¢ Validation F1: {logs.get('val_f1_score', 0):.4f}\")\n",
    "                print(f\"   ‚Ä¢ Learning Rate: {logs.get('lr', 0):.2e}\")\n",
    "    \n",
    "    callbacks_list.append(TrainingProgressCallback())\n",
    "    \n",
    "    return callbacks_list\n",
    "\n",
    "# üîÑ Create callbacks\n",
    "model_name = f\"{PRIMARY_ARCHITECTURE}_plant_disease\"\n",
    "training_callbacks = create_callbacks(model_name, TRAINING_CONFIG)\n",
    "\n",
    "print(f\"‚úÖ Created {len(training_callbacks)} training callbacks\")\n",
    "print(f\"   ‚Ä¢ Model checkpoint: ../models/{model_name}_best.h5\")\n",
    "print(f\"   ‚Ä¢ TensorBoard logs: ../models/logs/{model_name}_*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8871b283",
   "metadata": {},
   "source": [
    "## üöÇ Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a6e8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Start Training Process\n",
    "print(\"üöÇ Starting model training...\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"üèóÔ∏è Architecture: {PRIMARY_ARCHITECTURE}\")\n",
    "print(f\"üìä Training samples: {len(X_train):,}\")\n",
    "print(f\"üîç Validation samples: {len(X_val):,}\")\n",
    "print(f\"‚è±Ô∏è Max epochs: {TRAINING_CONFIG['epochs']}\")\n",
    "print(f\"üéØ Batch size: {BATCH_SIZE}\")\n",
    "print(f\"‚öñÔ∏è Using class weights: Yes\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate steps per epoch\n",
    "steps_per_epoch = len(X_train) // BATCH_SIZE\n",
    "validation_steps = len(X_val) // BATCH_SIZE\n",
    "\n",
    "# üèãÔ∏è Train the model\n",
    "start_time = datetime.now()\n",
    "\n",
    "try:\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=TRAINING_CONFIG['epochs'],\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        validation_data=val_dataset,\n",
    "        validation_steps=validation_steps,\n",
    "        class_weight=class_weights,\n",
    "        callbacks=training_callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    training_time = datetime.now() - start_time\n",
    "    \n",
    "    print(f\"\\n‚úÖ Training completed successfully!\")\n",
    "    print(f\"‚è±Ô∏è Total training time: {training_time}\")\n",
    "    print(f\"üìà Final training accuracy: {history.history['accuracy'][-1]:.4f}\")\n",
    "    print(f\"üîç Final validation accuracy: {history.history['val_accuracy'][-1]:.4f}\")\n",
    "    print(f\"üéØ Final validation F1: {history.history['val_f1_score'][-1]:.4f}\")\n",
    "    \n",
    "    # üíæ Save training history\n",
    "    history_path = f\"../models/{model_name}_history.json\"\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump({\n",
    "            'history': {k: [float(x) for x in v] for k, v in history.history.items()},\n",
    "            'config': TRAINING_CONFIG,\n",
    "            'training_time': str(training_time),\n",
    "            'architecture': PRIMARY_ARCHITECTURE,\n",
    "            'total_params': int(total_params),\n",
    "            'trainable_params': int(trainable_params)\n",
    "        }, f, indent=2)\n",
    "    \n",
    "    print(f\"üíæ Training history saved: {history_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Training failed: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576dc550",
   "metadata": {},
   "source": [
    "## üìä Training Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260de666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìà Interactive Training History Visualization\n",
    "def plot_training_history(history_dict: Dict[str, List[float]]) -> None:\n",
    "    \"\"\"Create comprehensive training history plots.\"\"\"\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=('üìà Accuracy', 'üìâ Loss', 'üéØ F1 Score', 'üìä Learning Rate'),\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    epochs = list(range(1, len(history_dict['accuracy']) + 1))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=history_dict['accuracy'], name='Training Accuracy', \n",
    "                  line=dict(color='blue'), mode='lines+markers'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=history_dict['val_accuracy'], name='Validation Accuracy', \n",
    "                  line=dict(color='red'), mode='lines+markers'),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Loss plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=history_dict['loss'], name='Training Loss', \n",
    "                  line=dict(color='blue'), mode='lines+markers'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=history_dict['val_loss'], name='Validation Loss', \n",
    "                  line=dict(color='red'), mode='lines+markers'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # F1 Score plot\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=history_dict['f1_score'], name='Training F1', \n",
    "                  line=dict(color='green'), mode='lines+markers'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=epochs, y=history_dict['val_f1_score'], name='Validation F1', \n",
    "                  line=dict(color='orange'), mode='lines+markers'),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Learning Rate plot\n",
    "    if 'lr' in history_dict:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=epochs, y=history_dict['lr'], name='Learning Rate', \n",
    "                      line=dict(color='purple'), mode='lines+markers'),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        title_text=f\"üß† {PRIMARY_ARCHITECTURE} Training History Analysis\",\n",
    "        title_x=0.5,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Update y-axes\n",
    "    fig.update_yaxes(title_text=\"Accuracy\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Loss\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"F1 Score\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Learning Rate\", type=\"log\", row=2, col=2)\n",
    "    \n",
    "    # Update x-axes\n",
    "    fig.update_xaxes(title_text=\"Epoch\", row=2, col=1)\n",
    "    fig.update_xaxes(title_text=\"Epoch\", row=2, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# üìä Plot training history\n",
    "if 'history' in locals():\n",
    "    plot_training_history(history.history)\n",
    "    \n",
    "    # üìà Performance Summary\n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "    best_val_f1 = max(history.history['val_f1_score'])\n",
    "    final_lr = history.history['lr'][-1] if 'lr' in history.history else 'N/A'\n",
    "    \n",
    "    print(f\"\\nüèÜ Training Performance Summary:\")\n",
    "    print(f\"   ‚Ä¢ Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Best Validation F1 Score: {best_val_f1:.4f}\")\n",
    "    print(f\"   ‚Ä¢ Final Learning Rate: {final_lr}\")\n",
    "    print(f\"   ‚Ä¢ Total Epochs Completed: {len(history.history['accuracy'])}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No training history available to plot\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
